// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;
using System.Runtime.CompilerServices;
using System.Threading;
using System.Threading.Tasks;
using Azure;
using Azure.Core;
using Azure.Core.Pipeline;

namespace Azure.Compute.Batch
{
    /// <summary> The Job service client. </summary>
    public partial class JobClient
    {
        private static readonly string[] AuthorizationScopes = new string[] { "https://batch.core.windows.net/.default" };
        private readonly TokenCredential _tokenCredential;
        private readonly HttpPipeline _pipeline;
        private readonly string _batchUrl;
        private readonly string _apiVersion;

        /// <summary> The ClientDiagnostics is used to provide tracing support for the client library. </summary>
        internal ClientDiagnostics ClientDiagnostics { get; }

        /// <summary> The HTTP pipeline for sending and receiving REST requests and responses. </summary>
        public virtual HttpPipeline Pipeline => _pipeline;

        /// <summary> Initializes a new instance of JobClient for mocking. </summary>
        protected JobClient()
        {
        }

        /// <summary> Initializes a new instance of JobClient. </summary>
        /// <param name="clientDiagnostics"> The handler for diagnostic messaging in the client. </param>
        /// <param name="pipeline"> The HTTP pipeline for sending and receiving REST requests and responses. </param>
        /// <param name="tokenCredential"> The token credential to copy. </param>
        /// <param name="batchUrl"> The base URL for all Azure Batch service requests. </param>
        /// <param name="apiVersion"> Api Version. </param>
        internal JobClient(ClientDiagnostics clientDiagnostics, HttpPipeline pipeline, TokenCredential tokenCredential, string batchUrl, string apiVersion)
        {
            ClientDiagnostics = clientDiagnostics;
            _pipeline = pipeline;
            _tokenCredential = tokenCredential;
            _batchUrl = batchUrl;
            _apiVersion = apiVersion;
        }

        /// <summary> Gets lifetime summary statistics for all of the Jobs in the specified Account. </summary>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. Details of the response body schema are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetAllLifetimeStatisticsAsync and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = await client.GetAllLifetimeStatisticsAsync();
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.GetProperty("url").ToString());
        /// Console.WriteLine(result.GetProperty("startTime").ToString());
        /// Console.WriteLine(result.GetProperty("lastUpdateTime").ToString());
        /// Console.WriteLine(result.GetProperty("userCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("kernelCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("wallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("readIOps").ToString());
        /// Console.WriteLine(result.GetProperty("writeIOps").ToString());
        /// Console.WriteLine(result.GetProperty("readIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("writeIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("numSucceededTasks").ToString());
        /// Console.WriteLine(result.GetProperty("numFailedTasks").ToString());
        /// Console.WriteLine(result.GetProperty("numTaskRetries").ToString());
        /// Console.WriteLine(result.GetProperty("waitTime").ToString());
        /// ]]></code>
        /// This sample shows how to call GetAllLifetimeStatisticsAsync with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = await client.GetAllLifetimeStatisticsAsync(1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow);
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.GetProperty("url").ToString());
        /// Console.WriteLine(result.GetProperty("startTime").ToString());
        /// Console.WriteLine(result.GetProperty("lastUpdateTime").ToString());
        /// Console.WriteLine(result.GetProperty("userCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("kernelCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("wallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("readIOps").ToString());
        /// Console.WriteLine(result.GetProperty("writeIOps").ToString());
        /// Console.WriteLine(result.GetProperty("readIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("writeIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("numSucceededTasks").ToString());
        /// Console.WriteLine(result.GetProperty("numFailedTasks").ToString());
        /// Console.WriteLine(result.GetProperty("numTaskRetries").ToString());
        /// Console.WriteLine(result.GetProperty("waitTime").ToString());
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// Statistics are aggregated across all Jobs that have ever existed in the Account, from Account creation to the last update time of the statistics. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// 
        /// Below is the JSON schema for the response payload.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>JobStatistics</c>:
        /// <code>{
        ///   url: string, # Required. The URL of the statistics.
        ///   startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///   lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///   userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///   kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///   wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///   readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///   writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///   readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///   writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///   numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///   numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///   numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///   waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual async Task<Response> GetAllLifetimeStatisticsAsync(int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            using var scope = ClientDiagnostics.CreateScope("JobClient.GetAllLifetimeStatistics");
            scope.Start();
            try
            {
                using HttpMessage message = CreateGetAllLifetimeStatisticsRequest(timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Gets lifetime summary statistics for all of the Jobs in the specified Account. </summary>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. Details of the response body schema are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetAllLifetimeStatistics and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = client.GetAllLifetimeStatistics();
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.GetProperty("url").ToString());
        /// Console.WriteLine(result.GetProperty("startTime").ToString());
        /// Console.WriteLine(result.GetProperty("lastUpdateTime").ToString());
        /// Console.WriteLine(result.GetProperty("userCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("kernelCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("wallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("readIOps").ToString());
        /// Console.WriteLine(result.GetProperty("writeIOps").ToString());
        /// Console.WriteLine(result.GetProperty("readIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("writeIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("numSucceededTasks").ToString());
        /// Console.WriteLine(result.GetProperty("numFailedTasks").ToString());
        /// Console.WriteLine(result.GetProperty("numTaskRetries").ToString());
        /// Console.WriteLine(result.GetProperty("waitTime").ToString());
        /// ]]></code>
        /// This sample shows how to call GetAllLifetimeStatistics with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = client.GetAllLifetimeStatistics(1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow);
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.GetProperty("url").ToString());
        /// Console.WriteLine(result.GetProperty("startTime").ToString());
        /// Console.WriteLine(result.GetProperty("lastUpdateTime").ToString());
        /// Console.WriteLine(result.GetProperty("userCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("kernelCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("wallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("readIOps").ToString());
        /// Console.WriteLine(result.GetProperty("writeIOps").ToString());
        /// Console.WriteLine(result.GetProperty("readIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("writeIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("numSucceededTasks").ToString());
        /// Console.WriteLine(result.GetProperty("numFailedTasks").ToString());
        /// Console.WriteLine(result.GetProperty("numTaskRetries").ToString());
        /// Console.WriteLine(result.GetProperty("waitTime").ToString());
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// Statistics are aggregated across all Jobs that have ever existed in the Account, from Account creation to the last update time of the statistics. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// 
        /// Below is the JSON schema for the response payload.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>JobStatistics</c>:
        /// <code>{
        ///   url: string, # Required. The URL of the statistics.
        ///   startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///   lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///   userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///   kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///   wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///   readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///   writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///   readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///   writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///   numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///   numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///   numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///   waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Response GetAllLifetimeStatistics(int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            using var scope = ClientDiagnostics.CreateScope("JobClient.GetAllLifetimeStatistics");
            scope.Start();
            try
            {
                using HttpMessage message = CreateGetAllLifetimeStatisticsRequest(timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Deletes a Job. </summary>
        /// <param name="jobId"> The ID of the Job to delete. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call DeleteAsync with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = await client.DeleteAsync("<jobId>");
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call DeleteAsync with all parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = await client.DeleteAsync("<jobId>", 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks> Deleting a Job also deletes all Tasks that are part of that Job, and all Job statistics. This also overrides the retention period for Task data; that is, if the Job contains Tasks which are still retained on Compute Nodes, the Batch services deletes those Tasks&apos; working directories and all their contents.  When a Delete Job request is received, the Batch service sets the Job to the deleting state. All update operations on a Job that is in deleting state will fail with status code 409 (Conflict), with additional information indicating that the Job is being deleted. </remarks>
        public virtual async Task<Response> DeleteAsync(string jobId, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Delete");
            scope.Start();
            try
            {
                using HttpMessage message = CreateDeleteRequest(jobId, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Deletes a Job. </summary>
        /// <param name="jobId"> The ID of the Job to delete. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call Delete with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = client.Delete("<jobId>");
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call Delete with all parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = client.Delete("<jobId>", 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks> Deleting a Job also deletes all Tasks that are part of that Job, and all Job statistics. This also overrides the retention period for Task data; that is, if the Job contains Tasks which are still retained on Compute Nodes, the Batch services deletes those Tasks&apos; working directories and all their contents.  When a Delete Job request is received, the Batch service sets the Job to the deleting state. All update operations on a Job that is in deleting state will fail with status code 409 (Conflict), with additional information indicating that the Job is being deleted. </remarks>
        public virtual Response Delete(string jobId, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Delete");
            scope.Start();
            try
            {
                using HttpMessage message = CreateDeleteRequest(jobId, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Gets information about the specified Job. </summary>
        /// <param name="jobId"> The ID of the Job. </param>
        /// <param name="select"> An OData $select clause. </param>
        /// <param name="expand"> An OData $expand clause. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. Details of the response body schema are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetJobAsync with required parameters and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = await client.GetJobAsync("<jobId>");
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.ToString());
        /// ]]></code>
        /// This sample shows how to call GetJobAsync with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = await client.GetJobAsync("<jobId>", "<select>", "<expand>", 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.GetProperty("id").ToString());
        /// Console.WriteLine(result.GetProperty("displayName").ToString());
        /// Console.WriteLine(result.GetProperty("usesTaskDependencies").ToString());
        /// Console.WriteLine(result.GetProperty("url").ToString());
        /// Console.WriteLine(result.GetProperty("eTag").ToString());
        /// Console.WriteLine(result.GetProperty("lastModified").ToString());
        /// Console.WriteLine(result.GetProperty("creationTime").ToString());
        /// Console.WriteLine(result.GetProperty("state").ToString());
        /// Console.WriteLine(result.GetProperty("stateTransitionTime").ToString());
        /// Console.WriteLine(result.GetProperty("previousState").ToString());
        /// Console.WriteLine(result.GetProperty("previousStateTransitionTime").ToString());
        /// Console.WriteLine(result.GetProperty("priority").ToString());
        /// Console.WriteLine(result.GetProperty("allowTaskPreemption").ToString());
        /// Console.WriteLine(result.GetProperty("maxParallelTasks").ToString());
        /// Console.WriteLine(result.GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("id").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("displayName").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("commandLine").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("filePattern").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("path").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("containerUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("uploadOptions").GetProperty("uploadCondition").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("requiredSlots").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("killJobOnCompletion").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("runExclusive").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("authenticationTokenSettings").GetProperty("access")[0].ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("allowLowPriorityNode").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("id").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("commandLine").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("waitForSuccess").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("rerunOnNodeRebootAfterSuccess").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("id").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("commandLine").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("maxWallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("retentionTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        /// Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("poolId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("autoPoolIdPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("poolLifetimeOption").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("keepAlive").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("displayName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("vmSize").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osFamily").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osVersion").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("publisher").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("offer").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("sku").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("version").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("virtualMachineImageId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("exactVersion").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodeAgentSKUId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("windowsConfiguration").GetProperty("enableAutomaticUpdates").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("lun").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("caching").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("diskSizeGB").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("storageAccountType").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("licenseType").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("type").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerImageNames")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("registryServer").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("diskEncryptionConfiguration").GetProperty("targets")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodePlacementConfiguration").GetProperty("policy").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("publisher").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("type").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("typeHandlerVersion").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("autoUpgradeMinorVersion").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("settings").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("protectedSettings").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("provisionAfterExtensions")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("osDisk").GetProperty("ephemeralOSDiskSettings").GetProperty("placement").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSlotsPerNode").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSchedulingPolicy").GetProperty("nodeFillType").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("resizeTimeout").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetDedicatedNodes").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetLowPriorityNodes").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableAutoScale").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleFormula").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleEvaluationInterval").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableInterNodeCommunication").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("dynamicVNetAssignmentScope").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("protocol").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("backendPort").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeStart").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeEnd").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("priority").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("access").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourceAddressPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourcePortRanges")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("provision").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("ipAddressIds")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("commandLine").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("maxTaskRetryCount").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("waitForSuccess").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprint").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprintAlgorithm").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeLocation").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("visibility")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationLicenses")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("elevationLevel").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("uid").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("gid").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("sshPrivateKey").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("windowsUserConfiguration").GetProperty("loginMode").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("containerName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountKey").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("sasKey").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("blobfuseOptions").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("relativeMountPath").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("source").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("relativeMountPath").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("mountOptions").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("source").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("relativeMountPath").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("mountOptions").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("azureFileUrl").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountKey").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("relativeMountPath").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("mountOptions").ToString());
        /// Console.WriteLine(result.GetProperty("onAllTasksComplete").ToString());
        /// Console.WriteLine(result.GetProperty("onTaskFailure").ToString());
        /// Console.WriteLine(result.GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        /// Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("startTime").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("endTime").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("poolId").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("category").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("code").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("message").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("terminateReason").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("url").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("startTime").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("lastUpdateTime").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("userCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("kernelCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("wallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("readIOps").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOps").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("readIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("numSucceededTasks").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("numFailedTasks").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("numTaskRetries").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("waitTime").ToString());
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// Below is the JSON schema for the response payload.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>Job</c>:
        /// <code>{
        ///   id: string, # Optional. The ID is case-preserving and case-insensitive (that is, you may not have two IDs within an Account that differ only by case).
        ///   displayName: string, # Optional. The display name for the Job.
        ///   usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is false.
        ///   url: string, # Optional. The URL of the Job.
        ///   eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job has changed between requests. In particular, you can be pass the ETag when updating a Job to specify that your changes should take effect only if nobody else has modified the Job in the meantime.
        ///   lastModified: string (ISO 8601 Format), # Optional. This is the last time at which the Job level data, such as the Job state or priority, changed. It does not factor in task-level changes such as adding new Tasks or Tasks changing state.
        ///   creationTime: string (ISO 8601 Format), # Optional. The creation time of the Job.
        ///   state: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. The state of the Job.
        ///   stateTransitionTime: string (ISO 8601 Format), # Optional. The time at which the Job entered its current state.
        ///   previousState: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. This property is not set if the Job is in its initial Active state.
        ///   previousStateTransitionTime: string (ISO 8601 Format), # Optional. This property is not set if the Job is in its initial Active state.
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. The default value is 0.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. The execution constraints for a Job.
        ///   jobManagerTask: {
        ///     id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters.
        ///     displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: {
        ///       containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///       imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///       registry: {
        ///         username: string, # Optional. The user name to log into the registry server.
        ///         password: string, # Optional. The password to log into the registry server.
        ///         registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///         identityReference: {
        ///           resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///         }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///       workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///     }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must be set as well. If the Pool that will run this Task doesn&apos;t have containerConfiguration set, this must not be set. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [
        ///       {
        ///         autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///         storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///         httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///         blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///         filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///         fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///         identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }
        ///     ], # Optional. Files listed under this element are located in the Task&apos;s working directory. There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     outputFiles: [
        ///       {
        ///         filePattern: string, # Required. Both relative and absolute paths are supported. Relative paths are relative to the Task working directory. The following wildcards are supported: * matches 0 or more characters (for example pattern abc* would match abc or abcdef), ** matches any directory, ? matches any single character, [abc] matches one character in the brackets, and [a-c] matches one character in the range. Brackets can include a negation to match any character not specified (for example [!abc] matches any character but a, b, or c). If a file name starts with &quot;.&quot; it is ignored by default but may be matched by specifying it explicitly (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\*.txt matches any file that does not start in &apos;.&apos; and ends with .txt in the Task working directory or any subdirectory. If the filename contains a wildcard character it can be escaped using brackets (for example abc[*] would match a file named abc*). Note that both \ and / are treated as directory separators on Windows, but only / is on Linux. Environment variables (%var% on Windows or $var on Linux) are expanded prior to the pattern being applied.
        ///         destination: {
        ///           container: {
        ///             path: string, # Optional. If filePattern refers to a specific file (i.e. contains no wildcards), then path is the name of the blob to which to upload that file. If filePattern contains one or more wildcards (and therefore may match multiple files), then path is the name of the blob virtual directory (which is prepended to each blob name) to which to upload the file(s). If omitted, file(s) are uploaded to the root of the container with a blob name matching their file name.
        ///             containerUrl: string, # Required. If not using a managed identity, the URL must include a Shared Access Signature (SAS) granting write permissions to the container.
        ///             identityReference: ComputeNodeIdentityReference, # Optional. The identity must have write access to the Azure Blob Storage container
        ///             uploadHeaders: [
        ///               {
        ///                 name: string, # Required. The case-insensitive name of the header to be used while uploading output files
        ///                 value: string, # Optional. The value of the header to be used while uploading output files
        ///               }
        ///             ], # Optional. These headers will be specified when uploading files to Azure Storage. For more information, see [Request Headers (All Blob Types)](https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types).
        ///           }, # Optional. Specifies a file upload destination within an Azure blob storage container.
        ///         }, # Required. The destination to which a file should be uploaded.
        ///         uploadOptions: {
        ///           uploadCondition: &quot;tasksuccess&quot; | &quot;taskfailure&quot; | &quot;taskcompletion&quot;, # Required. The default is taskcompletion.
        ///         }, # Required. Details about an output file upload operation, including under what conditions to perform the upload.
        ///       }
        ///     ], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node on which the primary Task is executed.
        ///     environmentSettings: [
        ///       {
        ///         name: string, # Required. The name of the environment variable.
        ///         value: string, # Optional. The value of the environment variable.
        ///       }
        ///     ], # Optional. A list of environment variable settings for the Job Manager Task.
        ///     constraints: {
        ///       maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        ///       retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///       maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task executable due to a nonzero exit code. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task after the first attempt. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///     }, # Optional. Execution constraints to apply to a Task.
        ///     requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the node has enough free scheduling slots available. For multi-instance Tasks, this property is not supported and must not be specified.
        ///     killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job as complete. If any Tasks are still running at this time (other than Job Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not affect the Job status. In this case, you should either use the onAllTasksComplete attribute to terminate the Job, or have a client or user terminate the Job explicitly. An example of this is if the Job Manager creates a set of Tasks but then takes no further role in their execution. The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job (not to monitor progress), then it is important to set killJobOnCompletion to false.
        ///     userIdentity: {
        ///       username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///       autoUser: {
        ///         scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///         elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///       }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///     }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///     runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a Compute Node. The Job Manager Task counts normally against the Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The default value is true.
        ///     applicationPackageReferences: [
        ///       {
        ///         applicationId: string, # Required. The ID of the application to deploy.
        ///         version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///       }
        ///     ], # Optional. Application Packages are downloaded and deployed to a shared directory, not the Task working directory. Therefore, if a referenced Application Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute Node is used. If a referenced Application Package cannot be installed, for example because the package has been deleted or because download failed, the Task fails.
        ///     authenticationTokenSettings: {
        ///       access: [string], # Optional. The authentication token grants access to a limited set of Batch service operations. Currently the only supported value for the access property is &apos;job&apos;, which grants access to all operations related to the Job which contains the Task.
        ///     }, # Optional. If this property is set, the Batch service provides the Task with an authentication token which can be used to authenticate Batch service operations without requiring an Account access key. The token is provided via the AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out using the token depend on the settings. For example, a Task can request Job permissions in order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the Job.
        ///     allowLowPriorityNode: boolean, # Optional. The default value is true.
        ///   }, # Optional. The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running &apos;normal&apos; Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
        ///   jobPreparationTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
        ///     constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
        ///     waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute Nodes.
        ///     rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.
        ///   }, # Optional. The Job Preparation Task is a special Task run on each Compute Node before any other Task of the Job.
        ///   jobReleaseTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute Node, measured from the time the Task starts. If the Task does not complete within the time limit, the Batch service terminates it. The default value is 15 minutes. You may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///     retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///   }, # Optional. The Job Release Task is a special Task run at the end of the Job on each Compute Node that has run any other Task of the Job.
        ///   commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by specifying the same setting name with a different value.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [ApplicationPackageReference], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. Specifies how a Job should be assigned to a Pool.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. The default is noaction.
        ///   onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. A Task is considered to have failed if has a failureInfo. A failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry count, or if there was an error starting the Task, for example due to a resource file download error. The default is noaction.
        ///   networkConfiguration: {
        ///     subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. This is of the form /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication from the Azure Batch service. For Pools created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound connections for communications to Azure Storage. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///   }, # Optional. The network configuration for the Job.
        ///   metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///   executionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. This is the time at which the Job was created.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Job is in the completed state.
        ///     poolId: string, # Optional. This element contains the actual Pool where the Job is assigned. When you get Job details from the service, they also contain a poolInfo element, which contains the Pool configuration data from when the Job was added or updated. That poolInfo element may also contain a poolId element. If it does, the two IDs are the same. If it does not, it means the Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        ///     schedulingError: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Job scheduling error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Job scheduling error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional error details related to the scheduling error.
        ///     }, # Optional. This property is not set if there was no error starting the Job.
        ///     terminateReason: string, # Optional. This property is set only if the Job is in the completed state. If the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a schedule, and the schedule terminated. AllTasksComplete - the Job&apos;s onAllTasksComplete attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job&apos;s onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed with an exit condition that specified a jobAction of terminatejob. Any other string is a user-defined reason specified in a call to the &apos;Terminate a Job&apos; operation.
        ///   }, # Optional. Contains information about the execution of a Job in the Azure Batch service.
        ///   stats: {
        ///     url: string, # Required. The URL of the statistics.
        ///     startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///     lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///     userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///     readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///     writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///     readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///     writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///     numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///     numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///     numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///     waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        ///   }, # Optional. This property is populated only if the CloudJob was retrieved with an expand clause including the &apos;stats&apos; attribute; otherwise it is null. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual async Task<Response> GetJobAsync(string jobId, string select = null, string expand = null, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            using var scope = ClientDiagnostics.CreateScope("JobClient.GetJob");
            scope.Start();
            try
            {
                using HttpMessage message = CreateGetJobRequest(jobId, select, expand, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Gets information about the specified Job. </summary>
        /// <param name="jobId"> The ID of the Job. </param>
        /// <param name="select"> An OData $select clause. </param>
        /// <param name="expand"> An OData $expand clause. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. Details of the response body schema are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetJob with required parameters and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = client.GetJob("<jobId>");
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.ToString());
        /// ]]></code>
        /// This sample shows how to call GetJob with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = client.GetJob("<jobId>", "<select>", "<expand>", 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.GetProperty("id").ToString());
        /// Console.WriteLine(result.GetProperty("displayName").ToString());
        /// Console.WriteLine(result.GetProperty("usesTaskDependencies").ToString());
        /// Console.WriteLine(result.GetProperty("url").ToString());
        /// Console.WriteLine(result.GetProperty("eTag").ToString());
        /// Console.WriteLine(result.GetProperty("lastModified").ToString());
        /// Console.WriteLine(result.GetProperty("creationTime").ToString());
        /// Console.WriteLine(result.GetProperty("state").ToString());
        /// Console.WriteLine(result.GetProperty("stateTransitionTime").ToString());
        /// Console.WriteLine(result.GetProperty("previousState").ToString());
        /// Console.WriteLine(result.GetProperty("previousStateTransitionTime").ToString());
        /// Console.WriteLine(result.GetProperty("priority").ToString());
        /// Console.WriteLine(result.GetProperty("allowTaskPreemption").ToString());
        /// Console.WriteLine(result.GetProperty("maxParallelTasks").ToString());
        /// Console.WriteLine(result.GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("id").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("displayName").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("commandLine").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("filePattern").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("path").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("containerUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("uploadOptions").GetProperty("uploadCondition").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("requiredSlots").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("killJobOnCompletion").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("runExclusive").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("authenticationTokenSettings").GetProperty("access")[0].ToString());
        /// Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("allowLowPriorityNode").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("id").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("commandLine").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("waitForSuccess").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        /// Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("rerunOnNodeRebootAfterSuccess").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("id").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("commandLine").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("maxWallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("retentionTime").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        /// Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        /// Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("poolId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("autoPoolIdPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("poolLifetimeOption").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("keepAlive").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("displayName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("vmSize").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osFamily").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osVersion").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("publisher").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("offer").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("sku").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("version").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("virtualMachineImageId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("exactVersion").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodeAgentSKUId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("windowsConfiguration").GetProperty("enableAutomaticUpdates").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("lun").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("caching").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("diskSizeGB").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("storageAccountType").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("licenseType").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("type").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerImageNames")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("registryServer").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("diskEncryptionConfiguration").GetProperty("targets")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodePlacementConfiguration").GetProperty("policy").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("publisher").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("type").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("typeHandlerVersion").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("autoUpgradeMinorVersion").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("settings").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("protectedSettings").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("provisionAfterExtensions")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("osDisk").GetProperty("ephemeralOSDiskSettings").GetProperty("placement").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSlotsPerNode").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSchedulingPolicy").GetProperty("nodeFillType").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("resizeTimeout").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetDedicatedNodes").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetLowPriorityNodes").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableAutoScale").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleFormula").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleEvaluationInterval").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableInterNodeCommunication").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("dynamicVNetAssignmentScope").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("protocol").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("backendPort").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeStart").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeEnd").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("priority").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("access").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourceAddressPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourcePortRanges")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("provision").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("ipAddressIds")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("commandLine").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("maxTaskRetryCount").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("waitForSuccess").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprint").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprintAlgorithm").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeLocation").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("visibility")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationLicenses")[0].ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("elevationLevel").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("uid").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("gid").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("sshPrivateKey").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("windowsUserConfiguration").GetProperty("loginMode").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("containerName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountKey").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("sasKey").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("blobfuseOptions").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("relativeMountPath").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("identityReference").GetProperty("resourceId").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("source").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("relativeMountPath").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("mountOptions").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("username").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("source").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("relativeMountPath").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("mountOptions").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("password").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountName").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("azureFileUrl").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountKey").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("relativeMountPath").ToString());
        /// Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("mountOptions").ToString());
        /// Console.WriteLine(result.GetProperty("onAllTasksComplete").ToString());
        /// Console.WriteLine(result.GetProperty("onTaskFailure").ToString());
        /// Console.WriteLine(result.GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        /// Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("startTime").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("endTime").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("poolId").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("category").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("code").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("message").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("name").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("value").ToString());
        /// Console.WriteLine(result.GetProperty("executionInfo").GetProperty("terminateReason").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("url").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("startTime").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("lastUpdateTime").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("userCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("kernelCPUTime").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("wallClockTime").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("readIOps").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOps").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("readIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOGiB").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("numSucceededTasks").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("numFailedTasks").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("numTaskRetries").ToString());
        /// Console.WriteLine(result.GetProperty("stats").GetProperty("waitTime").ToString());
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// Below is the JSON schema for the response payload.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>Job</c>:
        /// <code>{
        ///   id: string, # Optional. The ID is case-preserving and case-insensitive (that is, you may not have two IDs within an Account that differ only by case).
        ///   displayName: string, # Optional. The display name for the Job.
        ///   usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is false.
        ///   url: string, # Optional. The URL of the Job.
        ///   eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job has changed between requests. In particular, you can be pass the ETag when updating a Job to specify that your changes should take effect only if nobody else has modified the Job in the meantime.
        ///   lastModified: string (ISO 8601 Format), # Optional. This is the last time at which the Job level data, such as the Job state or priority, changed. It does not factor in task-level changes such as adding new Tasks or Tasks changing state.
        ///   creationTime: string (ISO 8601 Format), # Optional. The creation time of the Job.
        ///   state: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. The state of the Job.
        ///   stateTransitionTime: string (ISO 8601 Format), # Optional. The time at which the Job entered its current state.
        ///   previousState: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. This property is not set if the Job is in its initial Active state.
        ///   previousStateTransitionTime: string (ISO 8601 Format), # Optional. This property is not set if the Job is in its initial Active state.
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. The default value is 0.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. The execution constraints for a Job.
        ///   jobManagerTask: {
        ///     id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters.
        ///     displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: {
        ///       containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///       imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///       registry: {
        ///         username: string, # Optional. The user name to log into the registry server.
        ///         password: string, # Optional. The password to log into the registry server.
        ///         registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///         identityReference: {
        ///           resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///         }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///       workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///     }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must be set as well. If the Pool that will run this Task doesn&apos;t have containerConfiguration set, this must not be set. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [
        ///       {
        ///         autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///         storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///         httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///         blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///         filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///         fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///         identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }
        ///     ], # Optional. Files listed under this element are located in the Task&apos;s working directory. There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     outputFiles: [
        ///       {
        ///         filePattern: string, # Required. Both relative and absolute paths are supported. Relative paths are relative to the Task working directory. The following wildcards are supported: * matches 0 or more characters (for example pattern abc* would match abc or abcdef), ** matches any directory, ? matches any single character, [abc] matches one character in the brackets, and [a-c] matches one character in the range. Brackets can include a negation to match any character not specified (for example [!abc] matches any character but a, b, or c). If a file name starts with &quot;.&quot; it is ignored by default but may be matched by specifying it explicitly (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\*.txt matches any file that does not start in &apos;.&apos; and ends with .txt in the Task working directory or any subdirectory. If the filename contains a wildcard character it can be escaped using brackets (for example abc[*] would match a file named abc*). Note that both \ and / are treated as directory separators on Windows, but only / is on Linux. Environment variables (%var% on Windows or $var on Linux) are expanded prior to the pattern being applied.
        ///         destination: {
        ///           container: {
        ///             path: string, # Optional. If filePattern refers to a specific file (i.e. contains no wildcards), then path is the name of the blob to which to upload that file. If filePattern contains one or more wildcards (and therefore may match multiple files), then path is the name of the blob virtual directory (which is prepended to each blob name) to which to upload the file(s). If omitted, file(s) are uploaded to the root of the container with a blob name matching their file name.
        ///             containerUrl: string, # Required. If not using a managed identity, the URL must include a Shared Access Signature (SAS) granting write permissions to the container.
        ///             identityReference: ComputeNodeIdentityReference, # Optional. The identity must have write access to the Azure Blob Storage container
        ///             uploadHeaders: [
        ///               {
        ///                 name: string, # Required. The case-insensitive name of the header to be used while uploading output files
        ///                 value: string, # Optional. The value of the header to be used while uploading output files
        ///               }
        ///             ], # Optional. These headers will be specified when uploading files to Azure Storage. For more information, see [Request Headers (All Blob Types)](https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types).
        ///           }, # Optional. Specifies a file upload destination within an Azure blob storage container.
        ///         }, # Required. The destination to which a file should be uploaded.
        ///         uploadOptions: {
        ///           uploadCondition: &quot;tasksuccess&quot; | &quot;taskfailure&quot; | &quot;taskcompletion&quot;, # Required. The default is taskcompletion.
        ///         }, # Required. Details about an output file upload operation, including under what conditions to perform the upload.
        ///       }
        ///     ], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node on which the primary Task is executed.
        ///     environmentSettings: [
        ///       {
        ///         name: string, # Required. The name of the environment variable.
        ///         value: string, # Optional. The value of the environment variable.
        ///       }
        ///     ], # Optional. A list of environment variable settings for the Job Manager Task.
        ///     constraints: {
        ///       maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        ///       retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///       maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task executable due to a nonzero exit code. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task after the first attempt. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///     }, # Optional. Execution constraints to apply to a Task.
        ///     requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the node has enough free scheduling slots available. For multi-instance Tasks, this property is not supported and must not be specified.
        ///     killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job as complete. If any Tasks are still running at this time (other than Job Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not affect the Job status. In this case, you should either use the onAllTasksComplete attribute to terminate the Job, or have a client or user terminate the Job explicitly. An example of this is if the Job Manager creates a set of Tasks but then takes no further role in their execution. The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job (not to monitor progress), then it is important to set killJobOnCompletion to false.
        ///     userIdentity: {
        ///       username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///       autoUser: {
        ///         scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///         elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///       }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///     }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///     runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a Compute Node. The Job Manager Task counts normally against the Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The default value is true.
        ///     applicationPackageReferences: [
        ///       {
        ///         applicationId: string, # Required. The ID of the application to deploy.
        ///         version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///       }
        ///     ], # Optional. Application Packages are downloaded and deployed to a shared directory, not the Task working directory. Therefore, if a referenced Application Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute Node is used. If a referenced Application Package cannot be installed, for example because the package has been deleted or because download failed, the Task fails.
        ///     authenticationTokenSettings: {
        ///       access: [string], # Optional. The authentication token grants access to a limited set of Batch service operations. Currently the only supported value for the access property is &apos;job&apos;, which grants access to all operations related to the Job which contains the Task.
        ///     }, # Optional. If this property is set, the Batch service provides the Task with an authentication token which can be used to authenticate Batch service operations without requiring an Account access key. The token is provided via the AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out using the token depend on the settings. For example, a Task can request Job permissions in order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the Job.
        ///     allowLowPriorityNode: boolean, # Optional. The default value is true.
        ///   }, # Optional. The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running &apos;normal&apos; Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
        ///   jobPreparationTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
        ///     constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
        ///     waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute Nodes.
        ///     rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.
        ///   }, # Optional. The Job Preparation Task is a special Task run on each Compute Node before any other Task of the Job.
        ///   jobReleaseTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute Node, measured from the time the Task starts. If the Task does not complete within the time limit, the Batch service terminates it. The default value is 15 minutes. You may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///     retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///   }, # Optional. The Job Release Task is a special Task run at the end of the Job on each Compute Node that has run any other Task of the Job.
        ///   commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by specifying the same setting name with a different value.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [ApplicationPackageReference], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. Specifies how a Job should be assigned to a Pool.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. The default is noaction.
        ///   onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. A Task is considered to have failed if has a failureInfo. A failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry count, or if there was an error starting the Task, for example due to a resource file download error. The default is noaction.
        ///   networkConfiguration: {
        ///     subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. This is of the form /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication from the Azure Batch service. For Pools created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound connections for communications to Azure Storage. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///   }, # Optional. The network configuration for the Job.
        ///   metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///   executionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. This is the time at which the Job was created.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Job is in the completed state.
        ///     poolId: string, # Optional. This element contains the actual Pool where the Job is assigned. When you get Job details from the service, they also contain a poolInfo element, which contains the Pool configuration data from when the Job was added or updated. That poolInfo element may also contain a poolId element. If it does, the two IDs are the same. If it does not, it means the Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        ///     schedulingError: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Job scheduling error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Job scheduling error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional error details related to the scheduling error.
        ///     }, # Optional. This property is not set if there was no error starting the Job.
        ///     terminateReason: string, # Optional. This property is set only if the Job is in the completed state. If the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a schedule, and the schedule terminated. AllTasksComplete - the Job&apos;s onAllTasksComplete attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job&apos;s onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed with an exit condition that specified a jobAction of terminatejob. Any other string is a user-defined reason specified in a call to the &apos;Terminate a Job&apos; operation.
        ///   }, # Optional. Contains information about the execution of a Job in the Azure Batch service.
        ///   stats: {
        ///     url: string, # Required. The URL of the statistics.
        ///     startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///     lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///     userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///     readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///     writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///     readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///     writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///     numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///     numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///     numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///     waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        ///   }, # Optional. This property is populated only if the CloudJob was retrieved with an expand clause including the &apos;stats&apos; attribute; otherwise it is null. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Response GetJob(string jobId, string select = null, string expand = null, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            using var scope = ClientDiagnostics.CreateScope("JobClient.GetJob");
            scope.Start();
            try
            {
                using HttpMessage message = CreateGetJobRequest(jobId, select, expand, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Updates the properties of the specified Job. </summary>
        /// <param name="jobId"> The ID of the Job whose properties you want to update. </param>
        /// <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call PatchAsync with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {};
        /// 
        /// Response response = await client.PatchAsync("<jobId>", RequestContent.Create(data));
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call PatchAsync with all parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     priority = 1234,
        ///     maxParallelTasks = 1234,
        ///     allowTaskPreemption = true,
        ///     onAllTasksComplete = "noaction",
        ///     constraints = new {
        ///         maxWallClockTime = PT1H23M45S,
        ///         maxTaskRetryCount = 1234,
        ///     },
        ///     poolInfo = new {
        ///         poolId = "<poolId>",
        ///         autoPoolSpecification = new {
        ///             autoPoolIdPrefix = "<autoPoolIdPrefix>",
        ///             poolLifetimeOption = "jobschedule",
        ///             keepAlive = true,
        ///             pool = new {
        ///                 displayName = "<displayName>",
        ///                 vmSize = "<vmSize>",
        ///                 cloudServiceConfiguration = new {
        ///                     osFamily = "<osFamily>",
        ///                     osVersion = "<osVersion>",
        ///                 },
        ///                 virtualMachineConfiguration = new {
        ///                     imageReference = new {
        ///                         publisher = "<publisher>",
        ///                         offer = "<offer>",
        ///                         sku = "<sku>",
        ///                         version = "<version>",
        ///                         virtualMachineImageId = "<virtualMachineImageId>",
        ///                     },
        ///                     nodeAgentSKUId = "<nodeAgentSKUId>",
        ///                     windowsConfiguration = new {
        ///                         enableAutomaticUpdates = true,
        ///                     },
        ///                     dataDisks = new[] {
        ///                         new {
        ///                             lun = 1234,
        ///                             caching = "none",
        ///                             diskSizeGB = 1234,
        ///                             storageAccountType = "standard_lrs",
        ///                         }
        ///                     },
        ///                     licenseType = "<licenseType>",
        ///                     containerConfiguration = new {
        ///                         type = "<type>",
        ///                         containerImageNames = new[] {
        ///                             "<String>"
        ///                         },
        ///                         containerRegistries = new[] {
        ///                             new {
        ///                                 username = "<username>",
        ///                                 password = "<password>",
        ///                                 registryServer = "<registryServer>",
        ///                                 identityReference = new {
        ///                                     resourceId = "<resourceId>",
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     diskEncryptionConfiguration = new {
        ///                         targets = new[] {
        ///                             "osdisk"
        ///                         },
        ///                     },
        ///                     nodePlacementConfiguration = new {
        ///                         policy = "regional",
        ///                     },
        ///                     extensions = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             publisher = "<publisher>",
        ///                             type = "<type>",
        ///                             typeHandlerVersion = "<typeHandlerVersion>",
        ///                             autoUpgradeMinorVersion = true,
        ///                             settings = new {},
        ///                             protectedSettings = new {},
        ///                             provisionAfterExtensions = new[] {
        ///                                 "<String>"
        ///                             },
        ///                         }
        ///                     },
        ///                     osDisk = new {
        ///                         ephemeralOSDiskSettings = new {
        ///                             placement = "<placement>",
        ///                         },
        ///                     },
        ///                 },
        ///                 taskSlotsPerNode = 1234,
        ///                 taskSchedulingPolicy = new {
        ///                     nodeFillType = "spread",
        ///                 },
        ///                 resizeTimeout = PT1H23M45S,
        ///                 targetDedicatedNodes = 1234,
        ///                 targetLowPriorityNodes = 1234,
        ///                 enableAutoScale = true,
        ///                 autoScaleFormula = "<autoScaleFormula>",
        ///                 autoScaleEvaluationInterval = PT1H23M45S,
        ///                 enableInterNodeCommunication = true,
        ///                 networkConfiguration = new {
        ///                     subnetId = "<subnetId>",
        ///                     dynamicVNetAssignmentScope = "none",
        ///                     endpointConfiguration = new {
        ///                         inboundNATPools = new[] {
        ///                             new {
        ///                                 name = "<name>",
        ///                                 protocol = "tcp",
        ///                                 backendPort = 1234,
        ///                                 frontendPortRangeStart = 1234,
        ///                                 frontendPortRangeEnd = 1234,
        ///                                 networkSecurityGroupRules = new[] {
        ///                                     new {
        ///                                         priority = 1234,
        ///                                         access = "allow",
        ///                                         sourceAddressPrefix = "<sourceAddressPrefix>",
        ///                                         sourcePortRanges = new[] {
        ///                                             "<String>"
        ///                                         },
        ///                                     }
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     publicIPAddressConfiguration = new {
        ///                         provision = "batchmanaged",
        ///                         ipAddressIds = new[] {
        ///                             "<String>"
        ///                         },
        ///                     },
        ///                 },
        ///                 startTask = new {
        ///                     commandLine = "<commandLine>",
        ///                     containerSettings = new {
        ///                         containerRunOptions = "<containerRunOptions>",
        ///                         imageName = "<imageName>",
        ///                         registry = new {
        ///                             username = "<username>",
        ///                             password = "<password>",
        ///                             registryServer = "<registryServer>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         workingDirectory = "taskWorkingDirectory",
        ///                     },
        ///                     resourceFiles = new[] {
        ///                         new {
        ///                             autoStorageContainerName = "<autoStorageContainerName>",
        ///                             storageContainerUrl = "<storageContainerUrl>",
        ///                             httpUrl = "<httpUrl>",
        ///                             blobPrefix = "<blobPrefix>",
        ///                             filePath = "<filePath>",
        ///                             fileMode = "<fileMode>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         }
        ///                     },
        ///                     environmentSettings = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             value = "<value>",
        ///                         }
        ///                     },
        ///                     userIdentity = new {
        ///                         username = "<username>",
        ///                         autoUser = new {
        ///                             scope = "task",
        ///                             elevationLevel = "nonadmin",
        ///                         },
        ///                     },
        ///                     maxTaskRetryCount = 1234,
        ///                     waitForSuccess = true,
        ///                 },
        ///                 certificateReferences = new[] {
        ///                     new {
        ///                         thumbprint = "<thumbprint>",
        ///                         thumbprintAlgorithm = "<thumbprintAlgorithm>",
        ///                         storeLocation = "currentuser",
        ///                         storeName = "<storeName>",
        ///                         visibility = new[] {
        ///                             "starttask"
        ///                         },
        ///                     }
        ///                 },
        ///                 applicationPackageReferences = new[] {
        ///                     new {
        ///                         applicationId = "<applicationId>",
        ///                         version = "<version>",
        ///                     }
        ///                 },
        ///                 applicationLicenses = new[] {
        ///                     "<String>"
        ///                 },
        ///                 userAccounts = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         password = "<password>",
        ///                         elevationLevel = "nonadmin",
        ///                         linuxUserConfiguration = new {
        ///                             uid = 1234,
        ///                             gid = 1234,
        ///                             sshPrivateKey = "<sshPrivateKey>",
        ///                         },
        ///                         windowsUserConfiguration = new {
        ///                             loginMode = "batch",
        ///                         },
        ///                     }
        ///                 },
        ///                 metadata = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         value = "<value>",
        ///                     }
        ///                 },
        ///                 mountConfiguration = new[] {
        ///                     new {
        ///                         azureBlobFileSystemConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             containerName = "<containerName>",
        ///                             accountKey = "<accountKey>",
        ///                             sasKey = "<sasKey>",
        ///                             blobfuseOptions = "<blobfuseOptions>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         nfsMountConfiguration = new {
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                         cifsMountConfiguration = new {
        ///                             username = "<username>",
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                             password = "<password>",
        ///                         },
        ///                         azureFileShareConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             azureFileUrl = "<azureFileUrl>",
        ///                             accountKey = "<accountKey>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                     }
        ///                 },
        ///             },
        ///         },
        ///     },
        ///     metadata = new[] {
        ///         new {
        ///             name = "<name>",
        ///             value = "<value>",
        ///         }
        ///     },
        /// };
        /// 
        /// Response response = await client.PatchAsync("<jobId>", RequestContent.Create(data), 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// This replaces only the Job properties specified in the request. For example, if the Job has constraints, and a request does not specify the constraints element, then the Job keeps the existing constraints.
        /// 
        /// Below is the JSON schema for the request payload.
        /// 
        /// Request Body:
        /// 
        /// Schema for <c>JobUpdate</c>:
        /// <code>{
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. If omitted, the priority of the Job is left unchanged.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. If omitted, the completion behavior is left unchanged. You may not change the value from terminatejob to noaction - that is, once you have engaged automatic Job termination, you cannot turn it off again. If you try to do this, the request fails with an &apos;invalid property value&apos; error response; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. If omitted, the existing execution constraints are left unchanged.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [
        ///               {
        ///                 username: string, # Optional. The user name to log into the registry server.
        ///                 password: string, # Optional. The password to log into the registry server.
        ///                 registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///                 identityReference: {
        ///                   resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///                 }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///               }
        ///             ], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: {
        ///             containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///             imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///             registry: ContainerRegistry, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///             workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///           }, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [
        ///             {
        ///               autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///               storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///               httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///               blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///               filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///               fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///             }
        ///           ], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [
        ///             {
        ///               name: string, # Required. The name of the environment variable.
        ///               value: string, # Optional. The value of the environment variable.
        ///             }
        ///           ], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: {
        ///             username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///             autoUser: {
        ///               scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///               elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///           }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [
        ///           {
        ///             applicationId: string, # Required. The ID of the application to deploy.
        ///             version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///           }
        ///         ], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. You may change the Pool for a Job only when the Job is disabled. The Patch Job call will fail if you include the poolInfo element and the Job is not disabled. If you specify an autoPoolSpecification in the poolInfo, only the keepAlive property of the autoPoolSpecification can be updated, and then only if the autoPoolSpecification has a poolLifetimeOption of Job (other job properties can be updated as normal). If omitted, the Job continues to run on its current Pool.
        ///   metadata: [MetadataItem], # Optional. If omitted, the existing Job metadata is left unchanged.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual async Task<Response> PatchAsync(string jobId, RequestContent content, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Patch");
            scope.Start();
            try
            {
                using HttpMessage message = CreatePatchRequest(jobId, content, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Updates the properties of the specified Job. </summary>
        /// <param name="jobId"> The ID of the Job whose properties you want to update. </param>
        /// <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call Patch with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {};
        /// 
        /// Response response = client.Patch("<jobId>", RequestContent.Create(data));
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call Patch with all parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     priority = 1234,
        ///     maxParallelTasks = 1234,
        ///     allowTaskPreemption = true,
        ///     onAllTasksComplete = "noaction",
        ///     constraints = new {
        ///         maxWallClockTime = PT1H23M45S,
        ///         maxTaskRetryCount = 1234,
        ///     },
        ///     poolInfo = new {
        ///         poolId = "<poolId>",
        ///         autoPoolSpecification = new {
        ///             autoPoolIdPrefix = "<autoPoolIdPrefix>",
        ///             poolLifetimeOption = "jobschedule",
        ///             keepAlive = true,
        ///             pool = new {
        ///                 displayName = "<displayName>",
        ///                 vmSize = "<vmSize>",
        ///                 cloudServiceConfiguration = new {
        ///                     osFamily = "<osFamily>",
        ///                     osVersion = "<osVersion>",
        ///                 },
        ///                 virtualMachineConfiguration = new {
        ///                     imageReference = new {
        ///                         publisher = "<publisher>",
        ///                         offer = "<offer>",
        ///                         sku = "<sku>",
        ///                         version = "<version>",
        ///                         virtualMachineImageId = "<virtualMachineImageId>",
        ///                     },
        ///                     nodeAgentSKUId = "<nodeAgentSKUId>",
        ///                     windowsConfiguration = new {
        ///                         enableAutomaticUpdates = true,
        ///                     },
        ///                     dataDisks = new[] {
        ///                         new {
        ///                             lun = 1234,
        ///                             caching = "none",
        ///                             diskSizeGB = 1234,
        ///                             storageAccountType = "standard_lrs",
        ///                         }
        ///                     },
        ///                     licenseType = "<licenseType>",
        ///                     containerConfiguration = new {
        ///                         type = "<type>",
        ///                         containerImageNames = new[] {
        ///                             "<String>"
        ///                         },
        ///                         containerRegistries = new[] {
        ///                             new {
        ///                                 username = "<username>",
        ///                                 password = "<password>",
        ///                                 registryServer = "<registryServer>",
        ///                                 identityReference = new {
        ///                                     resourceId = "<resourceId>",
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     diskEncryptionConfiguration = new {
        ///                         targets = new[] {
        ///                             "osdisk"
        ///                         },
        ///                     },
        ///                     nodePlacementConfiguration = new {
        ///                         policy = "regional",
        ///                     },
        ///                     extensions = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             publisher = "<publisher>",
        ///                             type = "<type>",
        ///                             typeHandlerVersion = "<typeHandlerVersion>",
        ///                             autoUpgradeMinorVersion = true,
        ///                             settings = new {},
        ///                             protectedSettings = new {},
        ///                             provisionAfterExtensions = new[] {
        ///                                 "<String>"
        ///                             },
        ///                         }
        ///                     },
        ///                     osDisk = new {
        ///                         ephemeralOSDiskSettings = new {
        ///                             placement = "<placement>",
        ///                         },
        ///                     },
        ///                 },
        ///                 taskSlotsPerNode = 1234,
        ///                 taskSchedulingPolicy = new {
        ///                     nodeFillType = "spread",
        ///                 },
        ///                 resizeTimeout = PT1H23M45S,
        ///                 targetDedicatedNodes = 1234,
        ///                 targetLowPriorityNodes = 1234,
        ///                 enableAutoScale = true,
        ///                 autoScaleFormula = "<autoScaleFormula>",
        ///                 autoScaleEvaluationInterval = PT1H23M45S,
        ///                 enableInterNodeCommunication = true,
        ///                 networkConfiguration = new {
        ///                     subnetId = "<subnetId>",
        ///                     dynamicVNetAssignmentScope = "none",
        ///                     endpointConfiguration = new {
        ///                         inboundNATPools = new[] {
        ///                             new {
        ///                                 name = "<name>",
        ///                                 protocol = "tcp",
        ///                                 backendPort = 1234,
        ///                                 frontendPortRangeStart = 1234,
        ///                                 frontendPortRangeEnd = 1234,
        ///                                 networkSecurityGroupRules = new[] {
        ///                                     new {
        ///                                         priority = 1234,
        ///                                         access = "allow",
        ///                                         sourceAddressPrefix = "<sourceAddressPrefix>",
        ///                                         sourcePortRanges = new[] {
        ///                                             "<String>"
        ///                                         },
        ///                                     }
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     publicIPAddressConfiguration = new {
        ///                         provision = "batchmanaged",
        ///                         ipAddressIds = new[] {
        ///                             "<String>"
        ///                         },
        ///                     },
        ///                 },
        ///                 startTask = new {
        ///                     commandLine = "<commandLine>",
        ///                     containerSettings = new {
        ///                         containerRunOptions = "<containerRunOptions>",
        ///                         imageName = "<imageName>",
        ///                         registry = new {
        ///                             username = "<username>",
        ///                             password = "<password>",
        ///                             registryServer = "<registryServer>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         workingDirectory = "taskWorkingDirectory",
        ///                     },
        ///                     resourceFiles = new[] {
        ///                         new {
        ///                             autoStorageContainerName = "<autoStorageContainerName>",
        ///                             storageContainerUrl = "<storageContainerUrl>",
        ///                             httpUrl = "<httpUrl>",
        ///                             blobPrefix = "<blobPrefix>",
        ///                             filePath = "<filePath>",
        ///                             fileMode = "<fileMode>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         }
        ///                     },
        ///                     environmentSettings = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             value = "<value>",
        ///                         }
        ///                     },
        ///                     userIdentity = new {
        ///                         username = "<username>",
        ///                         autoUser = new {
        ///                             scope = "task",
        ///                             elevationLevel = "nonadmin",
        ///                         },
        ///                     },
        ///                     maxTaskRetryCount = 1234,
        ///                     waitForSuccess = true,
        ///                 },
        ///                 certificateReferences = new[] {
        ///                     new {
        ///                         thumbprint = "<thumbprint>",
        ///                         thumbprintAlgorithm = "<thumbprintAlgorithm>",
        ///                         storeLocation = "currentuser",
        ///                         storeName = "<storeName>",
        ///                         visibility = new[] {
        ///                             "starttask"
        ///                         },
        ///                     }
        ///                 },
        ///                 applicationPackageReferences = new[] {
        ///                     new {
        ///                         applicationId = "<applicationId>",
        ///                         version = "<version>",
        ///                     }
        ///                 },
        ///                 applicationLicenses = new[] {
        ///                     "<String>"
        ///                 },
        ///                 userAccounts = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         password = "<password>",
        ///                         elevationLevel = "nonadmin",
        ///                         linuxUserConfiguration = new {
        ///                             uid = 1234,
        ///                             gid = 1234,
        ///                             sshPrivateKey = "<sshPrivateKey>",
        ///                         },
        ///                         windowsUserConfiguration = new {
        ///                             loginMode = "batch",
        ///                         },
        ///                     }
        ///                 },
        ///                 metadata = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         value = "<value>",
        ///                     }
        ///                 },
        ///                 mountConfiguration = new[] {
        ///                     new {
        ///                         azureBlobFileSystemConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             containerName = "<containerName>",
        ///                             accountKey = "<accountKey>",
        ///                             sasKey = "<sasKey>",
        ///                             blobfuseOptions = "<blobfuseOptions>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         nfsMountConfiguration = new {
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                         cifsMountConfiguration = new {
        ///                             username = "<username>",
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                             password = "<password>",
        ///                         },
        ///                         azureFileShareConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             azureFileUrl = "<azureFileUrl>",
        ///                             accountKey = "<accountKey>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                     }
        ///                 },
        ///             },
        ///         },
        ///     },
        ///     metadata = new[] {
        ///         new {
        ///             name = "<name>",
        ///             value = "<value>",
        ///         }
        ///     },
        /// };
        /// 
        /// Response response = client.Patch("<jobId>", RequestContent.Create(data), 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// This replaces only the Job properties specified in the request. For example, if the Job has constraints, and a request does not specify the constraints element, then the Job keeps the existing constraints.
        /// 
        /// Below is the JSON schema for the request payload.
        /// 
        /// Request Body:
        /// 
        /// Schema for <c>JobUpdate</c>:
        /// <code>{
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. If omitted, the priority of the Job is left unchanged.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. If omitted, the completion behavior is left unchanged. You may not change the value from terminatejob to noaction - that is, once you have engaged automatic Job termination, you cannot turn it off again. If you try to do this, the request fails with an &apos;invalid property value&apos; error response; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. If omitted, the existing execution constraints are left unchanged.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [
        ///               {
        ///                 username: string, # Optional. The user name to log into the registry server.
        ///                 password: string, # Optional. The password to log into the registry server.
        ///                 registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///                 identityReference: {
        ///                   resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///                 }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///               }
        ///             ], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: {
        ///             containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///             imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///             registry: ContainerRegistry, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///             workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///           }, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [
        ///             {
        ///               autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///               storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///               httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///               blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///               filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///               fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///             }
        ///           ], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [
        ///             {
        ///               name: string, # Required. The name of the environment variable.
        ///               value: string, # Optional. The value of the environment variable.
        ///             }
        ///           ], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: {
        ///             username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///             autoUser: {
        ///               scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///               elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///           }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [
        ///           {
        ///             applicationId: string, # Required. The ID of the application to deploy.
        ///             version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///           }
        ///         ], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. You may change the Pool for a Job only when the Job is disabled. The Patch Job call will fail if you include the poolInfo element and the Job is not disabled. If you specify an autoPoolSpecification in the poolInfo, only the keepAlive property of the autoPoolSpecification can be updated, and then only if the autoPoolSpecification has a poolLifetimeOption of Job (other job properties can be updated as normal). If omitted, the Job continues to run on its current Pool.
        ///   metadata: [MetadataItem], # Optional. If omitted, the existing Job metadata is left unchanged.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Response Patch(string jobId, RequestContent content, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Patch");
            scope.Start();
            try
            {
                using HttpMessage message = CreatePatchRequest(jobId, content, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Updates the properties of the specified Job. </summary>
        /// <param name="jobId"> The ID of the Job whose properties you want to update. </param>
        /// <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call UpdateAsync with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {};
        /// 
        /// Response response = await client.UpdateAsync("<jobId>", RequestContent.Create(data));
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call UpdateAsync with all parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     id = "<id>",
        ///     displayName = "<displayName>",
        ///     usesTaskDependencies = true,
        ///     priority = 1234,
        ///     allowTaskPreemption = true,
        ///     maxParallelTasks = 1234,
        ///     constraints = new {
        ///         maxWallClockTime = PT1H23M45S,
        ///         maxTaskRetryCount = 1234,
        ///     },
        ///     jobManagerTask = new {
        ///         id = "<id>",
        ///         displayName = "<displayName>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         outputFiles = new[] {
        ///             new {
        ///                 filePattern = "<filePattern>",
        ///                 destination = new {
        ///                     container = new {
        ///                         path = "<path>",
        ///                         containerUrl = "<containerUrl>",
        ///                         identityReference = new {
        ///                             resourceId = "<resourceId>",
        ///                         },
        ///                         uploadHeaders = new[] {
        ///                             new {
        ///                                 name = "<name>",
        ///                                 value = "<value>",
        ///                             }
        ///                         },
        ///                     },
        ///                 },
        ///                 uploadOptions = new {
        ///                     uploadCondition = "tasksuccess",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         constraints = new {
        ///             maxWallClockTime = PT1H23M45S,
        ///             retentionTime = PT1H23M45S,
        ///             maxTaskRetryCount = 1234,
        ///         },
        ///         requiredSlots = 1234,
        ///         killJobOnCompletion = true,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///         runExclusive = true,
        ///         applicationPackageReferences = new[] {
        ///             new {
        ///                 applicationId = "<applicationId>",
        ///                 version = "<version>",
        ///             }
        ///         },
        ///         authenticationTokenSettings = new {
        ///             access = new[] {
        ///                 "<String>"
        ///             },
        ///         },
        ///         allowLowPriorityNode = true,
        ///     },
        ///     jobPreparationTask = new {
        ///         id = "<id>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         constraints = new {
        ///             maxWallClockTime = PT1H23M45S,
        ///             retentionTime = PT1H23M45S,
        ///             maxTaskRetryCount = 1234,
        ///         },
        ///         waitForSuccess = true,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///         rerunOnNodeRebootAfterSuccess = true,
        ///     },
        ///     jobReleaseTask = new {
        ///         id = "<id>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         maxWallClockTime = PT1H23M45S,
        ///         retentionTime = PT1H23M45S,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///     },
        ///     commonEnvironmentSettings = new[] {
        ///         new {
        ///             name = "<name>",
        ///             value = "<value>",
        ///         }
        ///     },
        ///     poolInfo = new {
        ///         poolId = "<poolId>",
        ///         autoPoolSpecification = new {
        ///             autoPoolIdPrefix = "<autoPoolIdPrefix>",
        ///             poolLifetimeOption = "jobschedule",
        ///             keepAlive = true,
        ///             pool = new {
        ///                 displayName = "<displayName>",
        ///                 vmSize = "<vmSize>",
        ///                 cloudServiceConfiguration = new {
        ///                     osFamily = "<osFamily>",
        ///                     osVersion = "<osVersion>",
        ///                 },
        ///                 virtualMachineConfiguration = new {
        ///                     imageReference = new {
        ///                         publisher = "<publisher>",
        ///                         offer = "<offer>",
        ///                         sku = "<sku>",
        ///                         version = "<version>",
        ///                         virtualMachineImageId = "<virtualMachineImageId>",
        ///                     },
        ///                     nodeAgentSKUId = "<nodeAgentSKUId>",
        ///                     windowsConfiguration = new {
        ///                         enableAutomaticUpdates = true,
        ///                     },
        ///                     dataDisks = new[] {
        ///                         new {
        ///                             lun = 1234,
        ///                             caching = "none",
        ///                             diskSizeGB = 1234,
        ///                             storageAccountType = "standard_lrs",
        ///                         }
        ///                     },
        ///                     licenseType = "<licenseType>",
        ///                     containerConfiguration = new {
        ///                         type = "<type>",
        ///                         containerImageNames = new[] {
        ///                             "<String>"
        ///                         },
        ///                         containerRegistries = new[] {
        ///                             new {
        ///                                 username = "<username>",
        ///                                 password = "<password>",
        ///                                 registryServer = "<registryServer>",
        ///                                 identityReference = new {
        ///                                     resourceId = "<resourceId>",
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     diskEncryptionConfiguration = new {
        ///                         targets = new[] {
        ///                             "osdisk"
        ///                         },
        ///                     },
        ///                     nodePlacementConfiguration = new {
        ///                         policy = "regional",
        ///                     },
        ///                     extensions = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             publisher = "<publisher>",
        ///                             type = "<type>",
        ///                             typeHandlerVersion = "<typeHandlerVersion>",
        ///                             autoUpgradeMinorVersion = true,
        ///                             settings = new {},
        ///                             protectedSettings = new {},
        ///                             provisionAfterExtensions = new[] {
        ///                                 "<String>"
        ///                             },
        ///                         }
        ///                     },
        ///                     osDisk = new {
        ///                         ephemeralOSDiskSettings = new {
        ///                             placement = "<placement>",
        ///                         },
        ///                     },
        ///                 },
        ///                 taskSlotsPerNode = 1234,
        ///                 taskSchedulingPolicy = new {
        ///                     nodeFillType = "spread",
        ///                 },
        ///                 resizeTimeout = PT1H23M45S,
        ///                 targetDedicatedNodes = 1234,
        ///                 targetLowPriorityNodes = 1234,
        ///                 enableAutoScale = true,
        ///                 autoScaleFormula = "<autoScaleFormula>",
        ///                 autoScaleEvaluationInterval = PT1H23M45S,
        ///                 enableInterNodeCommunication = true,
        ///                 networkConfiguration = new {
        ///                     subnetId = "<subnetId>",
        ///                     dynamicVNetAssignmentScope = "none",
        ///                     endpointConfiguration = new {
        ///                         inboundNATPools = new[] {
        ///                             new {
        ///                                 name = "<name>",
        ///                                 protocol = "tcp",
        ///                                 backendPort = 1234,
        ///                                 frontendPortRangeStart = 1234,
        ///                                 frontendPortRangeEnd = 1234,
        ///                                 networkSecurityGroupRules = new[] {
        ///                                     new {
        ///                                         priority = 1234,
        ///                                         access = "allow",
        ///                                         sourceAddressPrefix = "<sourceAddressPrefix>",
        ///                                         sourcePortRanges = new[] {
        ///                                             "<String>"
        ///                                         },
        ///                                     }
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     publicIPAddressConfiguration = new {
        ///                         provision = "batchmanaged",
        ///                         ipAddressIds = new[] {
        ///                             "<String>"
        ///                         },
        ///                     },
        ///                 },
        ///                 startTask = new {
        ///                     commandLine = "<commandLine>",
        ///                     containerSettings = new {
        ///                         containerRunOptions = "<containerRunOptions>",
        ///                         imageName = "<imageName>",
        ///                         registry = new {
        ///                             username = "<username>",
        ///                             password = "<password>",
        ///                             registryServer = "<registryServer>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         workingDirectory = "taskWorkingDirectory",
        ///                     },
        ///                     resourceFiles = new[] {
        ///                         new {
        ///                             autoStorageContainerName = "<autoStorageContainerName>",
        ///                             storageContainerUrl = "<storageContainerUrl>",
        ///                             httpUrl = "<httpUrl>",
        ///                             blobPrefix = "<blobPrefix>",
        ///                             filePath = "<filePath>",
        ///                             fileMode = "<fileMode>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         }
        ///                     },
        ///                     environmentSettings = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             value = "<value>",
        ///                         }
        ///                     },
        ///                     userIdentity = new {
        ///                         username = "<username>",
        ///                         autoUser = new {
        ///                             scope = "task",
        ///                             elevationLevel = "nonadmin",
        ///                         },
        ///                     },
        ///                     maxTaskRetryCount = 1234,
        ///                     waitForSuccess = true,
        ///                 },
        ///                 certificateReferences = new[] {
        ///                     new {
        ///                         thumbprint = "<thumbprint>",
        ///                         thumbprintAlgorithm = "<thumbprintAlgorithm>",
        ///                         storeLocation = "currentuser",
        ///                         storeName = "<storeName>",
        ///                         visibility = new[] {
        ///                             "starttask"
        ///                         },
        ///                     }
        ///                 },
        ///                 applicationPackageReferences = new[] {
        ///                     new {
        ///                         applicationId = "<applicationId>",
        ///                         version = "<version>",
        ///                     }
        ///                 },
        ///                 applicationLicenses = new[] {
        ///                     "<String>"
        ///                 },
        ///                 userAccounts = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         password = "<password>",
        ///                         elevationLevel = "nonadmin",
        ///                         linuxUserConfiguration = new {
        ///                             uid = 1234,
        ///                             gid = 1234,
        ///                             sshPrivateKey = "<sshPrivateKey>",
        ///                         },
        ///                         windowsUserConfiguration = new {
        ///                             loginMode = "batch",
        ///                         },
        ///                     }
        ///                 },
        ///                 metadata = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         value = "<value>",
        ///                     }
        ///                 },
        ///                 mountConfiguration = new[] {
        ///                     new {
        ///                         azureBlobFileSystemConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             containerName = "<containerName>",
        ///                             accountKey = "<accountKey>",
        ///                             sasKey = "<sasKey>",
        ///                             blobfuseOptions = "<blobfuseOptions>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         nfsMountConfiguration = new {
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                         cifsMountConfiguration = new {
        ///                             username = "<username>",
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                             password = "<password>",
        ///                         },
        ///                         azureFileShareConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             azureFileUrl = "<azureFileUrl>",
        ///                             accountKey = "<accountKey>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                     }
        ///                 },
        ///             },
        ///         },
        ///     },
        ///     onAllTasksComplete = "noaction",
        ///     onTaskFailure = "noaction",
        ///     networkConfiguration = new {
        ///         subnetId = "<subnetId>",
        ///     },
        ///     metadata = new[] {
        ///         new {
        ///             name = "<name>",
        ///             value = "<value>",
        ///         }
        ///     },
        /// };
        /// 
        /// Response response = await client.UpdateAsync("<jobId>", RequestContent.Create(data), 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// This fully replaces all the updatable properties of the Job. For example, if the Job has constraints associated with it and if constraints is not specified with this request, then the Batch service will remove the existing constraints.
        /// 
        /// Below is the JSON schema for the request payload.
        /// 
        /// Request Body:
        /// 
        /// Schema for <c>Job</c>:
        /// <code>{
        ///   id: string, # Optional. The ID is case-preserving and case-insensitive (that is, you may not have two IDs within an Account that differ only by case).
        ///   displayName: string, # Optional. The display name for the Job.
        ///   usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is false.
        ///   url: string, # Optional. The URL of the Job.
        ///   eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job has changed between requests. In particular, you can be pass the ETag when updating a Job to specify that your changes should take effect only if nobody else has modified the Job in the meantime.
        ///   lastModified: string (ISO 8601 Format), # Optional. This is the last time at which the Job level data, such as the Job state or priority, changed. It does not factor in task-level changes such as adding new Tasks or Tasks changing state.
        ///   creationTime: string (ISO 8601 Format), # Optional. The creation time of the Job.
        ///   state: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. The state of the Job.
        ///   stateTransitionTime: string (ISO 8601 Format), # Optional. The time at which the Job entered its current state.
        ///   previousState: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. This property is not set if the Job is in its initial Active state.
        ///   previousStateTransitionTime: string (ISO 8601 Format), # Optional. This property is not set if the Job is in its initial Active state.
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. The default value is 0.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. The execution constraints for a Job.
        ///   jobManagerTask: {
        ///     id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters.
        ///     displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: {
        ///       containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///       imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///       registry: {
        ///         username: string, # Optional. The user name to log into the registry server.
        ///         password: string, # Optional. The password to log into the registry server.
        ///         registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///         identityReference: {
        ///           resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///         }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///       workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///     }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must be set as well. If the Pool that will run this Task doesn&apos;t have containerConfiguration set, this must not be set. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [
        ///       {
        ///         autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///         storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///         httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///         blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///         filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///         fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///         identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }
        ///     ], # Optional. Files listed under this element are located in the Task&apos;s working directory. There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     outputFiles: [
        ///       {
        ///         filePattern: string, # Required. Both relative and absolute paths are supported. Relative paths are relative to the Task working directory. The following wildcards are supported: * matches 0 or more characters (for example pattern abc* would match abc or abcdef), ** matches any directory, ? matches any single character, [abc] matches one character in the brackets, and [a-c] matches one character in the range. Brackets can include a negation to match any character not specified (for example [!abc] matches any character but a, b, or c). If a file name starts with &quot;.&quot; it is ignored by default but may be matched by specifying it explicitly (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\*.txt matches any file that does not start in &apos;.&apos; and ends with .txt in the Task working directory or any subdirectory. If the filename contains a wildcard character it can be escaped using brackets (for example abc[*] would match a file named abc*). Note that both \ and / are treated as directory separators on Windows, but only / is on Linux. Environment variables (%var% on Windows or $var on Linux) are expanded prior to the pattern being applied.
        ///         destination: {
        ///           container: {
        ///             path: string, # Optional. If filePattern refers to a specific file (i.e. contains no wildcards), then path is the name of the blob to which to upload that file. If filePattern contains one or more wildcards (and therefore may match multiple files), then path is the name of the blob virtual directory (which is prepended to each blob name) to which to upload the file(s). If omitted, file(s) are uploaded to the root of the container with a blob name matching their file name.
        ///             containerUrl: string, # Required. If not using a managed identity, the URL must include a Shared Access Signature (SAS) granting write permissions to the container.
        ///             identityReference: ComputeNodeIdentityReference, # Optional. The identity must have write access to the Azure Blob Storage container
        ///             uploadHeaders: [
        ///               {
        ///                 name: string, # Required. The case-insensitive name of the header to be used while uploading output files
        ///                 value: string, # Optional. The value of the header to be used while uploading output files
        ///               }
        ///             ], # Optional. These headers will be specified when uploading files to Azure Storage. For more information, see [Request Headers (All Blob Types)](https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types).
        ///           }, # Optional. Specifies a file upload destination within an Azure blob storage container.
        ///         }, # Required. The destination to which a file should be uploaded.
        ///         uploadOptions: {
        ///           uploadCondition: &quot;tasksuccess&quot; | &quot;taskfailure&quot; | &quot;taskcompletion&quot;, # Required. The default is taskcompletion.
        ///         }, # Required. Details about an output file upload operation, including under what conditions to perform the upload.
        ///       }
        ///     ], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node on which the primary Task is executed.
        ///     environmentSettings: [
        ///       {
        ///         name: string, # Required. The name of the environment variable.
        ///         value: string, # Optional. The value of the environment variable.
        ///       }
        ///     ], # Optional. A list of environment variable settings for the Job Manager Task.
        ///     constraints: {
        ///       maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        ///       retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///       maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task executable due to a nonzero exit code. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task after the first attempt. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///     }, # Optional. Execution constraints to apply to a Task.
        ///     requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the node has enough free scheduling slots available. For multi-instance Tasks, this property is not supported and must not be specified.
        ///     killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job as complete. If any Tasks are still running at this time (other than Job Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not affect the Job status. In this case, you should either use the onAllTasksComplete attribute to terminate the Job, or have a client or user terminate the Job explicitly. An example of this is if the Job Manager creates a set of Tasks but then takes no further role in their execution. The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job (not to monitor progress), then it is important to set killJobOnCompletion to false.
        ///     userIdentity: {
        ///       username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///       autoUser: {
        ///         scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///         elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///       }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///     }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///     runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a Compute Node. The Job Manager Task counts normally against the Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The default value is true.
        ///     applicationPackageReferences: [
        ///       {
        ///         applicationId: string, # Required. The ID of the application to deploy.
        ///         version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///       }
        ///     ], # Optional. Application Packages are downloaded and deployed to a shared directory, not the Task working directory. Therefore, if a referenced Application Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute Node is used. If a referenced Application Package cannot be installed, for example because the package has been deleted or because download failed, the Task fails.
        ///     authenticationTokenSettings: {
        ///       access: [string], # Optional. The authentication token grants access to a limited set of Batch service operations. Currently the only supported value for the access property is &apos;job&apos;, which grants access to all operations related to the Job which contains the Task.
        ///     }, # Optional. If this property is set, the Batch service provides the Task with an authentication token which can be used to authenticate Batch service operations without requiring an Account access key. The token is provided via the AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out using the token depend on the settings. For example, a Task can request Job permissions in order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the Job.
        ///     allowLowPriorityNode: boolean, # Optional. The default value is true.
        ///   }, # Optional. The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running &apos;normal&apos; Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
        ///   jobPreparationTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
        ///     constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
        ///     waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute Nodes.
        ///     rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.
        ///   }, # Optional. The Job Preparation Task is a special Task run on each Compute Node before any other Task of the Job.
        ///   jobReleaseTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute Node, measured from the time the Task starts. If the Task does not complete within the time limit, the Batch service terminates it. The default value is 15 minutes. You may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///     retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///   }, # Optional. The Job Release Task is a special Task run at the end of the Job on each Compute Node that has run any other Task of the Job.
        ///   commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by specifying the same setting name with a different value.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [ApplicationPackageReference], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. Specifies how a Job should be assigned to a Pool.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. The default is noaction.
        ///   onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. A Task is considered to have failed if has a failureInfo. A failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry count, or if there was an error starting the Task, for example due to a resource file download error. The default is noaction.
        ///   networkConfiguration: {
        ///     subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. This is of the form /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication from the Azure Batch service. For Pools created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound connections for communications to Azure Storage. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///   }, # Optional. The network configuration for the Job.
        ///   metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///   executionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. This is the time at which the Job was created.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Job is in the completed state.
        ///     poolId: string, # Optional. This element contains the actual Pool where the Job is assigned. When you get Job details from the service, they also contain a poolInfo element, which contains the Pool configuration data from when the Job was added or updated. That poolInfo element may also contain a poolId element. If it does, the two IDs are the same. If it does not, it means the Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        ///     schedulingError: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Job scheduling error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Job scheduling error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional error details related to the scheduling error.
        ///     }, # Optional. This property is not set if there was no error starting the Job.
        ///     terminateReason: string, # Optional. This property is set only if the Job is in the completed state. If the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a schedule, and the schedule terminated. AllTasksComplete - the Job&apos;s onAllTasksComplete attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job&apos;s onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed with an exit condition that specified a jobAction of terminatejob. Any other string is a user-defined reason specified in a call to the &apos;Terminate a Job&apos; operation.
        ///   }, # Optional. Contains information about the execution of a Job in the Azure Batch service.
        ///   stats: {
        ///     url: string, # Required. The URL of the statistics.
        ///     startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///     lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///     userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///     readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///     writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///     readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///     writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///     numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///     numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///     numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///     waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        ///   }, # Optional. This property is populated only if the CloudJob was retrieved with an expand clause including the &apos;stats&apos; attribute; otherwise it is null. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual async Task<Response> UpdateAsync(string jobId, RequestContent content, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Update");
            scope.Start();
            try
            {
                using HttpMessage message = CreateUpdateRequest(jobId, content, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Updates the properties of the specified Job. </summary>
        /// <param name="jobId"> The ID of the Job whose properties you want to update. </param>
        /// <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call Update with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {};
        /// 
        /// Response response = client.Update("<jobId>", RequestContent.Create(data));
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call Update with all parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     id = "<id>",
        ///     displayName = "<displayName>",
        ///     usesTaskDependencies = true,
        ///     priority = 1234,
        ///     allowTaskPreemption = true,
        ///     maxParallelTasks = 1234,
        ///     constraints = new {
        ///         maxWallClockTime = PT1H23M45S,
        ///         maxTaskRetryCount = 1234,
        ///     },
        ///     jobManagerTask = new {
        ///         id = "<id>",
        ///         displayName = "<displayName>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         outputFiles = new[] {
        ///             new {
        ///                 filePattern = "<filePattern>",
        ///                 destination = new {
        ///                     container = new {
        ///                         path = "<path>",
        ///                         containerUrl = "<containerUrl>",
        ///                         identityReference = new {
        ///                             resourceId = "<resourceId>",
        ///                         },
        ///                         uploadHeaders = new[] {
        ///                             new {
        ///                                 name = "<name>",
        ///                                 value = "<value>",
        ///                             }
        ///                         },
        ///                     },
        ///                 },
        ///                 uploadOptions = new {
        ///                     uploadCondition = "tasksuccess",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         constraints = new {
        ///             maxWallClockTime = PT1H23M45S,
        ///             retentionTime = PT1H23M45S,
        ///             maxTaskRetryCount = 1234,
        ///         },
        ///         requiredSlots = 1234,
        ///         killJobOnCompletion = true,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///         runExclusive = true,
        ///         applicationPackageReferences = new[] {
        ///             new {
        ///                 applicationId = "<applicationId>",
        ///                 version = "<version>",
        ///             }
        ///         },
        ///         authenticationTokenSettings = new {
        ///             access = new[] {
        ///                 "<String>"
        ///             },
        ///         },
        ///         allowLowPriorityNode = true,
        ///     },
        ///     jobPreparationTask = new {
        ///         id = "<id>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         constraints = new {
        ///             maxWallClockTime = PT1H23M45S,
        ///             retentionTime = PT1H23M45S,
        ///             maxTaskRetryCount = 1234,
        ///         },
        ///         waitForSuccess = true,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///         rerunOnNodeRebootAfterSuccess = true,
        ///     },
        ///     jobReleaseTask = new {
        ///         id = "<id>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         maxWallClockTime = PT1H23M45S,
        ///         retentionTime = PT1H23M45S,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///     },
        ///     commonEnvironmentSettings = new[] {
        ///         new {
        ///             name = "<name>",
        ///             value = "<value>",
        ///         }
        ///     },
        ///     poolInfo = new {
        ///         poolId = "<poolId>",
        ///         autoPoolSpecification = new {
        ///             autoPoolIdPrefix = "<autoPoolIdPrefix>",
        ///             poolLifetimeOption = "jobschedule",
        ///             keepAlive = true,
        ///             pool = new {
        ///                 displayName = "<displayName>",
        ///                 vmSize = "<vmSize>",
        ///                 cloudServiceConfiguration = new {
        ///                     osFamily = "<osFamily>",
        ///                     osVersion = "<osVersion>",
        ///                 },
        ///                 virtualMachineConfiguration = new {
        ///                     imageReference = new {
        ///                         publisher = "<publisher>",
        ///                         offer = "<offer>",
        ///                         sku = "<sku>",
        ///                         version = "<version>",
        ///                         virtualMachineImageId = "<virtualMachineImageId>",
        ///                     },
        ///                     nodeAgentSKUId = "<nodeAgentSKUId>",
        ///                     windowsConfiguration = new {
        ///                         enableAutomaticUpdates = true,
        ///                     },
        ///                     dataDisks = new[] {
        ///                         new {
        ///                             lun = 1234,
        ///                             caching = "none",
        ///                             diskSizeGB = 1234,
        ///                             storageAccountType = "standard_lrs",
        ///                         }
        ///                     },
        ///                     licenseType = "<licenseType>",
        ///                     containerConfiguration = new {
        ///                         type = "<type>",
        ///                         containerImageNames = new[] {
        ///                             "<String>"
        ///                         },
        ///                         containerRegistries = new[] {
        ///                             new {
        ///                                 username = "<username>",
        ///                                 password = "<password>",
        ///                                 registryServer = "<registryServer>",
        ///                                 identityReference = new {
        ///                                     resourceId = "<resourceId>",
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     diskEncryptionConfiguration = new {
        ///                         targets = new[] {
        ///                             "osdisk"
        ///                         },
        ///                     },
        ///                     nodePlacementConfiguration = new {
        ///                         policy = "regional",
        ///                     },
        ///                     extensions = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             publisher = "<publisher>",
        ///                             type = "<type>",
        ///                             typeHandlerVersion = "<typeHandlerVersion>",
        ///                             autoUpgradeMinorVersion = true,
        ///                             settings = new {},
        ///                             protectedSettings = new {},
        ///                             provisionAfterExtensions = new[] {
        ///                                 "<String>"
        ///                             },
        ///                         }
        ///                     },
        ///                     osDisk = new {
        ///                         ephemeralOSDiskSettings = new {
        ///                             placement = "<placement>",
        ///                         },
        ///                     },
        ///                 },
        ///                 taskSlotsPerNode = 1234,
        ///                 taskSchedulingPolicy = new {
        ///                     nodeFillType = "spread",
        ///                 },
        ///                 resizeTimeout = PT1H23M45S,
        ///                 targetDedicatedNodes = 1234,
        ///                 targetLowPriorityNodes = 1234,
        ///                 enableAutoScale = true,
        ///                 autoScaleFormula = "<autoScaleFormula>",
        ///                 autoScaleEvaluationInterval = PT1H23M45S,
        ///                 enableInterNodeCommunication = true,
        ///                 networkConfiguration = new {
        ///                     subnetId = "<subnetId>",
        ///                     dynamicVNetAssignmentScope = "none",
        ///                     endpointConfiguration = new {
        ///                         inboundNATPools = new[] {
        ///                             new {
        ///                                 name = "<name>",
        ///                                 protocol = "tcp",
        ///                                 backendPort = 1234,
        ///                                 frontendPortRangeStart = 1234,
        ///                                 frontendPortRangeEnd = 1234,
        ///                                 networkSecurityGroupRules = new[] {
        ///                                     new {
        ///                                         priority = 1234,
        ///                                         access = "allow",
        ///                                         sourceAddressPrefix = "<sourceAddressPrefix>",
        ///                                         sourcePortRanges = new[] {
        ///                                             "<String>"
        ///                                         },
        ///                                     }
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     publicIPAddressConfiguration = new {
        ///                         provision = "batchmanaged",
        ///                         ipAddressIds = new[] {
        ///                             "<String>"
        ///                         },
        ///                     },
        ///                 },
        ///                 startTask = new {
        ///                     commandLine = "<commandLine>",
        ///                     containerSettings = new {
        ///                         containerRunOptions = "<containerRunOptions>",
        ///                         imageName = "<imageName>",
        ///                         registry = new {
        ///                             username = "<username>",
        ///                             password = "<password>",
        ///                             registryServer = "<registryServer>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         workingDirectory = "taskWorkingDirectory",
        ///                     },
        ///                     resourceFiles = new[] {
        ///                         new {
        ///                             autoStorageContainerName = "<autoStorageContainerName>",
        ///                             storageContainerUrl = "<storageContainerUrl>",
        ///                             httpUrl = "<httpUrl>",
        ///                             blobPrefix = "<blobPrefix>",
        ///                             filePath = "<filePath>",
        ///                             fileMode = "<fileMode>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         }
        ///                     },
        ///                     environmentSettings = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             value = "<value>",
        ///                         }
        ///                     },
        ///                     userIdentity = new {
        ///                         username = "<username>",
        ///                         autoUser = new {
        ///                             scope = "task",
        ///                             elevationLevel = "nonadmin",
        ///                         },
        ///                     },
        ///                     maxTaskRetryCount = 1234,
        ///                     waitForSuccess = true,
        ///                 },
        ///                 certificateReferences = new[] {
        ///                     new {
        ///                         thumbprint = "<thumbprint>",
        ///                         thumbprintAlgorithm = "<thumbprintAlgorithm>",
        ///                         storeLocation = "currentuser",
        ///                         storeName = "<storeName>",
        ///                         visibility = new[] {
        ///                             "starttask"
        ///                         },
        ///                     }
        ///                 },
        ///                 applicationPackageReferences = new[] {
        ///                     new {
        ///                         applicationId = "<applicationId>",
        ///                         version = "<version>",
        ///                     }
        ///                 },
        ///                 applicationLicenses = new[] {
        ///                     "<String>"
        ///                 },
        ///                 userAccounts = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         password = "<password>",
        ///                         elevationLevel = "nonadmin",
        ///                         linuxUserConfiguration = new {
        ///                             uid = 1234,
        ///                             gid = 1234,
        ///                             sshPrivateKey = "<sshPrivateKey>",
        ///                         },
        ///                         windowsUserConfiguration = new {
        ///                             loginMode = "batch",
        ///                         },
        ///                     }
        ///                 },
        ///                 metadata = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         value = "<value>",
        ///                     }
        ///                 },
        ///                 mountConfiguration = new[] {
        ///                     new {
        ///                         azureBlobFileSystemConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             containerName = "<containerName>",
        ///                             accountKey = "<accountKey>",
        ///                             sasKey = "<sasKey>",
        ///                             blobfuseOptions = "<blobfuseOptions>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         nfsMountConfiguration = new {
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                         cifsMountConfiguration = new {
        ///                             username = "<username>",
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                             password = "<password>",
        ///                         },
        ///                         azureFileShareConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             azureFileUrl = "<azureFileUrl>",
        ///                             accountKey = "<accountKey>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                     }
        ///                 },
        ///             },
        ///         },
        ///     },
        ///     onAllTasksComplete = "noaction",
        ///     onTaskFailure = "noaction",
        ///     networkConfiguration = new {
        ///         subnetId = "<subnetId>",
        ///     },
        ///     metadata = new[] {
        ///         new {
        ///             name = "<name>",
        ///             value = "<value>",
        ///         }
        ///     },
        /// };
        /// 
        /// Response response = client.Update("<jobId>", RequestContent.Create(data), 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// This fully replaces all the updatable properties of the Job. For example, if the Job has constraints associated with it and if constraints is not specified with this request, then the Batch service will remove the existing constraints.
        /// 
        /// Below is the JSON schema for the request payload.
        /// 
        /// Request Body:
        /// 
        /// Schema for <c>Job</c>:
        /// <code>{
        ///   id: string, # Optional. The ID is case-preserving and case-insensitive (that is, you may not have two IDs within an Account that differ only by case).
        ///   displayName: string, # Optional. The display name for the Job.
        ///   usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is false.
        ///   url: string, # Optional. The URL of the Job.
        ///   eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job has changed between requests. In particular, you can be pass the ETag when updating a Job to specify that your changes should take effect only if nobody else has modified the Job in the meantime.
        ///   lastModified: string (ISO 8601 Format), # Optional. This is the last time at which the Job level data, such as the Job state or priority, changed. It does not factor in task-level changes such as adding new Tasks or Tasks changing state.
        ///   creationTime: string (ISO 8601 Format), # Optional. The creation time of the Job.
        ///   state: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. The state of the Job.
        ///   stateTransitionTime: string (ISO 8601 Format), # Optional. The time at which the Job entered its current state.
        ///   previousState: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. This property is not set if the Job is in its initial Active state.
        ///   previousStateTransitionTime: string (ISO 8601 Format), # Optional. This property is not set if the Job is in its initial Active state.
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. The default value is 0.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. The execution constraints for a Job.
        ///   jobManagerTask: {
        ///     id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters.
        ///     displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: {
        ///       containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///       imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///       registry: {
        ///         username: string, # Optional. The user name to log into the registry server.
        ///         password: string, # Optional. The password to log into the registry server.
        ///         registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///         identityReference: {
        ///           resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///         }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///       workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///     }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must be set as well. If the Pool that will run this Task doesn&apos;t have containerConfiguration set, this must not be set. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [
        ///       {
        ///         autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///         storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///         httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///         blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///         filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///         fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///         identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }
        ///     ], # Optional. Files listed under this element are located in the Task&apos;s working directory. There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     outputFiles: [
        ///       {
        ///         filePattern: string, # Required. Both relative and absolute paths are supported. Relative paths are relative to the Task working directory. The following wildcards are supported: * matches 0 or more characters (for example pattern abc* would match abc or abcdef), ** matches any directory, ? matches any single character, [abc] matches one character in the brackets, and [a-c] matches one character in the range. Brackets can include a negation to match any character not specified (for example [!abc] matches any character but a, b, or c). If a file name starts with &quot;.&quot; it is ignored by default but may be matched by specifying it explicitly (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\*.txt matches any file that does not start in &apos;.&apos; and ends with .txt in the Task working directory or any subdirectory. If the filename contains a wildcard character it can be escaped using brackets (for example abc[*] would match a file named abc*). Note that both \ and / are treated as directory separators on Windows, but only / is on Linux. Environment variables (%var% on Windows or $var on Linux) are expanded prior to the pattern being applied.
        ///         destination: {
        ///           container: {
        ///             path: string, # Optional. If filePattern refers to a specific file (i.e. contains no wildcards), then path is the name of the blob to which to upload that file. If filePattern contains one or more wildcards (and therefore may match multiple files), then path is the name of the blob virtual directory (which is prepended to each blob name) to which to upload the file(s). If omitted, file(s) are uploaded to the root of the container with a blob name matching their file name.
        ///             containerUrl: string, # Required. If not using a managed identity, the URL must include a Shared Access Signature (SAS) granting write permissions to the container.
        ///             identityReference: ComputeNodeIdentityReference, # Optional. The identity must have write access to the Azure Blob Storage container
        ///             uploadHeaders: [
        ///               {
        ///                 name: string, # Required. The case-insensitive name of the header to be used while uploading output files
        ///                 value: string, # Optional. The value of the header to be used while uploading output files
        ///               }
        ///             ], # Optional. These headers will be specified when uploading files to Azure Storage. For more information, see [Request Headers (All Blob Types)](https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types).
        ///           }, # Optional. Specifies a file upload destination within an Azure blob storage container.
        ///         }, # Required. The destination to which a file should be uploaded.
        ///         uploadOptions: {
        ///           uploadCondition: &quot;tasksuccess&quot; | &quot;taskfailure&quot; | &quot;taskcompletion&quot;, # Required. The default is taskcompletion.
        ///         }, # Required. Details about an output file upload operation, including under what conditions to perform the upload.
        ///       }
        ///     ], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node on which the primary Task is executed.
        ///     environmentSettings: [
        ///       {
        ///         name: string, # Required. The name of the environment variable.
        ///         value: string, # Optional. The value of the environment variable.
        ///       }
        ///     ], # Optional. A list of environment variable settings for the Job Manager Task.
        ///     constraints: {
        ///       maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        ///       retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///       maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task executable due to a nonzero exit code. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task after the first attempt. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///     }, # Optional. Execution constraints to apply to a Task.
        ///     requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the node has enough free scheduling slots available. For multi-instance Tasks, this property is not supported and must not be specified.
        ///     killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job as complete. If any Tasks are still running at this time (other than Job Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not affect the Job status. In this case, you should either use the onAllTasksComplete attribute to terminate the Job, or have a client or user terminate the Job explicitly. An example of this is if the Job Manager creates a set of Tasks but then takes no further role in their execution. The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job (not to monitor progress), then it is important to set killJobOnCompletion to false.
        ///     userIdentity: {
        ///       username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///       autoUser: {
        ///         scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///         elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///       }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///     }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///     runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a Compute Node. The Job Manager Task counts normally against the Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The default value is true.
        ///     applicationPackageReferences: [
        ///       {
        ///         applicationId: string, # Required. The ID of the application to deploy.
        ///         version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///       }
        ///     ], # Optional. Application Packages are downloaded and deployed to a shared directory, not the Task working directory. Therefore, if a referenced Application Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute Node is used. If a referenced Application Package cannot be installed, for example because the package has been deleted or because download failed, the Task fails.
        ///     authenticationTokenSettings: {
        ///       access: [string], # Optional. The authentication token grants access to a limited set of Batch service operations. Currently the only supported value for the access property is &apos;job&apos;, which grants access to all operations related to the Job which contains the Task.
        ///     }, # Optional. If this property is set, the Batch service provides the Task with an authentication token which can be used to authenticate Batch service operations without requiring an Account access key. The token is provided via the AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out using the token depend on the settings. For example, a Task can request Job permissions in order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the Job.
        ///     allowLowPriorityNode: boolean, # Optional. The default value is true.
        ///   }, # Optional. The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running &apos;normal&apos; Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
        ///   jobPreparationTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
        ///     constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
        ///     waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute Nodes.
        ///     rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.
        ///   }, # Optional. The Job Preparation Task is a special Task run on each Compute Node before any other Task of the Job.
        ///   jobReleaseTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute Node, measured from the time the Task starts. If the Task does not complete within the time limit, the Batch service terminates it. The default value is 15 minutes. You may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///     retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///   }, # Optional. The Job Release Task is a special Task run at the end of the Job on each Compute Node that has run any other Task of the Job.
        ///   commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by specifying the same setting name with a different value.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [ApplicationPackageReference], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. Specifies how a Job should be assigned to a Pool.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. The default is noaction.
        ///   onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. A Task is considered to have failed if has a failureInfo. A failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry count, or if there was an error starting the Task, for example due to a resource file download error. The default is noaction.
        ///   networkConfiguration: {
        ///     subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. This is of the form /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication from the Azure Batch service. For Pools created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound connections for communications to Azure Storage. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///   }, # Optional. The network configuration for the Job.
        ///   metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///   executionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. This is the time at which the Job was created.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Job is in the completed state.
        ///     poolId: string, # Optional. This element contains the actual Pool where the Job is assigned. When you get Job details from the service, they also contain a poolInfo element, which contains the Pool configuration data from when the Job was added or updated. That poolInfo element may also contain a poolId element. If it does, the two IDs are the same. If it does not, it means the Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        ///     schedulingError: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Job scheduling error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Job scheduling error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional error details related to the scheduling error.
        ///     }, # Optional. This property is not set if there was no error starting the Job.
        ///     terminateReason: string, # Optional. This property is set only if the Job is in the completed state. If the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a schedule, and the schedule terminated. AllTasksComplete - the Job&apos;s onAllTasksComplete attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job&apos;s onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed with an exit condition that specified a jobAction of terminatejob. Any other string is a user-defined reason specified in a call to the &apos;Terminate a Job&apos; operation.
        ///   }, # Optional. Contains information about the execution of a Job in the Azure Batch service.
        ///   stats: {
        ///     url: string, # Required. The URL of the statistics.
        ///     startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///     lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///     userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///     readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///     writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///     readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///     writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///     numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///     numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///     numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///     waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        ///   }, # Optional. This property is populated only if the CloudJob was retrieved with an expand clause including the &apos;stats&apos; attribute; otherwise it is null. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Response Update(string jobId, RequestContent content, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Update");
            scope.Start();
            try
            {
                using HttpMessage message = CreateUpdateRequest(jobId, content, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Disables the specified Job, preventing new Tasks from running. </summary>
        /// <param name="jobId"> The ID of the Job to disable. </param>
        /// <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call DisableAsync with required parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     disableTasks = "requeue",
        /// };
        /// 
        /// Response response = await client.DisableAsync("<jobId>", RequestContent.Create(data));
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call DisableAsync with all parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     disableTasks = "requeue",
        /// };
        /// 
        /// Response response = await client.DisableAsync("<jobId>", RequestContent.Create(data), 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// The Batch Service immediately moves the Job to the disabling state. Batch then uses the disableTasks parameter to determine what to do with the currently running Tasks of the Job. The Job remains in the disabling state until the disable operation is completed and all Tasks have been dealt with according to the disableTasks option; the Job then moves to the disabled state. No new Tasks are started under the Job until it moves back to active state. If you try to disable a Job that is in any state other than active, disabling, or disabled, the request fails with status code 409.
        /// 
        /// Below is the JSON schema for the request payload.
        /// 
        /// Request Body:
        /// 
        /// Schema for <c>JobDisableParameter</c>:
        /// <code>{
        ///   disableTasks: &quot;requeue&quot; | &quot;terminate&quot; | &quot;wait&quot;, # Required. What to do with active Tasks associated with the Job.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual async Task<Response> DisableAsync(string jobId, RequestContent content, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Disable");
            scope.Start();
            try
            {
                using HttpMessage message = CreateDisableRequest(jobId, content, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Disables the specified Job, preventing new Tasks from running. </summary>
        /// <param name="jobId"> The ID of the Job to disable. </param>
        /// <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call Disable with required parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     disableTasks = "requeue",
        /// };
        /// 
        /// Response response = client.Disable("<jobId>", RequestContent.Create(data));
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call Disable with all parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     disableTasks = "requeue",
        /// };
        /// 
        /// Response response = client.Disable("<jobId>", RequestContent.Create(data), 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// The Batch Service immediately moves the Job to the disabling state. Batch then uses the disableTasks parameter to determine what to do with the currently running Tasks of the Job. The Job remains in the disabling state until the disable operation is completed and all Tasks have been dealt with according to the disableTasks option; the Job then moves to the disabled state. No new Tasks are started under the Job until it moves back to active state. If you try to disable a Job that is in any state other than active, disabling, or disabled, the request fails with status code 409.
        /// 
        /// Below is the JSON schema for the request payload.
        /// 
        /// Request Body:
        /// 
        /// Schema for <c>JobDisableParameter</c>:
        /// <code>{
        ///   disableTasks: &quot;requeue&quot; | &quot;terminate&quot; | &quot;wait&quot;, # Required. What to do with active Tasks associated with the Job.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Response Disable(string jobId, RequestContent content, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Disable");
            scope.Start();
            try
            {
                using HttpMessage message = CreateDisableRequest(jobId, content, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Enables the specified Job, allowing new Tasks to run. </summary>
        /// <param name="jobId"> The ID of the Job to enable. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call EnableAsync with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = await client.EnableAsync("<jobId>");
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call EnableAsync with all parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = await client.EnableAsync("<jobId>", 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks> When you call this API, the Batch service sets a disabled Job to the enabling state. After the this operation is completed, the Job moves to the active state, and scheduling of new Tasks under the Job resumes. The Batch service does not allow a Task to remain in the active state for more than 180 days. Therefore, if you enable a Job containing active Tasks which were added more than 180 days ago, those Tasks will not run. </remarks>
        public virtual async Task<Response> EnableAsync(string jobId, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Enable");
            scope.Start();
            try
            {
                using HttpMessage message = CreateEnableRequest(jobId, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Enables the specified Job, allowing new Tasks to run. </summary>
        /// <param name="jobId"> The ID of the Job to enable. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call Enable with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = client.Enable("<jobId>");
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call Enable with all parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = client.Enable("<jobId>", 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks> When you call this API, the Batch service sets a disabled Job to the enabling state. After the this operation is completed, the Job moves to the active state, and scheduling of new Tasks under the Job resumes. The Batch service does not allow a Task to remain in the active state for more than 180 days. Therefore, if you enable a Job containing active Tasks which were added more than 180 days ago, those Tasks will not run. </remarks>
        public virtual Response Enable(string jobId, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Enable");
            scope.Start();
            try
            {
                using HttpMessage message = CreateEnableRequest(jobId, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Terminates the specified Job, marking it as completed. </summary>
        /// <param name="jobId"> The ID of the Job to terminate. </param>
        /// <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call TerminateAsync with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {};
        /// 
        /// Response response = await client.TerminateAsync("<jobId>", RequestContent.Create(data));
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call TerminateAsync with all parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     terminateReason = "<terminateReason>",
        /// };
        /// 
        /// Response response = await client.TerminateAsync("<jobId>", RequestContent.Create(data), 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// When a Terminate Job request is received, the Batch service sets the Job to the terminating state. The Batch service then terminates any running Tasks associated with the Job and runs any required Job release Tasks. Then the Job moves into the completed state. If there are any Tasks in the Job in the active state, they will remain in the active state. Once a Job is terminated, new Tasks cannot be added and any remaining active Tasks will not be scheduled.
        /// 
        /// Below is the JSON schema for the request payload.
        /// 
        /// Request Body:
        /// 
        /// Schema for <c>JobTerminateParameter</c>:
        /// <code>{
        ///   terminateReason: string, # Optional. The text you want to appear as the Job&apos;s TerminateReason. The default is &apos;UserTerminate&apos;.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual async Task<Response> TerminateAsync(string jobId, RequestContent content, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Terminate");
            scope.Start();
            try
            {
                using HttpMessage message = CreateTerminateRequest(jobId, content, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Terminates the specified Job, marking it as completed. </summary>
        /// <param name="jobId"> The ID of the Job to terminate. </param>
        /// <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="requestConditions"> The content to send as the request conditions of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call Terminate with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {};
        /// 
        /// Response response = client.Terminate("<jobId>", RequestContent.Create(data));
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call Terminate with all parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     terminateReason = "<terminateReason>",
        /// };
        /// 
        /// Response response = client.Terminate("<jobId>", RequestContent.Create(data), 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow, null);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// When a Terminate Job request is received, the Batch service sets the Job to the terminating state. The Batch service then terminates any running Tasks associated with the Job and runs any required Job release Tasks. Then the Job moves into the completed state. If there are any Tasks in the Job in the active state, they will remain in the active state. Once a Job is terminated, new Tasks cannot be added and any remaining active Tasks will not be scheduled.
        /// 
        /// Below is the JSON schema for the request payload.
        /// 
        /// Request Body:
        /// 
        /// Schema for <c>JobTerminateParameter</c>:
        /// <code>{
        ///   terminateReason: string, # Optional. The text you want to appear as the Job&apos;s TerminateReason. The default is &apos;UserTerminate&apos;.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Response Terminate(string jobId, RequestContent content, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestConditions requestConditions = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Terminate");
            scope.Start();
            try
            {
                using HttpMessage message = CreateTerminateRequest(jobId, content, timeout, clientRequestId, returnClientRequestId, ocpDate, requestConditions, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Adds a Job to the specified Account. </summary>
        /// <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="content"/> is null. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call AddAsync with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {};
        /// 
        /// Response response = await client.AddAsync(RequestContent.Create(data));
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call AddAsync with all parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     id = "<id>",
        ///     displayName = "<displayName>",
        ///     usesTaskDependencies = true,
        ///     priority = 1234,
        ///     allowTaskPreemption = true,
        ///     maxParallelTasks = 1234,
        ///     constraints = new {
        ///         maxWallClockTime = PT1H23M45S,
        ///         maxTaskRetryCount = 1234,
        ///     },
        ///     jobManagerTask = new {
        ///         id = "<id>",
        ///         displayName = "<displayName>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         outputFiles = new[] {
        ///             new {
        ///                 filePattern = "<filePattern>",
        ///                 destination = new {
        ///                     container = new {
        ///                         path = "<path>",
        ///                         containerUrl = "<containerUrl>",
        ///                         identityReference = new {
        ///                             resourceId = "<resourceId>",
        ///                         },
        ///                         uploadHeaders = new[] {
        ///                             new {
        ///                                 name = "<name>",
        ///                                 value = "<value>",
        ///                             }
        ///                         },
        ///                     },
        ///                 },
        ///                 uploadOptions = new {
        ///                     uploadCondition = "tasksuccess",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         constraints = new {
        ///             maxWallClockTime = PT1H23M45S,
        ///             retentionTime = PT1H23M45S,
        ///             maxTaskRetryCount = 1234,
        ///         },
        ///         requiredSlots = 1234,
        ///         killJobOnCompletion = true,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///         runExclusive = true,
        ///         applicationPackageReferences = new[] {
        ///             new {
        ///                 applicationId = "<applicationId>",
        ///                 version = "<version>",
        ///             }
        ///         },
        ///         authenticationTokenSettings = new {
        ///             access = new[] {
        ///                 "<String>"
        ///             },
        ///         },
        ///         allowLowPriorityNode = true,
        ///     },
        ///     jobPreparationTask = new {
        ///         id = "<id>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         constraints = new {
        ///             maxWallClockTime = PT1H23M45S,
        ///             retentionTime = PT1H23M45S,
        ///             maxTaskRetryCount = 1234,
        ///         },
        ///         waitForSuccess = true,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///         rerunOnNodeRebootAfterSuccess = true,
        ///     },
        ///     jobReleaseTask = new {
        ///         id = "<id>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         maxWallClockTime = PT1H23M45S,
        ///         retentionTime = PT1H23M45S,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///     },
        ///     commonEnvironmentSettings = new[] {
        ///         new {
        ///             name = "<name>",
        ///             value = "<value>",
        ///         }
        ///     },
        ///     poolInfo = new {
        ///         poolId = "<poolId>",
        ///         autoPoolSpecification = new {
        ///             autoPoolIdPrefix = "<autoPoolIdPrefix>",
        ///             poolLifetimeOption = "jobschedule",
        ///             keepAlive = true,
        ///             pool = new {
        ///                 displayName = "<displayName>",
        ///                 vmSize = "<vmSize>",
        ///                 cloudServiceConfiguration = new {
        ///                     osFamily = "<osFamily>",
        ///                     osVersion = "<osVersion>",
        ///                 },
        ///                 virtualMachineConfiguration = new {
        ///                     imageReference = new {
        ///                         publisher = "<publisher>",
        ///                         offer = "<offer>",
        ///                         sku = "<sku>",
        ///                         version = "<version>",
        ///                         virtualMachineImageId = "<virtualMachineImageId>",
        ///                     },
        ///                     nodeAgentSKUId = "<nodeAgentSKUId>",
        ///                     windowsConfiguration = new {
        ///                         enableAutomaticUpdates = true,
        ///                     },
        ///                     dataDisks = new[] {
        ///                         new {
        ///                             lun = 1234,
        ///                             caching = "none",
        ///                             diskSizeGB = 1234,
        ///                             storageAccountType = "standard_lrs",
        ///                         }
        ///                     },
        ///                     licenseType = "<licenseType>",
        ///                     containerConfiguration = new {
        ///                         type = "<type>",
        ///                         containerImageNames = new[] {
        ///                             "<String>"
        ///                         },
        ///                         containerRegistries = new[] {
        ///                             new {
        ///                                 username = "<username>",
        ///                                 password = "<password>",
        ///                                 registryServer = "<registryServer>",
        ///                                 identityReference = new {
        ///                                     resourceId = "<resourceId>",
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     diskEncryptionConfiguration = new {
        ///                         targets = new[] {
        ///                             "osdisk"
        ///                         },
        ///                     },
        ///                     nodePlacementConfiguration = new {
        ///                         policy = "regional",
        ///                     },
        ///                     extensions = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             publisher = "<publisher>",
        ///                             type = "<type>",
        ///                             typeHandlerVersion = "<typeHandlerVersion>",
        ///                             autoUpgradeMinorVersion = true,
        ///                             settings = new {},
        ///                             protectedSettings = new {},
        ///                             provisionAfterExtensions = new[] {
        ///                                 "<String>"
        ///                             },
        ///                         }
        ///                     },
        ///                     osDisk = new {
        ///                         ephemeralOSDiskSettings = new {
        ///                             placement = "<placement>",
        ///                         },
        ///                     },
        ///                 },
        ///                 taskSlotsPerNode = 1234,
        ///                 taskSchedulingPolicy = new {
        ///                     nodeFillType = "spread",
        ///                 },
        ///                 resizeTimeout = PT1H23M45S,
        ///                 targetDedicatedNodes = 1234,
        ///                 targetLowPriorityNodes = 1234,
        ///                 enableAutoScale = true,
        ///                 autoScaleFormula = "<autoScaleFormula>",
        ///                 autoScaleEvaluationInterval = PT1H23M45S,
        ///                 enableInterNodeCommunication = true,
        ///                 networkConfiguration = new {
        ///                     subnetId = "<subnetId>",
        ///                     dynamicVNetAssignmentScope = "none",
        ///                     endpointConfiguration = new {
        ///                         inboundNATPools = new[] {
        ///                             new {
        ///                                 name = "<name>",
        ///                                 protocol = "tcp",
        ///                                 backendPort = 1234,
        ///                                 frontendPortRangeStart = 1234,
        ///                                 frontendPortRangeEnd = 1234,
        ///                                 networkSecurityGroupRules = new[] {
        ///                                     new {
        ///                                         priority = 1234,
        ///                                         access = "allow",
        ///                                         sourceAddressPrefix = "<sourceAddressPrefix>",
        ///                                         sourcePortRanges = new[] {
        ///                                             "<String>"
        ///                                         },
        ///                                     }
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     publicIPAddressConfiguration = new {
        ///                         provision = "batchmanaged",
        ///                         ipAddressIds = new[] {
        ///                             "<String>"
        ///                         },
        ///                     },
        ///                 },
        ///                 startTask = new {
        ///                     commandLine = "<commandLine>",
        ///                     containerSettings = new {
        ///                         containerRunOptions = "<containerRunOptions>",
        ///                         imageName = "<imageName>",
        ///                         registry = new {
        ///                             username = "<username>",
        ///                             password = "<password>",
        ///                             registryServer = "<registryServer>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         workingDirectory = "taskWorkingDirectory",
        ///                     },
        ///                     resourceFiles = new[] {
        ///                         new {
        ///                             autoStorageContainerName = "<autoStorageContainerName>",
        ///                             storageContainerUrl = "<storageContainerUrl>",
        ///                             httpUrl = "<httpUrl>",
        ///                             blobPrefix = "<blobPrefix>",
        ///                             filePath = "<filePath>",
        ///                             fileMode = "<fileMode>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         }
        ///                     },
        ///                     environmentSettings = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             value = "<value>",
        ///                         }
        ///                     },
        ///                     userIdentity = new {
        ///                         username = "<username>",
        ///                         autoUser = new {
        ///                             scope = "task",
        ///                             elevationLevel = "nonadmin",
        ///                         },
        ///                     },
        ///                     maxTaskRetryCount = 1234,
        ///                     waitForSuccess = true,
        ///                 },
        ///                 certificateReferences = new[] {
        ///                     new {
        ///                         thumbprint = "<thumbprint>",
        ///                         thumbprintAlgorithm = "<thumbprintAlgorithm>",
        ///                         storeLocation = "currentuser",
        ///                         storeName = "<storeName>",
        ///                         visibility = new[] {
        ///                             "starttask"
        ///                         },
        ///                     }
        ///                 },
        ///                 applicationPackageReferences = new[] {
        ///                     new {
        ///                         applicationId = "<applicationId>",
        ///                         version = "<version>",
        ///                     }
        ///                 },
        ///                 applicationLicenses = new[] {
        ///                     "<String>"
        ///                 },
        ///                 userAccounts = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         password = "<password>",
        ///                         elevationLevel = "nonadmin",
        ///                         linuxUserConfiguration = new {
        ///                             uid = 1234,
        ///                             gid = 1234,
        ///                             sshPrivateKey = "<sshPrivateKey>",
        ///                         },
        ///                         windowsUserConfiguration = new {
        ///                             loginMode = "batch",
        ///                         },
        ///                     }
        ///                 },
        ///                 metadata = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         value = "<value>",
        ///                     }
        ///                 },
        ///                 mountConfiguration = new[] {
        ///                     new {
        ///                         azureBlobFileSystemConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             containerName = "<containerName>",
        ///                             accountKey = "<accountKey>",
        ///                             sasKey = "<sasKey>",
        ///                             blobfuseOptions = "<blobfuseOptions>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         nfsMountConfiguration = new {
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                         cifsMountConfiguration = new {
        ///                             username = "<username>",
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                             password = "<password>",
        ///                         },
        ///                         azureFileShareConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             azureFileUrl = "<azureFileUrl>",
        ///                             accountKey = "<accountKey>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                     }
        ///                 },
        ///             },
        ///         },
        ///     },
        ///     onAllTasksComplete = "noaction",
        ///     onTaskFailure = "noaction",
        ///     networkConfiguration = new {
        ///         subnetId = "<subnetId>",
        ///     },
        ///     metadata = new[] {
        ///         new {
        ///             name = "<name>",
        ///             value = "<value>",
        ///         }
        ///     },
        /// };
        /// 
        /// Response response = await client.AddAsync(RequestContent.Create(data), 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// The Batch service supports two ways to control the work done as part of a Job. In the first approach, the user specifies a Job Manager Task. The Batch service launches this Task when it is ready to start the Job. The Job Manager Task controls all other Tasks that run under this Job, by using the Task APIs. In the second approach, the user directly controls the execution of Tasks under an active Job, by using the Task APIs. Also note: when naming Jobs, avoid including sensitive information such as user names or secret project names. This information may appear in telemetry logs accessible to Microsoft Support engineers.
        /// 
        /// Below is the JSON schema for the request payload.
        /// 
        /// Request Body:
        /// 
        /// Schema for <c>Job</c>:
        /// <code>{
        ///   id: string, # Optional. The ID is case-preserving and case-insensitive (that is, you may not have two IDs within an Account that differ only by case).
        ///   displayName: string, # Optional. The display name for the Job.
        ///   usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is false.
        ///   url: string, # Optional. The URL of the Job.
        ///   eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job has changed between requests. In particular, you can be pass the ETag when updating a Job to specify that your changes should take effect only if nobody else has modified the Job in the meantime.
        ///   lastModified: string (ISO 8601 Format), # Optional. This is the last time at which the Job level data, such as the Job state or priority, changed. It does not factor in task-level changes such as adding new Tasks or Tasks changing state.
        ///   creationTime: string (ISO 8601 Format), # Optional. The creation time of the Job.
        ///   state: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. The state of the Job.
        ///   stateTransitionTime: string (ISO 8601 Format), # Optional. The time at which the Job entered its current state.
        ///   previousState: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. This property is not set if the Job is in its initial Active state.
        ///   previousStateTransitionTime: string (ISO 8601 Format), # Optional. This property is not set if the Job is in its initial Active state.
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. The default value is 0.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. The execution constraints for a Job.
        ///   jobManagerTask: {
        ///     id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters.
        ///     displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: {
        ///       containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///       imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///       registry: {
        ///         username: string, # Optional. The user name to log into the registry server.
        ///         password: string, # Optional. The password to log into the registry server.
        ///         registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///         identityReference: {
        ///           resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///         }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///       workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///     }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must be set as well. If the Pool that will run this Task doesn&apos;t have containerConfiguration set, this must not be set. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [
        ///       {
        ///         autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///         storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///         httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///         blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///         filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///         fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///         identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }
        ///     ], # Optional. Files listed under this element are located in the Task&apos;s working directory. There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     outputFiles: [
        ///       {
        ///         filePattern: string, # Required. Both relative and absolute paths are supported. Relative paths are relative to the Task working directory. The following wildcards are supported: * matches 0 or more characters (for example pattern abc* would match abc or abcdef), ** matches any directory, ? matches any single character, [abc] matches one character in the brackets, and [a-c] matches one character in the range. Brackets can include a negation to match any character not specified (for example [!abc] matches any character but a, b, or c). If a file name starts with &quot;.&quot; it is ignored by default but may be matched by specifying it explicitly (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\*.txt matches any file that does not start in &apos;.&apos; and ends with .txt in the Task working directory or any subdirectory. If the filename contains a wildcard character it can be escaped using brackets (for example abc[*] would match a file named abc*). Note that both \ and / are treated as directory separators on Windows, but only / is on Linux. Environment variables (%var% on Windows or $var on Linux) are expanded prior to the pattern being applied.
        ///         destination: {
        ///           container: {
        ///             path: string, # Optional. If filePattern refers to a specific file (i.e. contains no wildcards), then path is the name of the blob to which to upload that file. If filePattern contains one or more wildcards (and therefore may match multiple files), then path is the name of the blob virtual directory (which is prepended to each blob name) to which to upload the file(s). If omitted, file(s) are uploaded to the root of the container with a blob name matching their file name.
        ///             containerUrl: string, # Required. If not using a managed identity, the URL must include a Shared Access Signature (SAS) granting write permissions to the container.
        ///             identityReference: ComputeNodeIdentityReference, # Optional. The identity must have write access to the Azure Blob Storage container
        ///             uploadHeaders: [
        ///               {
        ///                 name: string, # Required. The case-insensitive name of the header to be used while uploading output files
        ///                 value: string, # Optional. The value of the header to be used while uploading output files
        ///               }
        ///             ], # Optional. These headers will be specified when uploading files to Azure Storage. For more information, see [Request Headers (All Blob Types)](https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types).
        ///           }, # Optional. Specifies a file upload destination within an Azure blob storage container.
        ///         }, # Required. The destination to which a file should be uploaded.
        ///         uploadOptions: {
        ///           uploadCondition: &quot;tasksuccess&quot; | &quot;taskfailure&quot; | &quot;taskcompletion&quot;, # Required. The default is taskcompletion.
        ///         }, # Required. Details about an output file upload operation, including under what conditions to perform the upload.
        ///       }
        ///     ], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node on which the primary Task is executed.
        ///     environmentSettings: [
        ///       {
        ///         name: string, # Required. The name of the environment variable.
        ///         value: string, # Optional. The value of the environment variable.
        ///       }
        ///     ], # Optional. A list of environment variable settings for the Job Manager Task.
        ///     constraints: {
        ///       maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        ///       retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///       maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task executable due to a nonzero exit code. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task after the first attempt. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///     }, # Optional. Execution constraints to apply to a Task.
        ///     requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the node has enough free scheduling slots available. For multi-instance Tasks, this property is not supported and must not be specified.
        ///     killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job as complete. If any Tasks are still running at this time (other than Job Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not affect the Job status. In this case, you should either use the onAllTasksComplete attribute to terminate the Job, or have a client or user terminate the Job explicitly. An example of this is if the Job Manager creates a set of Tasks but then takes no further role in their execution. The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job (not to monitor progress), then it is important to set killJobOnCompletion to false.
        ///     userIdentity: {
        ///       username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///       autoUser: {
        ///         scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///         elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///       }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///     }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///     runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a Compute Node. The Job Manager Task counts normally against the Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The default value is true.
        ///     applicationPackageReferences: [
        ///       {
        ///         applicationId: string, # Required. The ID of the application to deploy.
        ///         version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///       }
        ///     ], # Optional. Application Packages are downloaded and deployed to a shared directory, not the Task working directory. Therefore, if a referenced Application Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute Node is used. If a referenced Application Package cannot be installed, for example because the package has been deleted or because download failed, the Task fails.
        ///     authenticationTokenSettings: {
        ///       access: [string], # Optional. The authentication token grants access to a limited set of Batch service operations. Currently the only supported value for the access property is &apos;job&apos;, which grants access to all operations related to the Job which contains the Task.
        ///     }, # Optional. If this property is set, the Batch service provides the Task with an authentication token which can be used to authenticate Batch service operations without requiring an Account access key. The token is provided via the AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out using the token depend on the settings. For example, a Task can request Job permissions in order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the Job.
        ///     allowLowPriorityNode: boolean, # Optional. The default value is true.
        ///   }, # Optional. The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running &apos;normal&apos; Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
        ///   jobPreparationTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
        ///     constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
        ///     waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute Nodes.
        ///     rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.
        ///   }, # Optional. The Job Preparation Task is a special Task run on each Compute Node before any other Task of the Job.
        ///   jobReleaseTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute Node, measured from the time the Task starts. If the Task does not complete within the time limit, the Batch service terminates it. The default value is 15 minutes. You may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///     retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///   }, # Optional. The Job Release Task is a special Task run at the end of the Job on each Compute Node that has run any other Task of the Job.
        ///   commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by specifying the same setting name with a different value.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [ApplicationPackageReference], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. Specifies how a Job should be assigned to a Pool.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. The default is noaction.
        ///   onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. A Task is considered to have failed if has a failureInfo. A failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry count, or if there was an error starting the Task, for example due to a resource file download error. The default is noaction.
        ///   networkConfiguration: {
        ///     subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. This is of the form /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication from the Azure Batch service. For Pools created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound connections for communications to Azure Storage. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///   }, # Optional. The network configuration for the Job.
        ///   metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///   executionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. This is the time at which the Job was created.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Job is in the completed state.
        ///     poolId: string, # Optional. This element contains the actual Pool where the Job is assigned. When you get Job details from the service, they also contain a poolInfo element, which contains the Pool configuration data from when the Job was added or updated. That poolInfo element may also contain a poolId element. If it does, the two IDs are the same. If it does not, it means the Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        ///     schedulingError: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Job scheduling error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Job scheduling error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional error details related to the scheduling error.
        ///     }, # Optional. This property is not set if there was no error starting the Job.
        ///     terminateReason: string, # Optional. This property is set only if the Job is in the completed state. If the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a schedule, and the schedule terminated. AllTasksComplete - the Job&apos;s onAllTasksComplete attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job&apos;s onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed with an exit condition that specified a jobAction of terminatejob. Any other string is a user-defined reason specified in a call to the &apos;Terminate a Job&apos; operation.
        ///   }, # Optional. Contains information about the execution of a Job in the Azure Batch service.
        ///   stats: {
        ///     url: string, # Required. The URL of the statistics.
        ///     startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///     lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///     userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///     readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///     writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///     readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///     writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///     numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///     numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///     numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///     waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        ///   }, # Optional. This property is populated only if the CloudJob was retrieved with an expand clause including the &apos;stats&apos; attribute; otherwise it is null. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual async Task<Response> AddAsync(RequestContent content, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Add");
            scope.Start();
            try
            {
                using HttpMessage message = CreateAddRequest(content, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Adds a Job to the specified Account. </summary>
        /// <param name="content"> The content to send as the body of the request. Details of the request body schema are in the Remarks section below. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="content"/> is null. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <example>
        /// This sample shows how to call Add with required parameters.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {};
        /// 
        /// Response response = client.Add(RequestContent.Create(data));
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// This sample shows how to call Add with all parameters and request content.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// var data = new {
        ///     id = "<id>",
        ///     displayName = "<displayName>",
        ///     usesTaskDependencies = true,
        ///     priority = 1234,
        ///     allowTaskPreemption = true,
        ///     maxParallelTasks = 1234,
        ///     constraints = new {
        ///         maxWallClockTime = PT1H23M45S,
        ///         maxTaskRetryCount = 1234,
        ///     },
        ///     jobManagerTask = new {
        ///         id = "<id>",
        ///         displayName = "<displayName>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         outputFiles = new[] {
        ///             new {
        ///                 filePattern = "<filePattern>",
        ///                 destination = new {
        ///                     container = new {
        ///                         path = "<path>",
        ///                         containerUrl = "<containerUrl>",
        ///                         identityReference = new {
        ///                             resourceId = "<resourceId>",
        ///                         },
        ///                         uploadHeaders = new[] {
        ///                             new {
        ///                                 name = "<name>",
        ///                                 value = "<value>",
        ///                             }
        ///                         },
        ///                     },
        ///                 },
        ///                 uploadOptions = new {
        ///                     uploadCondition = "tasksuccess",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         constraints = new {
        ///             maxWallClockTime = PT1H23M45S,
        ///             retentionTime = PT1H23M45S,
        ///             maxTaskRetryCount = 1234,
        ///         },
        ///         requiredSlots = 1234,
        ///         killJobOnCompletion = true,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///         runExclusive = true,
        ///         applicationPackageReferences = new[] {
        ///             new {
        ///                 applicationId = "<applicationId>",
        ///                 version = "<version>",
        ///             }
        ///         },
        ///         authenticationTokenSettings = new {
        ///             access = new[] {
        ///                 "<String>"
        ///             },
        ///         },
        ///         allowLowPriorityNode = true,
        ///     },
        ///     jobPreparationTask = new {
        ///         id = "<id>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         constraints = new {
        ///             maxWallClockTime = PT1H23M45S,
        ///             retentionTime = PT1H23M45S,
        ///             maxTaskRetryCount = 1234,
        ///         },
        ///         waitForSuccess = true,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///         rerunOnNodeRebootAfterSuccess = true,
        ///     },
        ///     jobReleaseTask = new {
        ///         id = "<id>",
        ///         commandLine = "<commandLine>",
        ///         containerSettings = new {
        ///             containerRunOptions = "<containerRunOptions>",
        ///             imageName = "<imageName>",
        ///             registry = new {
        ///                 username = "<username>",
        ///                 password = "<password>",
        ///                 registryServer = "<registryServer>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             },
        ///             workingDirectory = "taskWorkingDirectory",
        ///         },
        ///         resourceFiles = new[] {
        ///             new {
        ///                 autoStorageContainerName = "<autoStorageContainerName>",
        ///                 storageContainerUrl = "<storageContainerUrl>",
        ///                 httpUrl = "<httpUrl>",
        ///                 blobPrefix = "<blobPrefix>",
        ///                 filePath = "<filePath>",
        ///                 fileMode = "<fileMode>",
        ///                 identityReference = new {
        ///                     resourceId = "<resourceId>",
        ///                 },
        ///             }
        ///         },
        ///         environmentSettings = new[] {
        ///             new {
        ///                 name = "<name>",
        ///                 value = "<value>",
        ///             }
        ///         },
        ///         maxWallClockTime = PT1H23M45S,
        ///         retentionTime = PT1H23M45S,
        ///         userIdentity = new {
        ///             username = "<username>",
        ///             autoUser = new {
        ///                 scope = "task",
        ///                 elevationLevel = "nonadmin",
        ///             },
        ///         },
        ///     },
        ///     commonEnvironmentSettings = new[] {
        ///         new {
        ///             name = "<name>",
        ///             value = "<value>",
        ///         }
        ///     },
        ///     poolInfo = new {
        ///         poolId = "<poolId>",
        ///         autoPoolSpecification = new {
        ///             autoPoolIdPrefix = "<autoPoolIdPrefix>",
        ///             poolLifetimeOption = "jobschedule",
        ///             keepAlive = true,
        ///             pool = new {
        ///                 displayName = "<displayName>",
        ///                 vmSize = "<vmSize>",
        ///                 cloudServiceConfiguration = new {
        ///                     osFamily = "<osFamily>",
        ///                     osVersion = "<osVersion>",
        ///                 },
        ///                 virtualMachineConfiguration = new {
        ///                     imageReference = new {
        ///                         publisher = "<publisher>",
        ///                         offer = "<offer>",
        ///                         sku = "<sku>",
        ///                         version = "<version>",
        ///                         virtualMachineImageId = "<virtualMachineImageId>",
        ///                     },
        ///                     nodeAgentSKUId = "<nodeAgentSKUId>",
        ///                     windowsConfiguration = new {
        ///                         enableAutomaticUpdates = true,
        ///                     },
        ///                     dataDisks = new[] {
        ///                         new {
        ///                             lun = 1234,
        ///                             caching = "none",
        ///                             diskSizeGB = 1234,
        ///                             storageAccountType = "standard_lrs",
        ///                         }
        ///                     },
        ///                     licenseType = "<licenseType>",
        ///                     containerConfiguration = new {
        ///                         type = "<type>",
        ///                         containerImageNames = new[] {
        ///                             "<String>"
        ///                         },
        ///                         containerRegistries = new[] {
        ///                             new {
        ///                                 username = "<username>",
        ///                                 password = "<password>",
        ///                                 registryServer = "<registryServer>",
        ///                                 identityReference = new {
        ///                                     resourceId = "<resourceId>",
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     diskEncryptionConfiguration = new {
        ///                         targets = new[] {
        ///                             "osdisk"
        ///                         },
        ///                     },
        ///                     nodePlacementConfiguration = new {
        ///                         policy = "regional",
        ///                     },
        ///                     extensions = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             publisher = "<publisher>",
        ///                             type = "<type>",
        ///                             typeHandlerVersion = "<typeHandlerVersion>",
        ///                             autoUpgradeMinorVersion = true,
        ///                             settings = new {},
        ///                             protectedSettings = new {},
        ///                             provisionAfterExtensions = new[] {
        ///                                 "<String>"
        ///                             },
        ///                         }
        ///                     },
        ///                     osDisk = new {
        ///                         ephemeralOSDiskSettings = new {
        ///                             placement = "<placement>",
        ///                         },
        ///                     },
        ///                 },
        ///                 taskSlotsPerNode = 1234,
        ///                 taskSchedulingPolicy = new {
        ///                     nodeFillType = "spread",
        ///                 },
        ///                 resizeTimeout = PT1H23M45S,
        ///                 targetDedicatedNodes = 1234,
        ///                 targetLowPriorityNodes = 1234,
        ///                 enableAutoScale = true,
        ///                 autoScaleFormula = "<autoScaleFormula>",
        ///                 autoScaleEvaluationInterval = PT1H23M45S,
        ///                 enableInterNodeCommunication = true,
        ///                 networkConfiguration = new {
        ///                     subnetId = "<subnetId>",
        ///                     dynamicVNetAssignmentScope = "none",
        ///                     endpointConfiguration = new {
        ///                         inboundNATPools = new[] {
        ///                             new {
        ///                                 name = "<name>",
        ///                                 protocol = "tcp",
        ///                                 backendPort = 1234,
        ///                                 frontendPortRangeStart = 1234,
        ///                                 frontendPortRangeEnd = 1234,
        ///                                 networkSecurityGroupRules = new[] {
        ///                                     new {
        ///                                         priority = 1234,
        ///                                         access = "allow",
        ///                                         sourceAddressPrefix = "<sourceAddressPrefix>",
        ///                                         sourcePortRanges = new[] {
        ///                                             "<String>"
        ///                                         },
        ///                                     }
        ///                                 },
        ///                             }
        ///                         },
        ///                     },
        ///                     publicIPAddressConfiguration = new {
        ///                         provision = "batchmanaged",
        ///                         ipAddressIds = new[] {
        ///                             "<String>"
        ///                         },
        ///                     },
        ///                 },
        ///                 startTask = new {
        ///                     commandLine = "<commandLine>",
        ///                     containerSettings = new {
        ///                         containerRunOptions = "<containerRunOptions>",
        ///                         imageName = "<imageName>",
        ///                         registry = new {
        ///                             username = "<username>",
        ///                             password = "<password>",
        ///                             registryServer = "<registryServer>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         workingDirectory = "taskWorkingDirectory",
        ///                     },
        ///                     resourceFiles = new[] {
        ///                         new {
        ///                             autoStorageContainerName = "<autoStorageContainerName>",
        ///                             storageContainerUrl = "<storageContainerUrl>",
        ///                             httpUrl = "<httpUrl>",
        ///                             blobPrefix = "<blobPrefix>",
        ///                             filePath = "<filePath>",
        ///                             fileMode = "<fileMode>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         }
        ///                     },
        ///                     environmentSettings = new[] {
        ///                         new {
        ///                             name = "<name>",
        ///                             value = "<value>",
        ///                         }
        ///                     },
        ///                     userIdentity = new {
        ///                         username = "<username>",
        ///                         autoUser = new {
        ///                             scope = "task",
        ///                             elevationLevel = "nonadmin",
        ///                         },
        ///                     },
        ///                     maxTaskRetryCount = 1234,
        ///                     waitForSuccess = true,
        ///                 },
        ///                 certificateReferences = new[] {
        ///                     new {
        ///                         thumbprint = "<thumbprint>",
        ///                         thumbprintAlgorithm = "<thumbprintAlgorithm>",
        ///                         storeLocation = "currentuser",
        ///                         storeName = "<storeName>",
        ///                         visibility = new[] {
        ///                             "starttask"
        ///                         },
        ///                     }
        ///                 },
        ///                 applicationPackageReferences = new[] {
        ///                     new {
        ///                         applicationId = "<applicationId>",
        ///                         version = "<version>",
        ///                     }
        ///                 },
        ///                 applicationLicenses = new[] {
        ///                     "<String>"
        ///                 },
        ///                 userAccounts = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         password = "<password>",
        ///                         elevationLevel = "nonadmin",
        ///                         linuxUserConfiguration = new {
        ///                             uid = 1234,
        ///                             gid = 1234,
        ///                             sshPrivateKey = "<sshPrivateKey>",
        ///                         },
        ///                         windowsUserConfiguration = new {
        ///                             loginMode = "batch",
        ///                         },
        ///                     }
        ///                 },
        ///                 metadata = new[] {
        ///                     new {
        ///                         name = "<name>",
        ///                         value = "<value>",
        ///                     }
        ///                 },
        ///                 mountConfiguration = new[] {
        ///                     new {
        ///                         azureBlobFileSystemConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             containerName = "<containerName>",
        ///                             accountKey = "<accountKey>",
        ///                             sasKey = "<sasKey>",
        ///                             blobfuseOptions = "<blobfuseOptions>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             identityReference = new {
        ///                                 resourceId = "<resourceId>",
        ///                             },
        ///                         },
        ///                         nfsMountConfiguration = new {
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                         cifsMountConfiguration = new {
        ///                             username = "<username>",
        ///                             source = "<source>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                             password = "<password>",
        ///                         },
        ///                         azureFileShareConfiguration = new {
        ///                             accountName = "<accountName>",
        ///                             azureFileUrl = "<azureFileUrl>",
        ///                             accountKey = "<accountKey>",
        ///                             relativeMountPath = "<relativeMountPath>",
        ///                             mountOptions = "<mountOptions>",
        ///                         },
        ///                     }
        ///                 },
        ///             },
        ///         },
        ///     },
        ///     onAllTasksComplete = "noaction",
        ///     onTaskFailure = "noaction",
        ///     networkConfiguration = new {
        ///         subnetId = "<subnetId>",
        ///     },
        ///     metadata = new[] {
        ///         new {
        ///             name = "<name>",
        ///             value = "<value>",
        ///         }
        ///     },
        /// };
        /// 
        /// Response response = client.Add(RequestContent.Create(data), 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow);
        /// Console.WriteLine(response.Status);
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// The Batch service supports two ways to control the work done as part of a Job. In the first approach, the user specifies a Job Manager Task. The Batch service launches this Task when it is ready to start the Job. The Job Manager Task controls all other Tasks that run under this Job, by using the Task APIs. In the second approach, the user directly controls the execution of Tasks under an active Job, by using the Task APIs. Also note: when naming Jobs, avoid including sensitive information such as user names or secret project names. This information may appear in telemetry logs accessible to Microsoft Support engineers.
        /// 
        /// Below is the JSON schema for the request payload.
        /// 
        /// Request Body:
        /// 
        /// Schema for <c>Job</c>:
        /// <code>{
        ///   id: string, # Optional. The ID is case-preserving and case-insensitive (that is, you may not have two IDs within an Account that differ only by case).
        ///   displayName: string, # Optional. The display name for the Job.
        ///   usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is false.
        ///   url: string, # Optional. The URL of the Job.
        ///   eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job has changed between requests. In particular, you can be pass the ETag when updating a Job to specify that your changes should take effect only if nobody else has modified the Job in the meantime.
        ///   lastModified: string (ISO 8601 Format), # Optional. This is the last time at which the Job level data, such as the Job state or priority, changed. It does not factor in task-level changes such as adding new Tasks or Tasks changing state.
        ///   creationTime: string (ISO 8601 Format), # Optional. The creation time of the Job.
        ///   state: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. The state of the Job.
        ///   stateTransitionTime: string (ISO 8601 Format), # Optional. The time at which the Job entered its current state.
        ///   previousState: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. This property is not set if the Job is in its initial Active state.
        ///   previousStateTransitionTime: string (ISO 8601 Format), # Optional. This property is not set if the Job is in its initial Active state.
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. The default value is 0.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. The execution constraints for a Job.
        ///   jobManagerTask: {
        ///     id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters.
        ///     displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: {
        ///       containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///       imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///       registry: {
        ///         username: string, # Optional. The user name to log into the registry server.
        ///         password: string, # Optional. The password to log into the registry server.
        ///         registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///         identityReference: {
        ///           resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///         }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///       workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///     }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must be set as well. If the Pool that will run this Task doesn&apos;t have containerConfiguration set, this must not be set. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [
        ///       {
        ///         autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///         storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///         httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///         blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///         filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///         fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///         identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }
        ///     ], # Optional. Files listed under this element are located in the Task&apos;s working directory. There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     outputFiles: [
        ///       {
        ///         filePattern: string, # Required. Both relative and absolute paths are supported. Relative paths are relative to the Task working directory. The following wildcards are supported: * matches 0 or more characters (for example pattern abc* would match abc or abcdef), ** matches any directory, ? matches any single character, [abc] matches one character in the brackets, and [a-c] matches one character in the range. Brackets can include a negation to match any character not specified (for example [!abc] matches any character but a, b, or c). If a file name starts with &quot;.&quot; it is ignored by default but may be matched by specifying it explicitly (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\*.txt matches any file that does not start in &apos;.&apos; and ends with .txt in the Task working directory or any subdirectory. If the filename contains a wildcard character it can be escaped using brackets (for example abc[*] would match a file named abc*). Note that both \ and / are treated as directory separators on Windows, but only / is on Linux. Environment variables (%var% on Windows or $var on Linux) are expanded prior to the pattern being applied.
        ///         destination: {
        ///           container: {
        ///             path: string, # Optional. If filePattern refers to a specific file (i.e. contains no wildcards), then path is the name of the blob to which to upload that file. If filePattern contains one or more wildcards (and therefore may match multiple files), then path is the name of the blob virtual directory (which is prepended to each blob name) to which to upload the file(s). If omitted, file(s) are uploaded to the root of the container with a blob name matching their file name.
        ///             containerUrl: string, # Required. If not using a managed identity, the URL must include a Shared Access Signature (SAS) granting write permissions to the container.
        ///             identityReference: ComputeNodeIdentityReference, # Optional. The identity must have write access to the Azure Blob Storage container
        ///             uploadHeaders: [
        ///               {
        ///                 name: string, # Required. The case-insensitive name of the header to be used while uploading output files
        ///                 value: string, # Optional. The value of the header to be used while uploading output files
        ///               }
        ///             ], # Optional. These headers will be specified when uploading files to Azure Storage. For more information, see [Request Headers (All Blob Types)](https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types).
        ///           }, # Optional. Specifies a file upload destination within an Azure blob storage container.
        ///         }, # Required. The destination to which a file should be uploaded.
        ///         uploadOptions: {
        ///           uploadCondition: &quot;tasksuccess&quot; | &quot;taskfailure&quot; | &quot;taskcompletion&quot;, # Required. The default is taskcompletion.
        ///         }, # Required. Details about an output file upload operation, including under what conditions to perform the upload.
        ///       }
        ///     ], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node on which the primary Task is executed.
        ///     environmentSettings: [
        ///       {
        ///         name: string, # Required. The name of the environment variable.
        ///         value: string, # Optional. The value of the environment variable.
        ///       }
        ///     ], # Optional. A list of environment variable settings for the Job Manager Task.
        ///     constraints: {
        ///       maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        ///       retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///       maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task executable due to a nonzero exit code. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task after the first attempt. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///     }, # Optional. Execution constraints to apply to a Task.
        ///     requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the node has enough free scheduling slots available. For multi-instance Tasks, this property is not supported and must not be specified.
        ///     killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job as complete. If any Tasks are still running at this time (other than Job Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not affect the Job status. In this case, you should either use the onAllTasksComplete attribute to terminate the Job, or have a client or user terminate the Job explicitly. An example of this is if the Job Manager creates a set of Tasks but then takes no further role in their execution. The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job (not to monitor progress), then it is important to set killJobOnCompletion to false.
        ///     userIdentity: {
        ///       username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///       autoUser: {
        ///         scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///         elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///       }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///     }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///     runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a Compute Node. The Job Manager Task counts normally against the Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The default value is true.
        ///     applicationPackageReferences: [
        ///       {
        ///         applicationId: string, # Required. The ID of the application to deploy.
        ///         version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///       }
        ///     ], # Optional. Application Packages are downloaded and deployed to a shared directory, not the Task working directory. Therefore, if a referenced Application Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute Node is used. If a referenced Application Package cannot be installed, for example because the package has been deleted or because download failed, the Task fails.
        ///     authenticationTokenSettings: {
        ///       access: [string], # Optional. The authentication token grants access to a limited set of Batch service operations. Currently the only supported value for the access property is &apos;job&apos;, which grants access to all operations related to the Job which contains the Task.
        ///     }, # Optional. If this property is set, the Batch service provides the Task with an authentication token which can be used to authenticate Batch service operations without requiring an Account access key. The token is provided via the AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out using the token depend on the settings. For example, a Task can request Job permissions in order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the Job.
        ///     allowLowPriorityNode: boolean, # Optional. The default value is true.
        ///   }, # Optional. The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running &apos;normal&apos; Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
        ///   jobPreparationTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
        ///     constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
        ///     waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute Nodes.
        ///     rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.
        ///   }, # Optional. The Job Preparation Task is a special Task run on each Compute Node before any other Task of the Job.
        ///   jobReleaseTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute Node, measured from the time the Task starts. If the Task does not complete within the time limit, the Batch service terminates it. The default value is 15 minutes. You may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///     retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///   }, # Optional. The Job Release Task is a special Task run at the end of the Job on each Compute Node that has run any other Task of the Job.
        ///   commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by specifying the same setting name with a different value.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [ApplicationPackageReference], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. Specifies how a Job should be assigned to a Pool.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. The default is noaction.
        ///   onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. A Task is considered to have failed if has a failureInfo. A failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry count, or if there was an error starting the Task, for example due to a resource file download error. The default is noaction.
        ///   networkConfiguration: {
        ///     subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. This is of the form /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication from the Azure Batch service. For Pools created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound connections for communications to Azure Storage. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///   }, # Optional. The network configuration for the Job.
        ///   metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///   executionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. This is the time at which the Job was created.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Job is in the completed state.
        ///     poolId: string, # Optional. This element contains the actual Pool where the Job is assigned. When you get Job details from the service, they also contain a poolInfo element, which contains the Pool configuration data from when the Job was added or updated. That poolInfo element may also contain a poolId element. If it does, the two IDs are the same. If it does not, it means the Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        ///     schedulingError: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Job scheduling error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Job scheduling error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional error details related to the scheduling error.
        ///     }, # Optional. This property is not set if there was no error starting the Job.
        ///     terminateReason: string, # Optional. This property is set only if the Job is in the completed state. If the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a schedule, and the schedule terminated. AllTasksComplete - the Job&apos;s onAllTasksComplete attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job&apos;s onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed with an exit condition that specified a jobAction of terminatejob. Any other string is a user-defined reason specified in a call to the &apos;Terminate a Job&apos; operation.
        ///   }, # Optional. Contains information about the execution of a Job in the Azure Batch service.
        ///   stats: {
        ///     url: string, # Required. The URL of the statistics.
        ///     startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///     lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///     userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///     readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///     writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///     readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///     writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///     numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///     numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///     numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///     waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        ///   }, # Optional. This property is populated only if the CloudJob was retrieved with an expand clause including the &apos;stats&apos; attribute; otherwise it is null. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Response Add(RequestContent content, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateScope("JobClient.Add");
            scope.Start();
            try
            {
                using HttpMessage message = CreateAddRequest(content, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Gets the Task counts for the specified Job. </summary>
        /// <param name="jobId"> The ID of the Job. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. Details of the response body schema are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetTaskCountsAsync with required parameters and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = await client.GetTaskCountsAsync("<jobId>");
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("active").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("running").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("completed").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("succeeded").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("failed").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("active").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("running").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("completed").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("succeeded").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("failed").ToString());
        /// ]]></code>
        /// This sample shows how to call GetTaskCountsAsync with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = await client.GetTaskCountsAsync("<jobId>", 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow);
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("active").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("running").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("completed").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("succeeded").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("failed").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("active").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("running").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("completed").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("succeeded").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("failed").ToString());
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// Task counts provide a count of the Tasks by active, running or completed Task state, and a count of Tasks which succeeded or failed. Tasks in the preparing state are counted as running. Note that the numbers returned may not always be up to date. If you need exact task counts, use a list query.
        /// 
        /// Below is the JSON schema for the response payload.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>TaskCountsResult</c>:
        /// <code>{
        ///   taskCounts: {
        ///     active: number, # Required. The number of Tasks in the active state.
        ///     running: number, # Required. The number of Tasks in the running or preparing state.
        ///     completed: number, # Required. The number of Tasks in the completed state.
        ///     succeeded: number, # Required. The number of Tasks which succeeded. A Task succeeds if its result (found in the executionInfo property) is &apos;success&apos;.
        ///     failed: number, # Required. The number of Tasks which failed. A Task fails if its result (found in the executionInfo property) is &apos;failure&apos;.
        ///   }, # Required. The Task counts for a Job.
        ///   taskSlotCounts: {
        ///     active: number, # Required. The number of TaskSlots for active Tasks.
        ///     running: number, # Required. The number of TaskSlots for running Tasks.
        ///     completed: number, # Required. The number of TaskSlots for completed Tasks.
        ///     succeeded: number, # Required. The number of TaskSlots for succeeded Tasks.
        ///     failed: number, # Required. The number of TaskSlots for failed Tasks.
        ///   }, # Required. The TaskSlot counts for a Job.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual async Task<Response> GetTaskCountsAsync(string jobId, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            using var scope = ClientDiagnostics.CreateScope("JobClient.GetTaskCounts");
            scope.Start();
            try
            {
                using HttpMessage message = CreateGetTaskCountsRequest(jobId, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Gets the Task counts for the specified Job. </summary>
        /// <param name="jobId"> The ID of the Job. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. Details of the response body schema are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetTaskCounts with required parameters and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = client.GetTaskCounts("<jobId>");
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("active").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("running").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("completed").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("succeeded").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("failed").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("active").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("running").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("completed").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("succeeded").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("failed").ToString());
        /// ]]></code>
        /// This sample shows how to call GetTaskCounts with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// Response response = client.GetTaskCounts("<jobId>", 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow);
        /// 
        /// JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("active").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("running").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("completed").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("succeeded").ToString());
        /// Console.WriteLine(result.GetProperty("taskCounts").GetProperty("failed").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("active").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("running").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("completed").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("succeeded").ToString());
        /// Console.WriteLine(result.GetProperty("taskSlotCounts").GetProperty("failed").ToString());
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// Task counts provide a count of the Tasks by active, running or completed Task state, and a count of Tasks which succeeded or failed. Tasks in the preparing state are counted as running. Note that the numbers returned may not always be up to date. If you need exact task counts, use a list query.
        /// 
        /// Below is the JSON schema for the response payload.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>TaskCountsResult</c>:
        /// <code>{
        ///   taskCounts: {
        ///     active: number, # Required. The number of Tasks in the active state.
        ///     running: number, # Required. The number of Tasks in the running or preparing state.
        ///     completed: number, # Required. The number of Tasks in the completed state.
        ///     succeeded: number, # Required. The number of Tasks which succeeded. A Task succeeds if its result (found in the executionInfo property) is &apos;success&apos;.
        ///     failed: number, # Required. The number of Tasks which failed. A Task fails if its result (found in the executionInfo property) is &apos;failure&apos;.
        ///   }, # Required. The Task counts for a Job.
        ///   taskSlotCounts: {
        ///     active: number, # Required. The number of TaskSlots for active Tasks.
        ///     running: number, # Required. The number of TaskSlots for running Tasks.
        ///     completed: number, # Required. The number of TaskSlots for completed Tasks.
        ///     succeeded: number, # Required. The number of TaskSlots for succeeded Tasks.
        ///     failed: number, # Required. The number of TaskSlots for failed Tasks.
        ///   }, # Required. The TaskSlot counts for a Job.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Response GetTaskCounts(string jobId, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            using var scope = ClientDiagnostics.CreateScope("JobClient.GetTaskCounts");
            scope.Start();
            try
            {
                using HttpMessage message = CreateGetTaskCountsRequest(jobId, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Lists all of the Jobs in the specified Account. </summary>
        /// <param name="filter"> An OData $filter clause. For more information on constructing this filter, see https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-jobs. </param>
        /// <param name="select"> An OData $select clause. </param>
        /// <param name="expand"> An OData $expand clause. </param>
        /// <param name="maxResults"> The maximum number of items to return in the response. A maximum of 1000 Jobs can be returned. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The <see cref="AsyncPageable{T}"/> from the service containing a list of <see cref="BinaryData"/> objects. Details of the body schema for each item in the collection are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetJobsAsync and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// await foreach (var data in client.GetJobsAsync())
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.ToString());
        /// }
        /// ]]></code>
        /// This sample shows how to call GetJobsAsync with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// await foreach (var data in client.GetJobsAsync("<filter>", "<select>", "<expand>", 1234, 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow))
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("usesTaskDependencies").ToString());
        ///     Console.WriteLine(result.GetProperty("url").ToString());
        ///     Console.WriteLine(result.GetProperty("eTag").ToString());
        ///     Console.WriteLine(result.GetProperty("lastModified").ToString());
        ///     Console.WriteLine(result.GetProperty("creationTime").ToString());
        ///     Console.WriteLine(result.GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("stateTransitionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("previousState").ToString());
        ///     Console.WriteLine(result.GetProperty("previousStateTransitionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("priority").ToString());
        ///     Console.WriteLine(result.GetProperty("allowTaskPreemption").ToString());
        ///     Console.WriteLine(result.GetProperty("maxParallelTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("filePattern").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("path").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("containerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("uploadOptions").GetProperty("uploadCondition").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("requiredSlots").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("killJobOnCompletion").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("runExclusive").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("authenticationTokenSettings").GetProperty("access")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("allowLowPriorityNode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("waitForSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("rerunOnNodeRebootAfterSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("poolId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("autoPoolIdPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("poolLifetimeOption").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("keepAlive").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("vmSize").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osFamily").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("publisher").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("offer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("sku").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("virtualMachineImageId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("exactVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodeAgentSKUId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("windowsConfiguration").GetProperty("enableAutomaticUpdates").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("lun").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("caching").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("diskSizeGB").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("storageAccountType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("licenseType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("type").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerImageNames")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("diskEncryptionConfiguration").GetProperty("targets")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodePlacementConfiguration").GetProperty("policy").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("publisher").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("type").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("typeHandlerVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("autoUpgradeMinorVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("settings").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("protectedSettings").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("provisionAfterExtensions")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("osDisk").GetProperty("ephemeralOSDiskSettings").GetProperty("placement").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSlotsPerNode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSchedulingPolicy").GetProperty("nodeFillType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("resizeTimeout").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetDedicatedNodes").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetLowPriorityNodes").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableAutoScale").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleFormula").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleEvaluationInterval").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableInterNodeCommunication").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("dynamicVNetAssignmentScope").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("protocol").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("backendPort").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeStart").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeEnd").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("priority").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("access").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourceAddressPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourcePortRanges")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("provision").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("ipAddressIds")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("waitForSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprint").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprintAlgorithm").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeLocation").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("visibility")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationLicenses")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("uid").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("gid").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("sshPrivateKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("windowsUserConfiguration").GetProperty("loginMode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("containerName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("sasKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("blobfuseOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("source").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("source").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("azureFileUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("onAllTasksComplete").ToString());
        ///     Console.WriteLine(result.GetProperty("onTaskFailure").ToString());
        ///     Console.WriteLine(result.GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        ///     Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("endTime").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("poolId").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("category").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("code").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("message").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("terminateReason").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("url").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("lastUpdateTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("userCPUTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("kernelCPUTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("wallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("readIOps").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOps").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("readIOGiB").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOGiB").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numSucceededTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numFailedTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numTaskRetries").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("waitTime").ToString());
        /// }
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// Below is the JSON schema for one item in the pageable response.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>JobListResultValue</c>:
        /// <code>{
        ///   id: string, # Optional. The ID is case-preserving and case-insensitive (that is, you may not have two IDs within an Account that differ only by case).
        ///   displayName: string, # Optional. The display name for the Job.
        ///   usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is false.
        ///   url: string, # Optional. The URL of the Job.
        ///   eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job has changed between requests. In particular, you can be pass the ETag when updating a Job to specify that your changes should take effect only if nobody else has modified the Job in the meantime.
        ///   lastModified: string (ISO 8601 Format), # Optional. This is the last time at which the Job level data, such as the Job state or priority, changed. It does not factor in task-level changes such as adding new Tasks or Tasks changing state.
        ///   creationTime: string (ISO 8601 Format), # Optional. The creation time of the Job.
        ///   state: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. The state of the Job.
        ///   stateTransitionTime: string (ISO 8601 Format), # Optional. The time at which the Job entered its current state.
        ///   previousState: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. This property is not set if the Job is in its initial Active state.
        ///   previousStateTransitionTime: string (ISO 8601 Format), # Optional. This property is not set if the Job is in its initial Active state.
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. The default value is 0.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. The execution constraints for a Job.
        ///   jobManagerTask: {
        ///     id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters.
        ///     displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: {
        ///       containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///       imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///       registry: {
        ///         username: string, # Optional. The user name to log into the registry server.
        ///         password: string, # Optional. The password to log into the registry server.
        ///         registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///         identityReference: {
        ///           resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///         }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///       workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///     }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must be set as well. If the Pool that will run this Task doesn&apos;t have containerConfiguration set, this must not be set. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [
        ///       {
        ///         autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///         storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///         httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///         blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///         filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///         fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///         identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }
        ///     ], # Optional. Files listed under this element are located in the Task&apos;s working directory. There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     outputFiles: [
        ///       {
        ///         filePattern: string, # Required. Both relative and absolute paths are supported. Relative paths are relative to the Task working directory. The following wildcards are supported: * matches 0 or more characters (for example pattern abc* would match abc or abcdef), ** matches any directory, ? matches any single character, [abc] matches one character in the brackets, and [a-c] matches one character in the range. Brackets can include a negation to match any character not specified (for example [!abc] matches any character but a, b, or c). If a file name starts with &quot;.&quot; it is ignored by default but may be matched by specifying it explicitly (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\*.txt matches any file that does not start in &apos;.&apos; and ends with .txt in the Task working directory or any subdirectory. If the filename contains a wildcard character it can be escaped using brackets (for example abc[*] would match a file named abc*). Note that both \ and / are treated as directory separators on Windows, but only / is on Linux. Environment variables (%var% on Windows or $var on Linux) are expanded prior to the pattern being applied.
        ///         destination: {
        ///           container: {
        ///             path: string, # Optional. If filePattern refers to a specific file (i.e. contains no wildcards), then path is the name of the blob to which to upload that file. If filePattern contains one or more wildcards (and therefore may match multiple files), then path is the name of the blob virtual directory (which is prepended to each blob name) to which to upload the file(s). If omitted, file(s) are uploaded to the root of the container with a blob name matching their file name.
        ///             containerUrl: string, # Required. If not using a managed identity, the URL must include a Shared Access Signature (SAS) granting write permissions to the container.
        ///             identityReference: ComputeNodeIdentityReference, # Optional. The identity must have write access to the Azure Blob Storage container
        ///             uploadHeaders: [
        ///               {
        ///                 name: string, # Required. The case-insensitive name of the header to be used while uploading output files
        ///                 value: string, # Optional. The value of the header to be used while uploading output files
        ///               }
        ///             ], # Optional. These headers will be specified when uploading files to Azure Storage. For more information, see [Request Headers (All Blob Types)](https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types).
        ///           }, # Optional. Specifies a file upload destination within an Azure blob storage container.
        ///         }, # Required. The destination to which a file should be uploaded.
        ///         uploadOptions: {
        ///           uploadCondition: &quot;tasksuccess&quot; | &quot;taskfailure&quot; | &quot;taskcompletion&quot;, # Required. The default is taskcompletion.
        ///         }, # Required. Details about an output file upload operation, including under what conditions to perform the upload.
        ///       }
        ///     ], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node on which the primary Task is executed.
        ///     environmentSettings: [
        ///       {
        ///         name: string, # Required. The name of the environment variable.
        ///         value: string, # Optional. The value of the environment variable.
        ///       }
        ///     ], # Optional. A list of environment variable settings for the Job Manager Task.
        ///     constraints: {
        ///       maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        ///       retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///       maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task executable due to a nonzero exit code. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task after the first attempt. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///     }, # Optional. Execution constraints to apply to a Task.
        ///     requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the node has enough free scheduling slots available. For multi-instance Tasks, this property is not supported and must not be specified.
        ///     killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job as complete. If any Tasks are still running at this time (other than Job Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not affect the Job status. In this case, you should either use the onAllTasksComplete attribute to terminate the Job, or have a client or user terminate the Job explicitly. An example of this is if the Job Manager creates a set of Tasks but then takes no further role in their execution. The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job (not to monitor progress), then it is important to set killJobOnCompletion to false.
        ///     userIdentity: {
        ///       username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///       autoUser: {
        ///         scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///         elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///       }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///     }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///     runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a Compute Node. The Job Manager Task counts normally against the Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The default value is true.
        ///     applicationPackageReferences: [
        ///       {
        ///         applicationId: string, # Required. The ID of the application to deploy.
        ///         version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///       }
        ///     ], # Optional. Application Packages are downloaded and deployed to a shared directory, not the Task working directory. Therefore, if a referenced Application Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute Node is used. If a referenced Application Package cannot be installed, for example because the package has been deleted or because download failed, the Task fails.
        ///     authenticationTokenSettings: {
        ///       access: [string], # Optional. The authentication token grants access to a limited set of Batch service operations. Currently the only supported value for the access property is &apos;job&apos;, which grants access to all operations related to the Job which contains the Task.
        ///     }, # Optional. If this property is set, the Batch service provides the Task with an authentication token which can be used to authenticate Batch service operations without requiring an Account access key. The token is provided via the AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out using the token depend on the settings. For example, a Task can request Job permissions in order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the Job.
        ///     allowLowPriorityNode: boolean, # Optional. The default value is true.
        ///   }, # Optional. The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running &apos;normal&apos; Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
        ///   jobPreparationTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
        ///     constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
        ///     waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute Nodes.
        ///     rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.
        ///   }, # Optional. The Job Preparation Task is a special Task run on each Compute Node before any other Task of the Job.
        ///   jobReleaseTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute Node, measured from the time the Task starts. If the Task does not complete within the time limit, the Batch service terminates it. The default value is 15 minutes. You may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///     retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///   }, # Optional. The Job Release Task is a special Task run at the end of the Job on each Compute Node that has run any other Task of the Job.
        ///   commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by specifying the same setting name with a different value.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [ApplicationPackageReference], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. Specifies how a Job should be assigned to a Pool.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. The default is noaction.
        ///   onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. A Task is considered to have failed if has a failureInfo. A failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry count, or if there was an error starting the Task, for example due to a resource file download error. The default is noaction.
        ///   networkConfiguration: {
        ///     subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. This is of the form /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication from the Azure Batch service. For Pools created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound connections for communications to Azure Storage. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///   }, # Optional. The network configuration for the Job.
        ///   metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///   executionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. This is the time at which the Job was created.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Job is in the completed state.
        ///     poolId: string, # Optional. This element contains the actual Pool where the Job is assigned. When you get Job details from the service, they also contain a poolInfo element, which contains the Pool configuration data from when the Job was added or updated. That poolInfo element may also contain a poolId element. If it does, the two IDs are the same. If it does not, it means the Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        ///     schedulingError: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Job scheduling error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Job scheduling error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional error details related to the scheduling error.
        ///     }, # Optional. This property is not set if there was no error starting the Job.
        ///     terminateReason: string, # Optional. This property is set only if the Job is in the completed state. If the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a schedule, and the schedule terminated. AllTasksComplete - the Job&apos;s onAllTasksComplete attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job&apos;s onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed with an exit condition that specified a jobAction of terminatejob. Any other string is a user-defined reason specified in a call to the &apos;Terminate a Job&apos; operation.
        ///   }, # Optional. Contains information about the execution of a Job in the Azure Batch service.
        ///   stats: {
        ///     url: string, # Required. The URL of the statistics.
        ///     startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///     lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///     userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///     readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///     writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///     readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///     writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///     numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///     numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///     numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///     waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        ///   }, # Optional. This property is populated only if the CloudJob was retrieved with an expand clause including the &apos;stats&apos; attribute; otherwise it is null. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual AsyncPageable<BinaryData> GetJobsAsync(string filter = null, string select = null, string expand = null, int? maxResults = null, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            return GetJobsImplementationAsync("JobClient.GetJobs", filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
        }

        private AsyncPageable<BinaryData> GetJobsImplementationAsync(string diagnosticsScopeName, string filter, string select, string expand, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            return PageableHelpers.CreateAsyncPageable(CreateEnumerableAsync, ClientDiagnostics, diagnosticsScopeName);
            async IAsyncEnumerable<Page<BinaryData>> CreateEnumerableAsync(string nextLink, int? pageSizeHint, [EnumeratorCancellation] CancellationToken cancellationToken = default)
            {
                do
                {
                    var message = string.IsNullOrEmpty(nextLink)
                        ? CreateGetJobsRequest(filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context)
                        : CreateGetJobsNextPageRequest(nextLink, filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                    var page = await LowLevelPageableHelpers.ProcessMessageAsync(_pipeline, message, context, "value", "odata.nextLink", cancellationToken).ConfigureAwait(false);
                    nextLink = page.ContinuationToken;
                    yield return page;
                } while (!string.IsNullOrEmpty(nextLink));
            }
        }

        /// <summary> Lists all of the Jobs in the specified Account. </summary>
        /// <param name="filter"> An OData $filter clause. For more information on constructing this filter, see https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-jobs. </param>
        /// <param name="select"> An OData $select clause. </param>
        /// <param name="expand"> An OData $expand clause. </param>
        /// <param name="maxResults"> The maximum number of items to return in the response. A maximum of 1000 Jobs can be returned. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The <see cref="Pageable{T}"/> from the service containing a list of <see cref="BinaryData"/> objects. Details of the body schema for each item in the collection are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetJobs and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// foreach (var data in client.GetJobs())
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.ToString());
        /// }
        /// ]]></code>
        /// This sample shows how to call GetJobs with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// foreach (var data in client.GetJobs("<filter>", "<select>", "<expand>", 1234, 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow))
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("usesTaskDependencies").ToString());
        ///     Console.WriteLine(result.GetProperty("url").ToString());
        ///     Console.WriteLine(result.GetProperty("eTag").ToString());
        ///     Console.WriteLine(result.GetProperty("lastModified").ToString());
        ///     Console.WriteLine(result.GetProperty("creationTime").ToString());
        ///     Console.WriteLine(result.GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("stateTransitionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("previousState").ToString());
        ///     Console.WriteLine(result.GetProperty("previousStateTransitionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("priority").ToString());
        ///     Console.WriteLine(result.GetProperty("allowTaskPreemption").ToString());
        ///     Console.WriteLine(result.GetProperty("maxParallelTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("filePattern").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("path").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("containerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("uploadOptions").GetProperty("uploadCondition").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("requiredSlots").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("killJobOnCompletion").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("runExclusive").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("authenticationTokenSettings").GetProperty("access")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("allowLowPriorityNode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("waitForSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("rerunOnNodeRebootAfterSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("poolId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("autoPoolIdPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("poolLifetimeOption").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("keepAlive").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("vmSize").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osFamily").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("publisher").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("offer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("sku").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("virtualMachineImageId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("exactVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodeAgentSKUId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("windowsConfiguration").GetProperty("enableAutomaticUpdates").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("lun").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("caching").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("diskSizeGB").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("storageAccountType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("licenseType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("type").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerImageNames")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("diskEncryptionConfiguration").GetProperty("targets")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodePlacementConfiguration").GetProperty("policy").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("publisher").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("type").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("typeHandlerVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("autoUpgradeMinorVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("settings").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("protectedSettings").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("provisionAfterExtensions")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("osDisk").GetProperty("ephemeralOSDiskSettings").GetProperty("placement").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSlotsPerNode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSchedulingPolicy").GetProperty("nodeFillType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("resizeTimeout").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetDedicatedNodes").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetLowPriorityNodes").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableAutoScale").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleFormula").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleEvaluationInterval").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableInterNodeCommunication").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("dynamicVNetAssignmentScope").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("protocol").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("backendPort").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeStart").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeEnd").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("priority").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("access").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourceAddressPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourcePortRanges")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("provision").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("ipAddressIds")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("waitForSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprint").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprintAlgorithm").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeLocation").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("visibility")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationLicenses")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("uid").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("gid").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("sshPrivateKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("windowsUserConfiguration").GetProperty("loginMode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("containerName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("sasKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("blobfuseOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("source").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("source").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("azureFileUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("onAllTasksComplete").ToString());
        ///     Console.WriteLine(result.GetProperty("onTaskFailure").ToString());
        ///     Console.WriteLine(result.GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        ///     Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("endTime").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("poolId").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("category").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("code").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("message").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("terminateReason").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("url").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("lastUpdateTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("userCPUTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("kernelCPUTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("wallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("readIOps").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOps").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("readIOGiB").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOGiB").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numSucceededTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numFailedTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numTaskRetries").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("waitTime").ToString());
        /// }
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// Below is the JSON schema for one item in the pageable response.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>JobListResultValue</c>:
        /// <code>{
        ///   id: string, # Optional. The ID is case-preserving and case-insensitive (that is, you may not have two IDs within an Account that differ only by case).
        ///   displayName: string, # Optional. The display name for the Job.
        ///   usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is false.
        ///   url: string, # Optional. The URL of the Job.
        ///   eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job has changed between requests. In particular, you can be pass the ETag when updating a Job to specify that your changes should take effect only if nobody else has modified the Job in the meantime.
        ///   lastModified: string (ISO 8601 Format), # Optional. This is the last time at which the Job level data, such as the Job state or priority, changed. It does not factor in task-level changes such as adding new Tasks or Tasks changing state.
        ///   creationTime: string (ISO 8601 Format), # Optional. The creation time of the Job.
        ///   state: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. The state of the Job.
        ///   stateTransitionTime: string (ISO 8601 Format), # Optional. The time at which the Job entered its current state.
        ///   previousState: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. This property is not set if the Job is in its initial Active state.
        ///   previousStateTransitionTime: string (ISO 8601 Format), # Optional. This property is not set if the Job is in its initial Active state.
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. The default value is 0.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. The execution constraints for a Job.
        ///   jobManagerTask: {
        ///     id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters.
        ///     displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: {
        ///       containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///       imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///       registry: {
        ///         username: string, # Optional. The user name to log into the registry server.
        ///         password: string, # Optional. The password to log into the registry server.
        ///         registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///         identityReference: {
        ///           resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///         }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///       workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///     }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must be set as well. If the Pool that will run this Task doesn&apos;t have containerConfiguration set, this must not be set. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [
        ///       {
        ///         autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///         storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///         httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///         blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///         filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///         fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///         identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }
        ///     ], # Optional. Files listed under this element are located in the Task&apos;s working directory. There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     outputFiles: [
        ///       {
        ///         filePattern: string, # Required. Both relative and absolute paths are supported. Relative paths are relative to the Task working directory. The following wildcards are supported: * matches 0 or more characters (for example pattern abc* would match abc or abcdef), ** matches any directory, ? matches any single character, [abc] matches one character in the brackets, and [a-c] matches one character in the range. Brackets can include a negation to match any character not specified (for example [!abc] matches any character but a, b, or c). If a file name starts with &quot;.&quot; it is ignored by default but may be matched by specifying it explicitly (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\*.txt matches any file that does not start in &apos;.&apos; and ends with .txt in the Task working directory or any subdirectory. If the filename contains a wildcard character it can be escaped using brackets (for example abc[*] would match a file named abc*). Note that both \ and / are treated as directory separators on Windows, but only / is on Linux. Environment variables (%var% on Windows or $var on Linux) are expanded prior to the pattern being applied.
        ///         destination: {
        ///           container: {
        ///             path: string, # Optional. If filePattern refers to a specific file (i.e. contains no wildcards), then path is the name of the blob to which to upload that file. If filePattern contains one or more wildcards (and therefore may match multiple files), then path is the name of the blob virtual directory (which is prepended to each blob name) to which to upload the file(s). If omitted, file(s) are uploaded to the root of the container with a blob name matching their file name.
        ///             containerUrl: string, # Required. If not using a managed identity, the URL must include a Shared Access Signature (SAS) granting write permissions to the container.
        ///             identityReference: ComputeNodeIdentityReference, # Optional. The identity must have write access to the Azure Blob Storage container
        ///             uploadHeaders: [
        ///               {
        ///                 name: string, # Required. The case-insensitive name of the header to be used while uploading output files
        ///                 value: string, # Optional. The value of the header to be used while uploading output files
        ///               }
        ///             ], # Optional. These headers will be specified when uploading files to Azure Storage. For more information, see [Request Headers (All Blob Types)](https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types).
        ///           }, # Optional. Specifies a file upload destination within an Azure blob storage container.
        ///         }, # Required. The destination to which a file should be uploaded.
        ///         uploadOptions: {
        ///           uploadCondition: &quot;tasksuccess&quot; | &quot;taskfailure&quot; | &quot;taskcompletion&quot;, # Required. The default is taskcompletion.
        ///         }, # Required. Details about an output file upload operation, including under what conditions to perform the upload.
        ///       }
        ///     ], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node on which the primary Task is executed.
        ///     environmentSettings: [
        ///       {
        ///         name: string, # Required. The name of the environment variable.
        ///         value: string, # Optional. The value of the environment variable.
        ///       }
        ///     ], # Optional. A list of environment variable settings for the Job Manager Task.
        ///     constraints: {
        ///       maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        ///       retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///       maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task executable due to a nonzero exit code. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task after the first attempt. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///     }, # Optional. Execution constraints to apply to a Task.
        ///     requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the node has enough free scheduling slots available. For multi-instance Tasks, this property is not supported and must not be specified.
        ///     killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job as complete. If any Tasks are still running at this time (other than Job Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not affect the Job status. In this case, you should either use the onAllTasksComplete attribute to terminate the Job, or have a client or user terminate the Job explicitly. An example of this is if the Job Manager creates a set of Tasks but then takes no further role in their execution. The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job (not to monitor progress), then it is important to set killJobOnCompletion to false.
        ///     userIdentity: {
        ///       username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///       autoUser: {
        ///         scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///         elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///       }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///     }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///     runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a Compute Node. The Job Manager Task counts normally against the Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The default value is true.
        ///     applicationPackageReferences: [
        ///       {
        ///         applicationId: string, # Required. The ID of the application to deploy.
        ///         version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///       }
        ///     ], # Optional. Application Packages are downloaded and deployed to a shared directory, not the Task working directory. Therefore, if a referenced Application Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute Node is used. If a referenced Application Package cannot be installed, for example because the package has been deleted or because download failed, the Task fails.
        ///     authenticationTokenSettings: {
        ///       access: [string], # Optional. The authentication token grants access to a limited set of Batch service operations. Currently the only supported value for the access property is &apos;job&apos;, which grants access to all operations related to the Job which contains the Task.
        ///     }, # Optional. If this property is set, the Batch service provides the Task with an authentication token which can be used to authenticate Batch service operations without requiring an Account access key. The token is provided via the AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out using the token depend on the settings. For example, a Task can request Job permissions in order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the Job.
        ///     allowLowPriorityNode: boolean, # Optional. The default value is true.
        ///   }, # Optional. The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running &apos;normal&apos; Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
        ///   jobPreparationTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
        ///     constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
        ///     waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute Nodes.
        ///     rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.
        ///   }, # Optional. The Job Preparation Task is a special Task run on each Compute Node before any other Task of the Job.
        ///   jobReleaseTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute Node, measured from the time the Task starts. If the Task does not complete within the time limit, the Batch service terminates it. The default value is 15 minutes. You may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///     retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///   }, # Optional. The Job Release Task is a special Task run at the end of the Job on each Compute Node that has run any other Task of the Job.
        ///   commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by specifying the same setting name with a different value.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [ApplicationPackageReference], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. Specifies how a Job should be assigned to a Pool.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. The default is noaction.
        ///   onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. A Task is considered to have failed if has a failureInfo. A failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry count, or if there was an error starting the Task, for example due to a resource file download error. The default is noaction.
        ///   networkConfiguration: {
        ///     subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. This is of the form /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication from the Azure Batch service. For Pools created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound connections for communications to Azure Storage. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///   }, # Optional. The network configuration for the Job.
        ///   metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///   executionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. This is the time at which the Job was created.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Job is in the completed state.
        ///     poolId: string, # Optional. This element contains the actual Pool where the Job is assigned. When you get Job details from the service, they also contain a poolInfo element, which contains the Pool configuration data from when the Job was added or updated. That poolInfo element may also contain a poolId element. If it does, the two IDs are the same. If it does not, it means the Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        ///     schedulingError: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Job scheduling error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Job scheduling error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional error details related to the scheduling error.
        ///     }, # Optional. This property is not set if there was no error starting the Job.
        ///     terminateReason: string, # Optional. This property is set only if the Job is in the completed state. If the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a schedule, and the schedule terminated. AllTasksComplete - the Job&apos;s onAllTasksComplete attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job&apos;s onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed with an exit condition that specified a jobAction of terminatejob. Any other string is a user-defined reason specified in a call to the &apos;Terminate a Job&apos; operation.
        ///   }, # Optional. Contains information about the execution of a Job in the Azure Batch service.
        ///   stats: {
        ///     url: string, # Required. The URL of the statistics.
        ///     startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///     lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///     userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///     readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///     writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///     readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///     writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///     numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///     numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///     numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///     waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        ///   }, # Optional. This property is populated only if the CloudJob was retrieved with an expand clause including the &apos;stats&apos; attribute; otherwise it is null. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Pageable<BinaryData> GetJobs(string filter = null, string select = null, string expand = null, int? maxResults = null, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            return GetJobsImplementation("JobClient.GetJobs", filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
        }

        private Pageable<BinaryData> GetJobsImplementation(string diagnosticsScopeName, string filter, string select, string expand, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            return PageableHelpers.CreatePageable(CreateEnumerable, ClientDiagnostics, diagnosticsScopeName);
            IEnumerable<Page<BinaryData>> CreateEnumerable(string nextLink, int? pageSizeHint)
            {
                do
                {
                    var message = string.IsNullOrEmpty(nextLink)
                        ? CreateGetJobsRequest(filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context)
                        : CreateGetJobsNextPageRequest(nextLink, filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                    var page = LowLevelPageableHelpers.ProcessMessage(_pipeline, message, context, "value", "odata.nextLink");
                    nextLink = page.ContinuationToken;
                    yield return page;
                } while (!string.IsNullOrEmpty(nextLink));
            }
        }

        /// <summary> Lists the Jobs that have been created under the specified Job Schedule. </summary>
        /// <param name="jobScheduleId"> The ID of the Job Schedule from which you want to get a list of Jobs. </param>
        /// <param name="filter"> An OData $filter clause. For more information on constructing this filter, see https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-jobs-in-a-job-schedule. </param>
        /// <param name="select"> An OData $select clause. </param>
        /// <param name="expand"> An OData $expand clause. </param>
        /// <param name="maxResults"> The maximum number of items to return in the response. A maximum of 1000 Jobs can be returned. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobScheduleId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobScheduleId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The <see cref="AsyncPageable{T}"/> from the service containing a list of <see cref="BinaryData"/> objects. Details of the body schema for each item in the collection are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetFromJobSchedulesAsync with required parameters and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// await foreach (var data in client.GetFromJobSchedulesAsync("<jobScheduleId>"))
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.ToString());
        /// }
        /// ]]></code>
        /// This sample shows how to call GetFromJobSchedulesAsync with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// await foreach (var data in client.GetFromJobSchedulesAsync("<jobScheduleId>", "<filter>", "<select>", "<expand>", 1234, 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow))
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("usesTaskDependencies").ToString());
        ///     Console.WriteLine(result.GetProperty("url").ToString());
        ///     Console.WriteLine(result.GetProperty("eTag").ToString());
        ///     Console.WriteLine(result.GetProperty("lastModified").ToString());
        ///     Console.WriteLine(result.GetProperty("creationTime").ToString());
        ///     Console.WriteLine(result.GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("stateTransitionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("previousState").ToString());
        ///     Console.WriteLine(result.GetProperty("previousStateTransitionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("priority").ToString());
        ///     Console.WriteLine(result.GetProperty("allowTaskPreemption").ToString());
        ///     Console.WriteLine(result.GetProperty("maxParallelTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("filePattern").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("path").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("containerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("uploadOptions").GetProperty("uploadCondition").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("requiredSlots").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("killJobOnCompletion").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("runExclusive").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("authenticationTokenSettings").GetProperty("access")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("allowLowPriorityNode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("waitForSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("rerunOnNodeRebootAfterSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("poolId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("autoPoolIdPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("poolLifetimeOption").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("keepAlive").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("vmSize").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osFamily").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("publisher").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("offer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("sku").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("virtualMachineImageId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("exactVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodeAgentSKUId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("windowsConfiguration").GetProperty("enableAutomaticUpdates").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("lun").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("caching").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("diskSizeGB").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("storageAccountType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("licenseType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("type").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerImageNames")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("diskEncryptionConfiguration").GetProperty("targets")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodePlacementConfiguration").GetProperty("policy").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("publisher").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("type").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("typeHandlerVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("autoUpgradeMinorVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("settings").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("protectedSettings").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("provisionAfterExtensions")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("osDisk").GetProperty("ephemeralOSDiskSettings").GetProperty("placement").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSlotsPerNode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSchedulingPolicy").GetProperty("nodeFillType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("resizeTimeout").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetDedicatedNodes").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetLowPriorityNodes").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableAutoScale").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleFormula").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleEvaluationInterval").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableInterNodeCommunication").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("dynamicVNetAssignmentScope").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("protocol").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("backendPort").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeStart").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeEnd").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("priority").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("access").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourceAddressPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourcePortRanges")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("provision").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("ipAddressIds")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("waitForSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprint").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprintAlgorithm").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeLocation").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("visibility")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationLicenses")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("uid").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("gid").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("sshPrivateKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("windowsUserConfiguration").GetProperty("loginMode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("containerName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("sasKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("blobfuseOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("source").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("source").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("azureFileUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("onAllTasksComplete").ToString());
        ///     Console.WriteLine(result.GetProperty("onTaskFailure").ToString());
        ///     Console.WriteLine(result.GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        ///     Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("endTime").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("poolId").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("category").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("code").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("message").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("terminateReason").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("url").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("lastUpdateTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("userCPUTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("kernelCPUTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("wallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("readIOps").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOps").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("readIOGiB").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOGiB").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numSucceededTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numFailedTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numTaskRetries").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("waitTime").ToString());
        /// }
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// Below is the JSON schema for one item in the pageable response.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>JobListResultValue</c>:
        /// <code>{
        ///   id: string, # Optional. The ID is case-preserving and case-insensitive (that is, you may not have two IDs within an Account that differ only by case).
        ///   displayName: string, # Optional. The display name for the Job.
        ///   usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is false.
        ///   url: string, # Optional. The URL of the Job.
        ///   eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job has changed between requests. In particular, you can be pass the ETag when updating a Job to specify that your changes should take effect only if nobody else has modified the Job in the meantime.
        ///   lastModified: string (ISO 8601 Format), # Optional. This is the last time at which the Job level data, such as the Job state or priority, changed. It does not factor in task-level changes such as adding new Tasks or Tasks changing state.
        ///   creationTime: string (ISO 8601 Format), # Optional. The creation time of the Job.
        ///   state: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. The state of the Job.
        ///   stateTransitionTime: string (ISO 8601 Format), # Optional. The time at which the Job entered its current state.
        ///   previousState: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. This property is not set if the Job is in its initial Active state.
        ///   previousStateTransitionTime: string (ISO 8601 Format), # Optional. This property is not set if the Job is in its initial Active state.
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. The default value is 0.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. The execution constraints for a Job.
        ///   jobManagerTask: {
        ///     id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters.
        ///     displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: {
        ///       containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///       imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///       registry: {
        ///         username: string, # Optional. The user name to log into the registry server.
        ///         password: string, # Optional. The password to log into the registry server.
        ///         registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///         identityReference: {
        ///           resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///         }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///       workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///     }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must be set as well. If the Pool that will run this Task doesn&apos;t have containerConfiguration set, this must not be set. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [
        ///       {
        ///         autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///         storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///         httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///         blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///         filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///         fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///         identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }
        ///     ], # Optional. Files listed under this element are located in the Task&apos;s working directory. There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     outputFiles: [
        ///       {
        ///         filePattern: string, # Required. Both relative and absolute paths are supported. Relative paths are relative to the Task working directory. The following wildcards are supported: * matches 0 or more characters (for example pattern abc* would match abc or abcdef), ** matches any directory, ? matches any single character, [abc] matches one character in the brackets, and [a-c] matches one character in the range. Brackets can include a negation to match any character not specified (for example [!abc] matches any character but a, b, or c). If a file name starts with &quot;.&quot; it is ignored by default but may be matched by specifying it explicitly (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\*.txt matches any file that does not start in &apos;.&apos; and ends with .txt in the Task working directory or any subdirectory. If the filename contains a wildcard character it can be escaped using brackets (for example abc[*] would match a file named abc*). Note that both \ and / are treated as directory separators on Windows, but only / is on Linux. Environment variables (%var% on Windows or $var on Linux) are expanded prior to the pattern being applied.
        ///         destination: {
        ///           container: {
        ///             path: string, # Optional. If filePattern refers to a specific file (i.e. contains no wildcards), then path is the name of the blob to which to upload that file. If filePattern contains one or more wildcards (and therefore may match multiple files), then path is the name of the blob virtual directory (which is prepended to each blob name) to which to upload the file(s). If omitted, file(s) are uploaded to the root of the container with a blob name matching their file name.
        ///             containerUrl: string, # Required. If not using a managed identity, the URL must include a Shared Access Signature (SAS) granting write permissions to the container.
        ///             identityReference: ComputeNodeIdentityReference, # Optional. The identity must have write access to the Azure Blob Storage container
        ///             uploadHeaders: [
        ///               {
        ///                 name: string, # Required. The case-insensitive name of the header to be used while uploading output files
        ///                 value: string, # Optional. The value of the header to be used while uploading output files
        ///               }
        ///             ], # Optional. These headers will be specified when uploading files to Azure Storage. For more information, see [Request Headers (All Blob Types)](https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types).
        ///           }, # Optional. Specifies a file upload destination within an Azure blob storage container.
        ///         }, # Required. The destination to which a file should be uploaded.
        ///         uploadOptions: {
        ///           uploadCondition: &quot;tasksuccess&quot; | &quot;taskfailure&quot; | &quot;taskcompletion&quot;, # Required. The default is taskcompletion.
        ///         }, # Required. Details about an output file upload operation, including under what conditions to perform the upload.
        ///       }
        ///     ], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node on which the primary Task is executed.
        ///     environmentSettings: [
        ///       {
        ///         name: string, # Required. The name of the environment variable.
        ///         value: string, # Optional. The value of the environment variable.
        ///       }
        ///     ], # Optional. A list of environment variable settings for the Job Manager Task.
        ///     constraints: {
        ///       maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        ///       retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///       maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task executable due to a nonzero exit code. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task after the first attempt. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///     }, # Optional. Execution constraints to apply to a Task.
        ///     requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the node has enough free scheduling slots available. For multi-instance Tasks, this property is not supported and must not be specified.
        ///     killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job as complete. If any Tasks are still running at this time (other than Job Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not affect the Job status. In this case, you should either use the onAllTasksComplete attribute to terminate the Job, or have a client or user terminate the Job explicitly. An example of this is if the Job Manager creates a set of Tasks but then takes no further role in their execution. The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job (not to monitor progress), then it is important to set killJobOnCompletion to false.
        ///     userIdentity: {
        ///       username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///       autoUser: {
        ///         scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///         elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///       }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///     }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///     runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a Compute Node. The Job Manager Task counts normally against the Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The default value is true.
        ///     applicationPackageReferences: [
        ///       {
        ///         applicationId: string, # Required. The ID of the application to deploy.
        ///         version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///       }
        ///     ], # Optional. Application Packages are downloaded and deployed to a shared directory, not the Task working directory. Therefore, if a referenced Application Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute Node is used. If a referenced Application Package cannot be installed, for example because the package has been deleted or because download failed, the Task fails.
        ///     authenticationTokenSettings: {
        ///       access: [string], # Optional. The authentication token grants access to a limited set of Batch service operations. Currently the only supported value for the access property is &apos;job&apos;, which grants access to all operations related to the Job which contains the Task.
        ///     }, # Optional. If this property is set, the Batch service provides the Task with an authentication token which can be used to authenticate Batch service operations without requiring an Account access key. The token is provided via the AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out using the token depend on the settings. For example, a Task can request Job permissions in order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the Job.
        ///     allowLowPriorityNode: boolean, # Optional. The default value is true.
        ///   }, # Optional. The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running &apos;normal&apos; Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
        ///   jobPreparationTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
        ///     constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
        ///     waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute Nodes.
        ///     rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.
        ///   }, # Optional. The Job Preparation Task is a special Task run on each Compute Node before any other Task of the Job.
        ///   jobReleaseTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute Node, measured from the time the Task starts. If the Task does not complete within the time limit, the Batch service terminates it. The default value is 15 minutes. You may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///     retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///   }, # Optional. The Job Release Task is a special Task run at the end of the Job on each Compute Node that has run any other Task of the Job.
        ///   commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by specifying the same setting name with a different value.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [ApplicationPackageReference], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. Specifies how a Job should be assigned to a Pool.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. The default is noaction.
        ///   onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. A Task is considered to have failed if has a failureInfo. A failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry count, or if there was an error starting the Task, for example due to a resource file download error. The default is noaction.
        ///   networkConfiguration: {
        ///     subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. This is of the form /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication from the Azure Batch service. For Pools created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound connections for communications to Azure Storage. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///   }, # Optional. The network configuration for the Job.
        ///   metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///   executionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. This is the time at which the Job was created.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Job is in the completed state.
        ///     poolId: string, # Optional. This element contains the actual Pool where the Job is assigned. When you get Job details from the service, they also contain a poolInfo element, which contains the Pool configuration data from when the Job was added or updated. That poolInfo element may also contain a poolId element. If it does, the two IDs are the same. If it does not, it means the Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        ///     schedulingError: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Job scheduling error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Job scheduling error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional error details related to the scheduling error.
        ///     }, # Optional. This property is not set if there was no error starting the Job.
        ///     terminateReason: string, # Optional. This property is set only if the Job is in the completed state. If the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a schedule, and the schedule terminated. AllTasksComplete - the Job&apos;s onAllTasksComplete attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job&apos;s onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed with an exit condition that specified a jobAction of terminatejob. Any other string is a user-defined reason specified in a call to the &apos;Terminate a Job&apos; operation.
        ///   }, # Optional. Contains information about the execution of a Job in the Azure Batch service.
        ///   stats: {
        ///     url: string, # Required. The URL of the statistics.
        ///     startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///     lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///     userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///     readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///     writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///     readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///     writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///     numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///     numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///     numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///     waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        ///   }, # Optional. This property is populated only if the CloudJob was retrieved with an expand clause including the &apos;stats&apos; attribute; otherwise it is null. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual AsyncPageable<BinaryData> GetFromJobSchedulesAsync(string jobScheduleId, string filter = null, string select = null, string expand = null, int? maxResults = null, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobScheduleId, nameof(jobScheduleId));

            return GetFromJobSchedulesImplementationAsync("JobClient.GetFromJobSchedules", jobScheduleId, filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
        }

        private AsyncPageable<BinaryData> GetFromJobSchedulesImplementationAsync(string diagnosticsScopeName, string jobScheduleId, string filter, string select, string expand, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            return PageableHelpers.CreateAsyncPageable(CreateEnumerableAsync, ClientDiagnostics, diagnosticsScopeName);
            async IAsyncEnumerable<Page<BinaryData>> CreateEnumerableAsync(string nextLink, int? pageSizeHint, [EnumeratorCancellation] CancellationToken cancellationToken = default)
            {
                do
                {
                    var message = string.IsNullOrEmpty(nextLink)
                        ? CreateGetFromJobSchedulesRequest(jobScheduleId, filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context)
                        : CreateGetFromJobSchedulesNextPageRequest(nextLink, jobScheduleId, filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                    var page = await LowLevelPageableHelpers.ProcessMessageAsync(_pipeline, message, context, "value", "odata.nextLink", cancellationToken).ConfigureAwait(false);
                    nextLink = page.ContinuationToken;
                    yield return page;
                } while (!string.IsNullOrEmpty(nextLink));
            }
        }

        /// <summary> Lists the Jobs that have been created under the specified Job Schedule. </summary>
        /// <param name="jobScheduleId"> The ID of the Job Schedule from which you want to get a list of Jobs. </param>
        /// <param name="filter"> An OData $filter clause. For more information on constructing this filter, see https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-jobs-in-a-job-schedule. </param>
        /// <param name="select"> An OData $select clause. </param>
        /// <param name="expand"> An OData $expand clause. </param>
        /// <param name="maxResults"> The maximum number of items to return in the response. A maximum of 1000 Jobs can be returned. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobScheduleId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobScheduleId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The <see cref="Pageable{T}"/> from the service containing a list of <see cref="BinaryData"/> objects. Details of the body schema for each item in the collection are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetFromJobSchedules with required parameters and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// foreach (var data in client.GetFromJobSchedules("<jobScheduleId>"))
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.ToString());
        /// }
        /// ]]></code>
        /// This sample shows how to call GetFromJobSchedules with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// foreach (var data in client.GetFromJobSchedules("<jobScheduleId>", "<filter>", "<select>", "<expand>", 1234, 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow))
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("usesTaskDependencies").ToString());
        ///     Console.WriteLine(result.GetProperty("url").ToString());
        ///     Console.WriteLine(result.GetProperty("eTag").ToString());
        ///     Console.WriteLine(result.GetProperty("lastModified").ToString());
        ///     Console.WriteLine(result.GetProperty("creationTime").ToString());
        ///     Console.WriteLine(result.GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("stateTransitionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("previousState").ToString());
        ///     Console.WriteLine(result.GetProperty("previousStateTransitionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("priority").ToString());
        ///     Console.WriteLine(result.GetProperty("allowTaskPreemption").ToString());
        ///     Console.WriteLine(result.GetProperty("maxParallelTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("filePattern").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("path").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("containerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("uploadOptions").GetProperty("uploadCondition").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("requiredSlots").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("killJobOnCompletion").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("runExclusive").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("authenticationTokenSettings").GetProperty("access")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("jobManagerTask").GetProperty("allowLowPriorityNode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("waitForSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTask").GetProperty("rerunOnNodeRebootAfterSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("id").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("maxWallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("retentionTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("commonEnvironmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("poolId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("autoPoolIdPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("poolLifetimeOption").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("keepAlive").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("displayName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("vmSize").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osFamily").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("publisher").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("offer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("sku").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("virtualMachineImageId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("exactVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodeAgentSKUId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("windowsConfiguration").GetProperty("enableAutomaticUpdates").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("lun").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("caching").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("diskSizeGB").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("storageAccountType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("licenseType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("type").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerImageNames")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("diskEncryptionConfiguration").GetProperty("targets")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodePlacementConfiguration").GetProperty("policy").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("publisher").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("type").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("typeHandlerVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("autoUpgradeMinorVersion").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("settings").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("protectedSettings").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("provisionAfterExtensions")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("osDisk").GetProperty("ephemeralOSDiskSettings").GetProperty("placement").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSlotsPerNode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSchedulingPolicy").GetProperty("nodeFillType").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("resizeTimeout").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetDedicatedNodes").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetLowPriorityNodes").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableAutoScale").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleFormula").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleEvaluationInterval").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableInterNodeCommunication").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("dynamicVNetAssignmentScope").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("protocol").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("backendPort").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeStart").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeEnd").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("priority").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("access").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourceAddressPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourcePortRanges")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("provision").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("ipAddressIds")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("commandLine").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("maxTaskRetryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("waitForSuccess").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprint").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprintAlgorithm").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeLocation").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("visibility")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationLicenses")[0].ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("elevationLevel").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("uid").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("gid").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("sshPrivateKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("windowsUserConfiguration").GetProperty("loginMode").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("containerName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("sasKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("blobfuseOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("identityReference").GetProperty("resourceId").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("source").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("username").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("source").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("password").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountName").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("azureFileUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountKey").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("relativeMountPath").ToString());
        ///     Console.WriteLine(result.GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("mountOptions").ToString());
        ///     Console.WriteLine(result.GetProperty("onAllTasksComplete").ToString());
        ///     Console.WriteLine(result.GetProperty("onTaskFailure").ToString());
        ///     Console.WriteLine(result.GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
        ///     Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("endTime").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("poolId").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("category").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("code").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("message").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("schedulingError").GetProperty("details")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("executionInfo").GetProperty("terminateReason").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("url").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("lastUpdateTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("userCPUTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("kernelCPUTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("wallClockTime").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("readIOps").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOps").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("readIOGiB").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOGiB").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numSucceededTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numFailedTasks").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("numTaskRetries").ToString());
        ///     Console.WriteLine(result.GetProperty("stats").GetProperty("waitTime").ToString());
        /// }
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// Below is the JSON schema for one item in the pageable response.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>JobListResultValue</c>:
        /// <code>{
        ///   id: string, # Optional. The ID is case-preserving and case-insensitive (that is, you may not have two IDs within an Account that differ only by case).
        ///   displayName: string, # Optional. The display name for the Job.
        ///   usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is false.
        ///   url: string, # Optional. The URL of the Job.
        ///   eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job has changed between requests. In particular, you can be pass the ETag when updating a Job to specify that your changes should take effect only if nobody else has modified the Job in the meantime.
        ///   lastModified: string (ISO 8601 Format), # Optional. This is the last time at which the Job level data, such as the Job state or priority, changed. It does not factor in task-level changes such as adding new Tasks or Tasks changing state.
        ///   creationTime: string (ISO 8601 Format), # Optional. The creation time of the Job.
        ///   state: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. The state of the Job.
        ///   stateTransitionTime: string (ISO 8601 Format), # Optional. The time at which the Job entered its current state.
        ///   previousState: &quot;active&quot; | &quot;disabling&quot; | &quot;disabled&quot; | &quot;enabling&quot; | &quot;terminating&quot; | &quot;completed&quot; | &quot;deleting&quot;, # Optional. This property is not set if the Job is in its initial Active state.
        ///   previousStateTransitionTime: string (ISO 8601 Format), # Optional. This property is not set if the Job is in its initial Active state.
        ///   priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest priority and 1000 being the highest priority. The default value is 0.
        ///   allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system will take precedence and will be able requeue tasks from this job. You can update a job&apos;s allowTaskPreemption after it has been created using the update job API.
        ///   maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not specified, the default value is -1, which means there&apos;s no limit to the number of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after it has been created using the update job API.
        ///   constraints: {
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service terminates it and any Tasks that are still running. In this case, the termination reason will be MaxWallClockTimeExpiry. If this property is not specified, there is no time limit on how long the Job may run.
        ///     maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch service will try each Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry Tasks. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///   }, # Optional. The execution constraints for a Job.
        ///   jobManagerTask: {
        ///     id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters.
        ///     displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: {
        ///       containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot; command, in addition to those controlled by the Batch Service.
        ///       imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a default.
        ///       registry: {
        ///         username: string, # Optional. The user name to log into the registry server.
        ///         password: string, # Optional. The password to log into the registry server.
        ///         registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
        ///         identityReference: {
        ///           resourceId: string, # Optional. The ARM resource id of the user assigned identity.
        ///         }, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }, # Optional. This setting can be omitted if was already provided at Pool creation.
        ///       workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
        ///     }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must be set as well. If the Pool that will run this Task doesn&apos;t have containerConfiguration set, this must not be set. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [
        ///       {
        ///         autoStorageContainerName: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified.
        ///         storageContainerUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. This URL must be readable and listable from compute nodes. There are three ways to get such a URL for a container in Azure storage: include a Shared Access Signature (SAS) granting read and list permissions on the container, use a managed identity with read and list permissions, or set the ACL for the container to allow public access.
        ///         httpUrl: string, # Optional. The autoStorageContainerName, storageContainerUrl and httpUrl properties are mutually exclusive and one of them must be specified. If the URL points to Azure Blob Storage, it must be readable from compute nodes. There are three ways to get such a URL for a blob in Azure storage: include a Shared Access Signature (SAS) granting read permissions on the blob, use a managed identity with read permission, or set the ACL for the blob or its container to allow public access.
        ///         blobPrefix: string, # Optional. The property is valid only when autoStorageContainerName or storageContainerUrl is used. This prefix can be a partial filename or a subdirectory. If a prefix is not specified, all the files in the container will be downloaded.
        ///         filePath: string, # Optional. If the httpUrl property is specified, the filePath is required and describes the path which the file will be downloaded to, including the filename. Otherwise, if the autoStorageContainerName or storageContainerUrl property is specified, filePath is optional and is the directory to download the files to. In the case where filePath is used as a directory, any directory structure already associated with the input data will be retained in full and appended to the specified filePath directory. The specified relative path cannot break out of the Task&apos;s working directory (for example by using &apos;..&apos;).
        ///         fileMode: string, # Optional. This property applies only to files being downloaded to Linux Compute Nodes. It will be ignored if it is specified for a resourceFile which will be downloaded to a Windows Compute Node. If this property is not specified for a Linux Compute Node, then a default value of 0770 is applied to the file.
        ///         identityReference: ComputeNodeIdentityReference, # Optional. The reference to a user assigned identity associated with the Batch pool which a compute node will use.
        ///       }
        ///     ], # Optional. Files listed under this element are located in the Task&apos;s working directory. There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     outputFiles: [
        ///       {
        ///         filePattern: string, # Required. Both relative and absolute paths are supported. Relative paths are relative to the Task working directory. The following wildcards are supported: * matches 0 or more characters (for example pattern abc* would match abc or abcdef), ** matches any directory, ? matches any single character, [abc] matches one character in the brackets, and [a-c] matches one character in the range. Brackets can include a negation to match any character not specified (for example [!abc] matches any character but a, b, or c). If a file name starts with &quot;.&quot; it is ignored by default but may be matched by specifying it explicitly (for example *.gif will not match .a.gif, but .*.gif will). A simple example: **\*.txt matches any file that does not start in &apos;.&apos; and ends with .txt in the Task working directory or any subdirectory. If the filename contains a wildcard character it can be escaped using brackets (for example abc[*] would match a file named abc*). Note that both \ and / are treated as directory separators on Windows, but only / is on Linux. Environment variables (%var% on Windows or $var on Linux) are expanded prior to the pattern being applied.
        ///         destination: {
        ///           container: {
        ///             path: string, # Optional. If filePattern refers to a specific file (i.e. contains no wildcards), then path is the name of the blob to which to upload that file. If filePattern contains one or more wildcards (and therefore may match multiple files), then path is the name of the blob virtual directory (which is prepended to each blob name) to which to upload the file(s). If omitted, file(s) are uploaded to the root of the container with a blob name matching their file name.
        ///             containerUrl: string, # Required. If not using a managed identity, the URL must include a Shared Access Signature (SAS) granting write permissions to the container.
        ///             identityReference: ComputeNodeIdentityReference, # Optional. The identity must have write access to the Azure Blob Storage container
        ///             uploadHeaders: [
        ///               {
        ///                 name: string, # Required. The case-insensitive name of the header to be used while uploading output files
        ///                 value: string, # Optional. The value of the header to be used while uploading output files
        ///               }
        ///             ], # Optional. These headers will be specified when uploading files to Azure Storage. For more information, see [Request Headers (All Blob Types)](https://docs.microsoft.com/rest/api/storageservices/put-blob#request-headers-all-blob-types).
        ///           }, # Optional. Specifies a file upload destination within an Azure blob storage container.
        ///         }, # Required. The destination to which a file should be uploaded.
        ///         uploadOptions: {
        ///           uploadCondition: &quot;tasksuccess&quot; | &quot;taskfailure&quot; | &quot;taskcompletion&quot;, # Required. The default is taskcompletion.
        ///         }, # Required. Details about an output file upload operation, including under what conditions to perform the upload.
        ///       }
        ///     ], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node on which the primary Task is executed.
        ///     environmentSettings: [
        ///       {
        ///         name: string, # Required. The name of the environment variable.
        ///         value: string, # Optional. The value of the environment variable.
        ///       }
        ///     ], # Optional. A list of environment variable settings for the Job Manager Task.
        ///     constraints: {
        ///       maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        ///       retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///       maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task executable due to a nonzero exit code. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task after the first attempt. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///     }, # Optional. Execution constraints to apply to a Task.
        ///     requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the node has enough free scheduling slots available. For multi-instance Tasks, this property is not supported and must not be specified.
        ///     killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job as complete. If any Tasks are still running at this time (other than Job Release), those Tasks are terminated. If false, the completion of the Job Manager Task does not affect the Job status. In this case, you should either use the onAllTasksComplete attribute to terminate the Job, or have a client or user terminate the Job explicitly. An example of this is if the Job Manager creates a set of Tasks but then takes no further role in their execution. The default value is true. If you are using the onAllTasksComplete and onTaskFailure attributes to control Job lifetime, and using the Job Manager Task only to create the Tasks for the Job (not to monitor progress), then it is important to set killJobOnCompletion to false.
        ///     userIdentity: {
        ///       username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///       autoUser: {
        ///         scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task should be specified if stricter isolation between tasks is required. For example, if the task mutates the registry in a way which could impact other tasks, or if certificates have been specified on the pool which should not be accessible by normal tasks but should be accessible by StartTasks.
        ///         elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///       }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify one but not both.
        ///     }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///     runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job Manager is running. If false, other Tasks can run simultaneously with the Job Manager on a Compute Node. The Job Manager Task counts normally against the Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute Node allows multiple concurrent Tasks. The default value is true.
        ///     applicationPackageReferences: [
        ///       {
        ///         applicationId: string, # Required. The ID of the application to deploy.
        ///         version: string, # Optional. If this is omitted on a Pool, and no default version is specified for this application, the request fails with the error code InvalidApplicationPackageReferences and HTTP status code 409. If this is omitted on a Task, and no default version is specified for this application, the Task fails with a pre-processing error.
        ///       }
        ///     ], # Optional. Application Packages are downloaded and deployed to a shared directory, not the Task working directory. Therefore, if a referenced Application Package is already on the Compute Node, and is up to date, then it is not re-downloaded; the existing copy on the Compute Node is used. If a referenced Application Package cannot be installed, for example because the package has been deleted or because download failed, the Task fails.
        ///     authenticationTokenSettings: {
        ///       access: [string], # Optional. The authentication token grants access to a limited set of Batch service operations. Currently the only supported value for the access property is &apos;job&apos;, which grants access to all operations related to the Job which contains the Task.
        ///     }, # Optional. If this property is set, the Batch service provides the Task with an authentication token which can be used to authenticate Batch service operations without requiring an Account access key. The token is provided via the AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the Task can carry out using the token depend on the settings. For example, a Task can request Job permissions in order to add other Tasks to the Job, or check the status of the Job or of other Tasks under the Job.
        ///     allowLowPriorityNode: boolean, # Optional. The default value is true.
        ///   }, # Optional. The Job Manager Task is automatically started when the Job is created. The Batch service tries to schedule the Job Manager Task before any other Tasks in the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where Job Manager Tasks are running for as long as possible (that is, Compute Nodes running &apos;normal&apos; Tasks are removed before Compute Nodes running Job Manager Tasks). When a Job Manager Task fails and needs to be restarted, the system tries to schedule it at the highest priority. If there are no idle Compute Nodes available, the system may terminate one of the running Tasks in the Pool and return it to the queue in order to make room for the Job Manager Task to restart. Note that a Job Manager Task in one Job does not have priority over Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For example, if a Job Manager in a priority 0 Job needs to be restarted, it will not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
        ///   jobPreparationTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
        ///     constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
        ///     waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux Compute Nodes.
        ///     rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.
        ///   }, # Optional. The Job Preparation Task is a special Task run on each Compute Node before any other Task of the Job.
        ///   jobReleaseTask: {
        ///     id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of &apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobReleaseTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).
        ///     commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///     containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///     resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///     environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
        ///     maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute Node, measured from the time the Task starts. If the Task does not complete within the time limit, the Batch service terminates it. The default value is 15 minutes. You may not specify a timeout longer than 15 minutes. If you do, the Batch service rejects it with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///     retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days unless the Compute Node is removed or the Job is deleted.
        ///     userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///   }, # Optional. The Job Release Task is a special Task run at the end of the Job on each Compute Node that has run any other Task of the Job.
        ///   commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by specifying the same setting name with a different value.
        ///   poolInfo: {
        ///     poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool does not exist at the time the Batch service tries to schedule a Job, no Tasks for the Job will run until you create a Pool with that id. Note that the Batch service will not reject the Job request; it will simply not run Tasks until the Pool exists. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///     autoPoolSpecification: {
        ///       autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To distinguish between Pools created for different purposes, you can specify this element to add a prefix to the ID that is assigned. The prefix can be up to 20 characters long.
        ///       poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule are assigned to Pools.
        ///       keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined by the poolLifetimeOption setting) expires; that is, when the Job or Job Schedule completes. If true, the Batch service does not delete the Pool automatically. It is up to the user to delete auto Pools created with this option.
        ///       pool: {
        ///         displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up to a maximum length of 1024.
        ///         vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose a VM size for Compute Nodes in an Azure Batch Pool (https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
        ///         cloudServiceConfiguration: {
        ///           osFamily: string, # Required. Possible values are:
        /// 2 - OS Family 2, equivalent to Windows Server 2008 R2 SP1.
        /// 3 - OS Family 3, equivalent to Windows Server 2012.
        /// 4 - OS Family 4, equivalent to Windows Server 2012 R2.
        /// 5 - OS Family 5, equivalent to Windows Server 2016.
        /// 6 - OS Family 6, equivalent to Windows Server 2019. For more information, see Azure Guest OS Releases (https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
        ///           osVersion: string, # Optional. The default value is * which specifies the latest operating system version for the specified OS family.
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS VMs. This property and virtualMachineConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request). This property cannot be specified if the Batch Account was created with its poolAllocationMode property set to &apos;UserSubscription&apos;.
        ///         virtualMachineConfiguration: {
        ///           imageReference: {
        ///             publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
        ///             offer: string, # Optional. For example, UbuntuServer or WindowsServer.
        ///             sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
        ///             version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image. If omitted, the default is &apos;latest&apos;.
        ///             virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The Shared Image Gallery Image must have replicas in the same region and must be in the same subscription as the Azure Batch account. If the image version is not specified in the imageId, the latest version will be used. For information about the firewall settings for the Batch Compute Node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
        ///             exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create the node. This read-only field differs from &apos;version&apos; only if the value specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
        ///           }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image Gallery Image. To get the list of all Azure Marketplace Image references verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
        ///           nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the Pool, and provides the command-and-control interface between the Compute Node and the Batch service. There are different implementations of the Compute Node agent, known as SKUs, for different operating systems. You must specify a Compute Node agent SKU which matches the selected Image reference. To get the list of supported Compute Node agent SKUs along with their list of verified Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
        ///           windowsConfiguration: {
        ///             enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
        ///           }, # Optional. This property must not be specified if the imageReference property specifies a Linux OS Image.
        ///           dataDisks: [
        ///             {
        ///               lun: number, # Required. The lun is used to uniquely identify each data disk. If attaching multiple disks, each should have a distinct lun. The value must be between 0 and 63, inclusive.
        ///               caching: &quot;none&quot; | &quot;readonly&quot; | &quot;readwrite&quot;, # Optional. The default value for caching is readwrite. For information about the caching options see: https://blogs.msdn.microsoft.com/windowsazurestorage/2012/06/27/exploring-windows-azure-drives-disks-and-images/.
        ///               diskSizeGB: number, # Required. The initial disk size in gigabytes.
        ///               storageAccountType: &quot;standard_lrs&quot; | &quot;premium_lrs&quot;, # Optional. If omitted, the default is &quot;standard_lrs&quot;.
        ///             }
        ///           ], # Optional. This property must be specified if the Compute Nodes in the Pool need to have empty data disks attached to them. This cannot be updated. Each Compute Node gets its own disk (the disk is not a file share). Existing disks cannot be attached, each attached disk is empty. When the Compute Node is removed from the Pool, the disk and all data associated with it is also deleted. The disk is not formatted after being attached, it must be formatted before use - for more information see https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux and https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
        ///           licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and should only be used when you hold valid on-premises licenses for the Compute Nodes which will be deployed. If omitted, no on-premises licensing discount is applied. Values are:
        /// 
        ///  Windows_Server - The on-premises license is for Windows Server.
        ///  Windows_Client - The on-premises license is for Windows Client.
        /// 
        ///           containerConfiguration: {
        ///             type: string, # Required. The container technology to be used.
        ///             containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An Image will be sourced from the default Docker registry unless the Image is fully qualified with an alternative registry.
        ///             containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires credentials, then those credentials must be provided here.
        ///           }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow Tasks to run in containers. All regular Tasks and Job manager Tasks run on this Pool must specify the containerSettings property, and all other Tasks may specify it.
        ///           diskEncryptionConfiguration: {
        ///             targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot; and &quot;TemporaryDisk&quot; must be specified.
        ///           }, # Optional. If specified, encryption is performed on each node in the pool during node provisioning.
        ///           nodePlacementConfiguration: {
        ///             policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not specified, Batch will use the regional policy.
        ///           }, # Optional. This configuration will specify rules on how nodes in the pool will be physically allocated.
        ///           extensions: [
        ///             {
        ///               name: string, # Required. The name of the virtual machine extension.
        ///               publisher: string, # Required. The name of the extension handler publisher.
        ///               type: string, # Required. The type of the extension.
        ///               typeHandlerVersion: string, # Optional. The version of script handler.
        ///               autoUpgradeMinorVersion: boolean, # Optional. Indicates whether the extension should use a newer minor version if one is available at deployment time. Once deployed, however, the extension will not upgrade minor versions unless redeployed, even with this property set to true.
        ///               settings: AnyObject, # Optional. JSON formatted public settings for the extension.
        ///               protectedSettings: AnyObject, # Optional. The extension can contain either protectedSettings or protectedSettingsFromKeyVault or no protected settings at all. 
        ///               provisionAfterExtensions: [string], # Optional. Collection of extension names after which this extension needs to be provisioned.
        ///             }
        ///           ], # Optional. If specified, the extensions mentioned in this configuration will be installed on each node.
        ///           osDisk: {
        ///             ephemeralOSDiskSettings: {
        ///               placement: string, # Optional. This property can be used by user in the request to choose the location e.g., cache disk space for Ephemeral OS disk provisioning. For more information on Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size requirements for Windows VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements and Linux VMs at https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
        ///             }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the compute node (VM).
        ///           }, # Optional. Settings for the operating system disk of the compute node (VM).
        ///         }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS VMs. This property and cloudServiceConfiguration are mutually exclusive and one of the properties must be specified. If neither is specified then the Batch service returns an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number of cores of the vmSize of the pool or 256.
        ///         taskSchedulingPolicy: {
        ///           nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
        ///         }, # Optional. If not specified, the default is spread.
        ///         resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when enableAutoScale is set to true. The default value is 15 minutes. The minimum value is 5 minutes. If you specify a value less than 5 minutes, the Batch service rejects the request with an error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If enableAutoScale is set to false, then you must set either targetDedicatedNodes, targetLowPriorityNodes, or both.
        ///         enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must be specified. If true, the autoScaleFormula element is required. The Pool automatically resizes according to the formula. The default value is false.
        ///         autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is required if enableAutoScale is set to true. The formula is checked for validity before the Pool is created. If the formula is not valid, the Batch service rejects the request with detailed error information.
        ///         autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes and 168 hours respectively. If you specify a value less than 5 minutes or greater than 168 hours, the Batch service rejects the request with an invalid property value error; if you are calling the REST API directly, the HTTP status code is 400 (Bad Request).
        ///         enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to deployment restrictions on the Compute Nodes of the Pool. This may result in the Pool not reaching its desired size. The default value is false.
        ///         networkConfiguration: {
        ///           subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have enough free IP addresses, the Pool will partially allocate Nodes and a resize error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet. The specified subnet must allow communication from the Azure Batch service to be able to schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. For Pools created with virtualMachineConfiguration only ARM virtual networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools created with cloudServiceConfiguration both ARM and classic virtual networks are supported. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication. For Pools created with a virtual machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. For Pools created with a cloud service configuration, enable ports 10100, 20100, and 30100. Also enable outbound connections to Azure Storage on port 443. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///           dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
        ///           endpointConfiguration: {
        ///             inboundNATPools: [
        ///               {
        ///                 name: string, # Required. The name must be unique within a Batch Pool, can contain letters, numbers, underscores, periods, and hyphens. Names must start with a letter or number, must end with a letter, number, or underscore, and cannot exceed 77 characters.  If any invalid values are provided the request fails with HTTP status code 400.
        ///                 protocol: &quot;tcp&quot; | &quot;udp&quot;, # Required. The protocol of the endpoint.
        ///                 backendPort: number, # Required. This must be unique within a Batch Pool. Acceptable values are between 1 and 65535 except for 22, 3389, 29876 and 29877 as these are reserved. If any reserved values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeStart: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 frontendPortRangeEnd: number, # Required. Acceptable values range between 1 and 65534 except ports from 50000 to 55000 which are reserved by the Batch service. All ranges within a Pool must be distinct and cannot overlap. Each range must contain at least 40 ports. If any reserved or overlapping values are provided the request fails with HTTP status code 400.
        ///                 networkSecurityGroupRules: [
        ///                   {
        ///                     priority: number, # Required. Priorities within a Pool must be unique and are evaluated in order of priority. The lower the number the higher the priority. For example, rules could be specified with order numbers of 150, 250, and 350. The rule with the order number of 150 takes precedence over the rule that has an order of 250. Allowed priorities are 150 to 4096. If any reserved or duplicate values are provided the request fails with HTTP status code 400.
        ///                     access: &quot;allow&quot; | &quot;deny&quot;, # Required. The action that should be taken for a specified IP address, subnet range or tag.
        ///                     sourceAddressPrefix: string, # Required. Valid values are a single IP address (i.e. 10.10.10.10), IP subnet (i.e. 192.168.1.0/24), default tag, or * (for all addresses).  If any other values are provided the request fails with HTTP status code 400.
        ///                     sourcePortRanges: [string], # Optional. Valid values are &apos;*&apos; (for all ports 0 - 65535), a specific port (i.e. 22), or a port range (i.e. 100-200). The ports must be in the range of 0 to 65535. Each entry in this collection must not overlap any other entry (either a range or an individual port). If any other values are provided the request fails with HTTP status code 400. The default value is &apos;*&apos;.
        ///                   }
        ///                 ], # Optional. The maximum number of rules that can be specified across all the endpoints on a Batch Pool is 25. If no network security group rules are specified, a default rule will be created to allow inbound access to the specified backendPort. If the maximum number of network security group rules is exceeded the request fails with HTTP status code 400.
        ///               }
        ///             ], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum number of inbound NAT Pools is exceeded the request fails with HTTP status code 400. This cannot be specified if the IPAddressProvisioningType is NoPublicIPAddresses.
        ///           }, # Optional. Pool endpoint configuration is only supported on Pools with the virtualMachineConfiguration property.
        ///           publicIPAddressConfiguration: {
        ///             provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
        ///             ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100 dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public IP. For example, a pool needing 250 dedicated VMs would need at least 3 public IPs specified. Each element of this collection is of the form: /subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
        ///           }, # Optional. Public IP configuration property is only supported on Pools with the virtualMachineConfiguration property.
        ///         }, # Optional. The network configuration for a Pool.
        ///         startTask: {
        ///           commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c MyCommand&quot; in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
        ///           containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are mapped into the container, all Task environment variables are mapped into the container, and the Task command line is executed in the container. Files produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that Batch file APIs will not be able to access those files.
        ///           resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
        ///           environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
        ///           userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        ///           maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this value specifically controls the number of retries. The Batch service will try the Task once, and may then retry up to this limit. For example, if the maximum retry count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries). If the maximum retry count is 0, the Batch service does not retry the Task. If the maximum retry count is -1, the Batch service retries the Task without limit, however this is not recommended for a start task or any task. The default value is 0 (no retries)
        ///           waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has still not completed successfully after all retries, then the Batch service marks the Node unusable, and will not schedule Tasks to it. This condition can be detected via the Compute Node state and failure info details. If false, the Batch service will not wait for the StartTask to complete. In this case, other Tasks can start executing on the Compute Node while the StartTask is still running; and even if the StartTask fails, new Tasks will continue to be scheduled on the Compute Node. The default is true.
        ///         }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing. In some cases the StartTask may be re-run even though the Compute Node was not rebooted. Special care should be taken to avoid StartTasks which create breakaway process or install/launch services from the StartTask working directory, as this will block Batch from being able to re-run the StartTask.
        ///         certificateReferences: [
        ///           {
        ///             thumbprint: string, # Required. The thumbprint of the Certificate.
        ///             thumbprintAlgorithm: string, # Required. The algorithm with which the thumbprint is associated. This must be sha1.
        ///             storeLocation: &quot;currentuser&quot; | &quot;localmachine&quot;, # Optional. The default value is currentuser. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///             storeName: string, # Optional. This property is applicable only for Pools configured with Windows Compute Nodes (that is, created with cloudServiceConfiguration, or with virtualMachineConfiguration using a Windows Image reference). Common store names include: My, Root, CA, Trust, Disallowed, TrustedPeople, TrustedPublisher, AuthRoot, AddressBook, but any custom store name can also be used. The default value is My.
        ///             visibility: [&quot;starttask&quot; | &quot;task&quot; | &quot;remoteuser&quot;], # Optional. You can specify more than one visibility in this collection. The default is all Accounts.
        ///           }
        ///         ], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified Certificate store and location. For Linux Compute Nodes, the Certificates are stored in a directory inside the Task working directory and an environment variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and Certificates are placed in that directory.
        ///         applicationPackageReferences: [ApplicationPackageReference], # Optional. Changes to Package references affect all new Nodes joining the Pool, but do not affect Compute Nodes that are already in the Pool until they are rebooted or reimaged. There is a maximum of 10 Package references on any given Pool.
        ///         applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service application licenses. If a license is requested which is not supported, Pool creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;, &apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application license added to the Pool.
        ///         userAccounts: [
        ///           {
        ///             name: string, # Required. The name of the user Account.
        ///             password: string, # Required. The password for the user Account.
        ///             elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        ///             linuxUserConfiguration: {
        ///               uid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the uid.
        ///               gid: number, # Optional. The uid and gid properties must be specified together or not at all. If not specified the underlying operating system picks the gid.
        ///               sshPrivateKey: string, # Optional. The private key must not be password protected. The private key is used to automatically configure asymmetric-key based authentication for SSH between Compute Nodes in a Linux Pool when the Pool&apos;s enableInterNodeCommunication property is true (it is ignored if enableInterNodeCommunication is false). It does this by placing the key pair into the user&apos;s .ssh directory. If not specified, password-less SSH is not configured between Compute Nodes (no modification of the user&apos;s .ssh directory is done).
        ///             }, # Optional. This property is ignored if specified on a Windows Pool. If not specified, the user is created with the default options.
        ///             windowsUserConfiguration: {
        ///               loginMode: &quot;batch&quot; | &quot;interactive&quot;, # Optional. The default value for VirtualMachineConfiguration Pools is &apos;batch&apos; and for CloudServiceConfiguration Pools is &apos;interactive&apos;.
        ///             }, # Optional. This property can only be specified if the user is on a Windows Pool. If not specified and on a Windows Pool, the user is created with the default options.
        ///           }
        ///         ], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
        ///         metadata: [
        ///           {
        ///             name: string, # Required. The name of the metadata item.
        ///             value: string, # Required. The value of the metadata item.
        ///           }
        ///         ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///         mountConfiguration: [
        ///           {
        ///             azureBlobFileSystemConfiguration: {
        ///               accountName: string, # Required. The Azure Storage Account name.
        ///               containerName: string, # Required. The Azure Blob Storage Container name.
        ///               accountKey: string, # Optional. This property is mutually exclusive with both sasKey and identity; exactly one must be specified.
        ///               sasKey: string, # Optional. This property is mutually exclusive with both accountKey and identity; exactly one must be specified.
        ///               blobfuseOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               identityReference: ComputeNodeIdentityReference, # Optional. This property is mutually exclusive with both accountKey and sasKey; exactly one must be specified.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             nfsMountConfiguration: {
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             cifsMountConfiguration: {
        ///               username: string, # Required. The user to use for authentication against the CIFS file system.
        ///               source: string, # Required. The URI of the file system to mount.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///               password: string, # Required. The password to use for authentication against the CIFS file system.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///             azureFileShareConfiguration: {
        ///               accountName: string, # Required. The Azure Storage account name.
        ///               azureFileUrl: string, # Required. This is of the form &apos;https://{account}.file.core.windows.net/&apos;.
        ///               accountKey: string, # Required. The Azure Storage account key.
        ///               relativeMountPath: string, # Required. All file systems are mounted relative to the Batch mounts directory, accessible via the AZ_BATCH_NODE_MOUNTS_DIR environment variable.
        ///               mountOptions: string, # Optional. These are &apos;net use&apos; options in Windows and &apos;mount&apos; options in Linux.
        ///             }, # Optional. This property is mutually exclusive with all other properties.
        ///           }
        ///         ], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
        ///       }, # Optional. Specification for creating a new Pool.
        ///     }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed state, and the Pool creation error is set in the Job&apos;s scheduling error property. The Batch service manages the lifetime (both creation and, unless keepAlive is specified, deletion) of the auto Pool. Any user actions that affect the lifetime of the auto Pool while the Job is active will result in unexpected behavior. You must specify either the Pool ID or the auto Pool specification, but not both.
        ///   }, # Optional. Specifies how a Job should be assigned to a Pool.
        ///   onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. The default is noaction.
        ///   onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. A Task is considered to have failed if has a failureInfo. A failureInfo is set if the Task completes with a non-zero exit code after exhausting its retry count, or if there was an error starting the Task, for example due to a resource file download error. The default is noaction.
        ///   networkConfiguration: {
        ///     subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure Batch Account. The specified subnet should have enough free IP addresses to accommodate the number of Compute Nodes which will run Tasks from the Job. This can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos; service principal must have the &apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for the specified VNet so that Azure Batch service can schedule Tasks on the Nodes. This can be verified by checking if the specified VNet has any associated Network Security Groups (NSG). If communication to the Nodes in the specified subnet is denied by an NSG, then the Batch service will set the state of the Compute Nodes to unusable. This is of the form /subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}. If the specified VNet has any associated Network Security Groups (NSG), then a few reserved system ports must be enabled for inbound communication from the Azure Batch service. For Pools created with a Virtual Machine configuration, enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows. Port 443 is also required to be open for outbound connections for communications to Azure Storage. For more details see: https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        ///   }, # Optional. The network configuration for the Job.
        ///   metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the use of user code.
        ///   executionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. This is the time at which the Job was created.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Job is in the completed state.
        ///     poolId: string, # Optional. This element contains the actual Pool where the Job is assigned. When you get Job details from the service, they also contain a poolInfo element, which contains the Pool configuration data from when the Job was added or updated. That poolInfo element may also contain a poolId element. If it does, the two IDs are the same. If it does not, it means the Job ran on an auto Pool, and this property contains the ID of that auto Pool.
        ///     schedulingError: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Job scheduling error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Job scheduling error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional error details related to the scheduling error.
        ///     }, # Optional. This property is not set if there was no error starting the Job.
        ///     terminateReason: string, # Optional. This property is set only if the Job is in the completed state. If the Batch service terminates the Job, it sets the reason as follows: JMComplete - the Job Manager Task completed, and killJobOnCompletion was set to true. MaxWallClockTimeExpiry - the Job reached its maxWallClockTime constraint. TerminateJobSchedule - the Job ran as part of a schedule, and the schedule terminated. AllTasksComplete - the Job&apos;s onAllTasksComplete attribute is set to terminatejob, and all Tasks in the Job are complete. TaskFailed - the Job&apos;s onTaskFailure attribute is set to performExitOptionsJobAction, and a Task in the Job failed with an exit condition that specified a jobAction of terminatejob. Any other string is a user-defined reason specified in a call to the &apos;Terminate a Job&apos; operation.
        ///   }, # Optional. Contains information about the execution of a Job in the Azure Batch service.
        ///   stats: {
        ///     url: string, # Required. The URL of the statistics.
        ///     startTime: string (ISO 8601 Format), # Required. The start time of the time range covered by the statistics.
        ///     lastUpdateTime: string (ISO 8601 Format), # Required. The time at which the statistics were last updated. All statistics are limited to the range between startTime and lastUpdateTime.
        ///     userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes) consumed by all Tasks in the Job.
        ///     wallClockTime: string (duration ISO 8601 Format), # Required.  The wall clock time is the elapsed time from when the Task started running on a Compute Node to when it finished (or to the last time the statistics were updated, if the Task had not finished by then). If a Task was retried, this includes the wall clock time of all the Task retries.
        ///     readIOps: number, # Required. The total number of disk read operations made by all Tasks in the Job.
        ///     writeIOps: number, # Required. The total number of disk write operations made by all Tasks in the Job.
        ///     readIOGiB: number, # Required. The total amount of data in GiB read from disk by all Tasks in the Job.
        ///     writeIOGiB: number, # Required. The total amount of data in GiB written to disk by all Tasks in the Job.
        ///     numSucceededTasks: number, # Required. A Task completes successfully if it returns exit code 0.
        ///     numFailedTasks: number, # Required. A Task fails if it exhausts its maximum retry count without returning exit code 0.
        ///     numTaskRetries: number, # Required. The total number of retries on all the Tasks in the Job during the given time range.
        ///     waitTime: string (duration ISO 8601 Format), # Required. The wait time for a Task is defined as the elapsed time between the creation of the Task and the start of Task execution. (If the Task is retried due to failures, the wait time is the time to the most recent Task execution.) This value is only reported in the Account lifetime statistics; it is not included in the Job statistics.
        ///   }, # Optional. This property is populated only if the CloudJob was retrieved with an expand clause including the &apos;stats&apos; attribute; otherwise it is null. The statistics may not be immediately available. The Batch service performs periodic roll-up of statistics. The typical delay is about 30 minutes.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Pageable<BinaryData> GetFromJobSchedules(string jobScheduleId, string filter = null, string select = null, string expand = null, int? maxResults = null, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobScheduleId, nameof(jobScheduleId));

            return GetFromJobSchedulesImplementation("JobClient.GetFromJobSchedules", jobScheduleId, filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
        }

        private Pageable<BinaryData> GetFromJobSchedulesImplementation(string diagnosticsScopeName, string jobScheduleId, string filter, string select, string expand, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            return PageableHelpers.CreatePageable(CreateEnumerable, ClientDiagnostics, diagnosticsScopeName);
            IEnumerable<Page<BinaryData>> CreateEnumerable(string nextLink, int? pageSizeHint)
            {
                do
                {
                    var message = string.IsNullOrEmpty(nextLink)
                        ? CreateGetFromJobSchedulesRequest(jobScheduleId, filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context)
                        : CreateGetFromJobSchedulesNextPageRequest(nextLink, jobScheduleId, filter, select, expand, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                    var page = LowLevelPageableHelpers.ProcessMessage(_pipeline, message, context, "value", "odata.nextLink");
                    nextLink = page.ContinuationToken;
                    yield return page;
                } while (!string.IsNullOrEmpty(nextLink));
            }
        }

        /// <summary> Lists the execution status of the Job Preparation and Job Release Task for the specified Job across the Compute Nodes where the Job has run. </summary>
        /// <param name="jobId"> The ID of the Job. </param>
        /// <param name="filter"> An OData $filter clause. For more information on constructing this filter, see https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-job-preparation-and-release-status. </param>
        /// <param name="select"> An OData $select clause. </param>
        /// <param name="maxResults"> The maximum number of items to return in the response. A maximum of 1000 Tasks can be returned. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The <see cref="AsyncPageable{T}"/> from the service containing a list of <see cref="BinaryData"/> objects. Details of the body schema for each item in the collection are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetPreparationAndReleaseTaskStatusesAsync with required parameters and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// await foreach (var data in client.GetPreparationAndReleaseTaskStatusesAsync("<jobId>"))
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.ToString());
        /// }
        /// ]]></code>
        /// This sample shows how to call GetPreparationAndReleaseTaskStatusesAsync with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// await foreach (var data in client.GetPreparationAndReleaseTaskStatusesAsync("<jobId>", "<filter>", "<select>", 1234, 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow))
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.GetProperty("poolId").ToString());
        ///     Console.WriteLine(result.GetProperty("nodeId").ToString());
        ///     Console.WriteLine(result.GetProperty("nodeUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("endTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("taskRootDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("taskRootDirectoryUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("exitCode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("containerInfo").GetProperty("containerId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("containerInfo").GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("containerInfo").GetProperty("error").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("failureInfo").GetProperty("category").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("failureInfo").GetProperty("code").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("failureInfo").GetProperty("message").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("failureInfo").GetProperty("details")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("failureInfo").GetProperty("details")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("retryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("lastRetryTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("result").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("endTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("taskRootDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("taskRootDirectoryUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("exitCode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("containerInfo").GetProperty("containerId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("containerInfo").GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("containerInfo").GetProperty("error").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("failureInfo").GetProperty("category").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("failureInfo").GetProperty("code").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("failureInfo").GetProperty("message").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("failureInfo").GetProperty("details")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("failureInfo").GetProperty("details")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("result").ToString());
        /// }
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// This API returns the Job Preparation and Job Release Task status on all Compute Nodes that have run the Job Preparation or Job Release Task. This includes Compute Nodes which have since been removed from the Pool. If this API is invoked on a Job which has no Job Preparation or Job Release Task, the Batch service returns HTTP status code 409 (Conflict) with an error code of JobPreparationTaskNotSpecified.
        /// 
        /// Below is the JSON schema for one item in the pageable response.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>JobListPreparationAndReleaseTaskStatusResultValue</c>:
        /// <code>{
        ///   poolId: string, # Optional. The ID of the Pool containing the Compute Node to which this entry refers.
        ///   nodeId: string, # Optional. The ID of the Compute Node to which this entry refers.
        ///   nodeUrl: string, # Optional. The URL of the Compute Node to which this entry refers.
        ///   jobPreparationTaskExecutionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. If the Task has been restarted or retried, this is the most recent time at which the Task started running.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Task is in the Completed state.
        ///     state: &quot;running&quot; | &quot;completed&quot;, # Required. The current state of the Job Preparation Task on the Compute Node.
        ///     taskRootDirectory: string, # Optional. The root directory of the Job Preparation Task on the Compute Node. You can use this path to retrieve files created by the Task, such as log files.
        ///     taskRootDirectoryUrl: string, # Optional. The URL to the root directory of the Job Preparation Task on the Compute Node.
        ///     exitCode: number, # Optional. This parameter is returned only if the Task is in the completed state. The exit code for a process reflects the specific convention implemented by the application developer for that process. If you use the exit code value to make decisions in your code, be sure that you know the exit code convention used by the application process. Note that the exit code may also be generated by the Compute Node operating system, such as when a process is forcibly terminated.
        ///     containerInfo: {
        ///       containerId: string, # Optional. The ID of the container.
        ///       state: string, # Optional. This is the state of the container according to the Docker service. It is equivalent to the status field returned by &quot;docker inspect&quot;.
        ///       error: string, # Optional. This is the detailed error string from the Docker service, if available. It is equivalent to the error field returned by &quot;docker inspect&quot;.
        ///     }, # Optional. This property is set only if the Task runs in a container context.
        ///     failureInfo: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Task error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Task error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional details related to the error.
        ///     }, # Optional. This property is set only if the Task is in the completed state and encountered a failure.
        ///     retryCount: number, # Required. Task application failures (non-zero exit code) are retried, pre-processing errors (the Task could not be run) and file upload errors are not retried. The Batch service will retry the Task up to the limit specified by the constraints.
        ///     lastRetryTime: string (ISO 8601 Format), # Optional. This property is set only if the Task was retried (i.e. retryCount is nonzero). If present, this is typically the same as startTime, but may be different if the Task has been restarted for reasons other than retry; for example, if the Compute Node was rebooted during a retry, then the startTime is updated but the lastRetryTime is not.
        ///     result: &quot;success&quot; | &quot;failure&quot;, # Optional. If the value is &apos;failed&apos;, then the details of the failure can be found in the failureInfo property.
        ///   }, # Optional. Contains information about the execution of a Job Preparation Task on a Compute Node.
        ///   jobReleaseTaskExecutionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. If the Task has been restarted or retried, this is the most recent time at which the Task started running.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Task is in the Completed state.
        ///     state: &quot;running&quot; | &quot;completed&quot;, # Required. The current state of the Job Release Task on the Compute Node.
        ///     taskRootDirectory: string, # Optional. The root directory of the Job Release Task on the Compute Node. You can use this path to retrieve files created by the Task, such as log files.
        ///     taskRootDirectoryUrl: string, # Optional. The URL to the root directory of the Job Release Task on the Compute Node.
        ///     exitCode: number, # Optional. This parameter is returned only if the Task is in the completed state. The exit code for a process reflects the specific convention implemented by the application developer for that process. If you use the exit code value to make decisions in your code, be sure that you know the exit code convention used by the application process. Note that the exit code may also be generated by the Compute Node operating system, such as when a process is forcibly terminated.
        ///     containerInfo: TaskContainerExecutionInformation, # Optional. This property is set only if the Task runs in a container context.
        ///     failureInfo: TaskFailureInformation, # Optional. This property is set only if the Task is in the completed state and encountered a failure.
        ///     result: &quot;success&quot; | &quot;failure&quot;, # Optional. If the value is &apos;failed&apos;, then the details of the failure can be found in the failureInfo property.
        ///   }, # Optional. This property is set only if the Job Release Task has run on the Compute Node.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual AsyncPageable<BinaryData> GetPreparationAndReleaseTaskStatusesAsync(string jobId, string filter = null, string select = null, int? maxResults = null, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            return GetPreparationAndReleaseTaskStatusesImplementationAsync("JobClient.GetPreparationAndReleaseTaskStatuses", jobId, filter, select, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
        }

        private AsyncPageable<BinaryData> GetPreparationAndReleaseTaskStatusesImplementationAsync(string diagnosticsScopeName, string jobId, string filter, string select, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            return PageableHelpers.CreateAsyncPageable(CreateEnumerableAsync, ClientDiagnostics, diagnosticsScopeName);
            async IAsyncEnumerable<Page<BinaryData>> CreateEnumerableAsync(string nextLink, int? pageSizeHint, [EnumeratorCancellation] CancellationToken cancellationToken = default)
            {
                do
                {
                    var message = string.IsNullOrEmpty(nextLink)
                        ? CreateGetPreparationAndReleaseTaskStatusesRequest(jobId, filter, select, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context)
                        : CreateGetPreparationAndReleaseTaskStatusesNextPageRequest(nextLink, jobId, filter, select, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                    var page = await LowLevelPageableHelpers.ProcessMessageAsync(_pipeline, message, context, "value", "odata.nextLink", cancellationToken).ConfigureAwait(false);
                    nextLink = page.ContinuationToken;
                    yield return page;
                } while (!string.IsNullOrEmpty(nextLink));
            }
        }

        /// <summary> Lists the execution status of the Job Preparation and Job Release Task for the specified Job across the Compute Nodes where the Job has run. </summary>
        /// <param name="jobId"> The ID of the Job. </param>
        /// <param name="filter"> An OData $filter clause. For more information on constructing this filter, see https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-job-preparation-and-release-status. </param>
        /// <param name="select"> An OData $select clause. </param>
        /// <param name="maxResults"> The maximum number of items to return in the response. A maximum of 1000 Tasks can be returned. </param>
        /// <param name="timeout"> The maximum time that the server can spend processing the request, in seconds. The default is 30 seconds. </param>
        /// <param name="clientRequestId"> The caller-generated request identity, in the form of a GUID with no decoration such as curly braces, e.g. 9C4D50EE-2D56-4CD3-8152-34347DC9F2B0. </param>
        /// <param name="returnClientRequestId"> Whether the server should return the client-request-id in the response. </param>
        /// <param name="ocpDate"> The time the request was issued. Client libraries typically set this to the current system clock time; set it explicitly if you are calling the REST API directly. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="jobId"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="jobId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The <see cref="Pageable{T}"/> from the service containing a list of <see cref="BinaryData"/> objects. Details of the body schema for each item in the collection are in the Remarks section below. </returns>
        /// <example>
        /// This sample shows how to call GetPreparationAndReleaseTaskStatuses with required parameters and parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// foreach (var data in client.GetPreparationAndReleaseTaskStatuses("<jobId>"))
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.ToString());
        /// }
        /// ]]></code>
        /// This sample shows how to call GetPreparationAndReleaseTaskStatuses with all parameters, and how to parse the result.
        /// <code><![CDATA[
        /// var credential = new DefaultAzureCredential();
        /// var client = new BatchClient(credential).GetJobClientClient("<batchUrl>", <2022-01-01.15.0>);
        /// 
        /// foreach (var data in client.GetPreparationAndReleaseTaskStatuses("<jobId>", "<filter>", "<select>", 1234, 1234, Guid.NewGuid(), true, DateTimeOffset.UtcNow))
        /// {
        ///     JsonElement result = JsonDocument.Parse(data.ToStream()).RootElement;
        ///     Console.WriteLine(result.GetProperty("poolId").ToString());
        ///     Console.WriteLine(result.GetProperty("nodeId").ToString());
        ///     Console.WriteLine(result.GetProperty("nodeUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("endTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("taskRootDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("taskRootDirectoryUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("exitCode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("containerInfo").GetProperty("containerId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("containerInfo").GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("containerInfo").GetProperty("error").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("failureInfo").GetProperty("category").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("failureInfo").GetProperty("code").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("failureInfo").GetProperty("message").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("failureInfo").GetProperty("details")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("failureInfo").GetProperty("details")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("retryCount").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("lastRetryTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobPreparationTaskExecutionInfo").GetProperty("result").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("startTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("endTime").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("taskRootDirectory").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("taskRootDirectoryUrl").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("exitCode").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("containerInfo").GetProperty("containerId").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("containerInfo").GetProperty("state").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("containerInfo").GetProperty("error").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("failureInfo").GetProperty("category").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("failureInfo").GetProperty("code").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("failureInfo").GetProperty("message").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("failureInfo").GetProperty("details")[0].GetProperty("name").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("failureInfo").GetProperty("details")[0].GetProperty("value").ToString());
        ///     Console.WriteLine(result.GetProperty("jobReleaseTaskExecutionInfo").GetProperty("result").ToString());
        /// }
        /// ]]></code>
        /// </example>
        /// <remarks>
        /// This API returns the Job Preparation and Job Release Task status on all Compute Nodes that have run the Job Preparation or Job Release Task. This includes Compute Nodes which have since been removed from the Pool. If this API is invoked on a Job which has no Job Preparation or Job Release Task, the Batch service returns HTTP status code 409 (Conflict) with an error code of JobPreparationTaskNotSpecified.
        /// 
        /// Below is the JSON schema for one item in the pageable response.
        /// 
        /// Response Body:
        /// 
        /// Schema for <c>JobListPreparationAndReleaseTaskStatusResultValue</c>:
        /// <code>{
        ///   poolId: string, # Optional. The ID of the Pool containing the Compute Node to which this entry refers.
        ///   nodeId: string, # Optional. The ID of the Compute Node to which this entry refers.
        ///   nodeUrl: string, # Optional. The URL of the Compute Node to which this entry refers.
        ///   jobPreparationTaskExecutionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. If the Task has been restarted or retried, this is the most recent time at which the Task started running.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Task is in the Completed state.
        ///     state: &quot;running&quot; | &quot;completed&quot;, # Required. The current state of the Job Preparation Task on the Compute Node.
        ///     taskRootDirectory: string, # Optional. The root directory of the Job Preparation Task on the Compute Node. You can use this path to retrieve files created by the Task, such as log files.
        ///     taskRootDirectoryUrl: string, # Optional. The URL to the root directory of the Job Preparation Task on the Compute Node.
        ///     exitCode: number, # Optional. This parameter is returned only if the Task is in the completed state. The exit code for a process reflects the specific convention implemented by the application developer for that process. If you use the exit code value to make decisions in your code, be sure that you know the exit code convention used by the application process. Note that the exit code may also be generated by the Compute Node operating system, such as when a process is forcibly terminated.
        ///     containerInfo: {
        ///       containerId: string, # Optional. The ID of the container.
        ///       state: string, # Optional. This is the state of the container according to the Docker service. It is equivalent to the status field returned by &quot;docker inspect&quot;.
        ///       error: string, # Optional. This is the detailed error string from the Docker service, if available. It is equivalent to the error field returned by &quot;docker inspect&quot;.
        ///     }, # Optional. This property is set only if the Task runs in a container context.
        ///     failureInfo: {
        ///       category: &quot;usererror&quot; | &quot;servererror&quot;, # Required. The category of the error.
        ///       code: string, # Optional. An identifier for the Task error. Codes are invariant and are intended to be consumed programmatically.
        ///       message: string, # Optional. A message describing the Task error, intended to be suitable for display in a user interface.
        ///       details: [
        ///         {
        ///           name: string, # Optional. The name in the name-value pair.
        ///           value: string, # Optional. The value in the name-value pair.
        ///         }
        ///       ], # Optional. A list of additional details related to the error.
        ///     }, # Optional. This property is set only if the Task is in the completed state and encountered a failure.
        ///     retryCount: number, # Required. Task application failures (non-zero exit code) are retried, pre-processing errors (the Task could not be run) and file upload errors are not retried. The Batch service will retry the Task up to the limit specified by the constraints.
        ///     lastRetryTime: string (ISO 8601 Format), # Optional. This property is set only if the Task was retried (i.e. retryCount is nonzero). If present, this is typically the same as startTime, but may be different if the Task has been restarted for reasons other than retry; for example, if the Compute Node was rebooted during a retry, then the startTime is updated but the lastRetryTime is not.
        ///     result: &quot;success&quot; | &quot;failure&quot;, # Optional. If the value is &apos;failed&apos;, then the details of the failure can be found in the failureInfo property.
        ///   }, # Optional. Contains information about the execution of a Job Preparation Task on a Compute Node.
        ///   jobReleaseTaskExecutionInfo: {
        ///     startTime: string (ISO 8601 Format), # Required. If the Task has been restarted or retried, this is the most recent time at which the Task started running.
        ///     endTime: string (ISO 8601 Format), # Optional. This property is set only if the Task is in the Completed state.
        ///     state: &quot;running&quot; | &quot;completed&quot;, # Required. The current state of the Job Release Task on the Compute Node.
        ///     taskRootDirectory: string, # Optional. The root directory of the Job Release Task on the Compute Node. You can use this path to retrieve files created by the Task, such as log files.
        ///     taskRootDirectoryUrl: string, # Optional. The URL to the root directory of the Job Release Task on the Compute Node.
        ///     exitCode: number, # Optional. This parameter is returned only if the Task is in the completed state. The exit code for a process reflects the specific convention implemented by the application developer for that process. If you use the exit code value to make decisions in your code, be sure that you know the exit code convention used by the application process. Note that the exit code may also be generated by the Compute Node operating system, such as when a process is forcibly terminated.
        ///     containerInfo: TaskContainerExecutionInformation, # Optional. This property is set only if the Task runs in a container context.
        ///     failureInfo: TaskFailureInformation, # Optional. This property is set only if the Task is in the completed state and encountered a failure.
        ///     result: &quot;success&quot; | &quot;failure&quot;, # Optional. If the value is &apos;failed&apos;, then the details of the failure can be found in the failureInfo property.
        ///   }, # Optional. This property is set only if the Job Release Task has run on the Compute Node.
        /// }
        /// </code>
        /// 
        /// </remarks>
        public virtual Pageable<BinaryData> GetPreparationAndReleaseTaskStatuses(string jobId, string filter = null, string select = null, int? maxResults = null, int? timeout = null, Guid? clientRequestId = null, bool? returnClientRequestId = null, DateTimeOffset? ocpDate = null, RequestContext context = null)
        {
            Argument.AssertNotNullOrEmpty(jobId, nameof(jobId));

            return GetPreparationAndReleaseTaskStatusesImplementation("JobClient.GetPreparationAndReleaseTaskStatuses", jobId, filter, select, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
        }

        private Pageable<BinaryData> GetPreparationAndReleaseTaskStatusesImplementation(string diagnosticsScopeName, string jobId, string filter, string select, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            return PageableHelpers.CreatePageable(CreateEnumerable, ClientDiagnostics, diagnosticsScopeName);
            IEnumerable<Page<BinaryData>> CreateEnumerable(string nextLink, int? pageSizeHint)
            {
                do
                {
                    var message = string.IsNullOrEmpty(nextLink)
                        ? CreateGetPreparationAndReleaseTaskStatusesRequest(jobId, filter, select, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context)
                        : CreateGetPreparationAndReleaseTaskStatusesNextPageRequest(nextLink, jobId, filter, select, maxResults, timeout, clientRequestId, returnClientRequestId, ocpDate, context);
                    var page = LowLevelPageableHelpers.ProcessMessage(_pipeline, message, context, "value", "odata.nextLink");
                    nextLink = page.ContinuationToken;
                    yield return page;
                } while (!string.IsNullOrEmpty(nextLink));
            }
        }

        internal HttpMessage CreateGetAllLifetimeStatisticsRequest(int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/lifetimejobstats", false);
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        internal HttpMessage CreateDeleteRequest(string jobId, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestConditions requestConditions, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier202);
            var request = message.Request;
            request.Method = RequestMethod.Delete;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs/", false);
            uri.AppendPath(jobId, true);
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            if (requestConditions != null)
            {
                request.Headers.Add(requestConditions, "R");
            }
            return message;
        }

        internal HttpMessage CreateGetJobRequest(string jobId, string select, string expand, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestConditions requestConditions, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs/", false);
            uri.AppendPath(jobId, true);
            if (select != null)
            {
                uri.AppendQuery("$select", select, true);
            }
            if (expand != null)
            {
                uri.AppendQuery("$expand", expand, true);
            }
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            if (requestConditions != null)
            {
                request.Headers.Add(requestConditions, "R");
            }
            return message;
        }

        internal HttpMessage CreatePatchRequest(string jobId, RequestContent content, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestConditions requestConditions, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Patch;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs/", false);
            uri.AppendPath(jobId, true);
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            if (requestConditions != null)
            {
                request.Headers.Add(requestConditions, "R");
            }
            request.Headers.Add("Content-Type", "application/json; odata=minimalmetadata");
            request.Content = content;
            return message;
        }

        internal HttpMessage CreateUpdateRequest(string jobId, RequestContent content, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestConditions requestConditions, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Put;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs/", false);
            uri.AppendPath(jobId, true);
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            if (requestConditions != null)
            {
                request.Headers.Add(requestConditions, "R");
            }
            request.Headers.Add("Content-Type", "application/json; odata=minimalmetadata");
            request.Content = content;
            return message;
        }

        internal HttpMessage CreateDisableRequest(string jobId, RequestContent content, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestConditions requestConditions, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier202);
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs/", false);
            uri.AppendPath(jobId, true);
            uri.AppendPath("/disable", false);
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            if (requestConditions != null)
            {
                request.Headers.Add(requestConditions, "R");
            }
            request.Headers.Add("Content-Type", "application/json; odata=minimalmetadata");
            request.Content = content;
            return message;
        }

        internal HttpMessage CreateEnableRequest(string jobId, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestConditions requestConditions, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier202);
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs/", false);
            uri.AppendPath(jobId, true);
            uri.AppendPath("/enable", false);
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            if (requestConditions != null)
            {
                request.Headers.Add(requestConditions, "R");
            }
            return message;
        }

        internal HttpMessage CreateTerminateRequest(string jobId, RequestContent content, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestConditions requestConditions, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier202);
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs/", false);
            uri.AppendPath(jobId, true);
            uri.AppendPath("/terminate", false);
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            if (requestConditions != null)
            {
                request.Headers.Add(requestConditions, "R");
            }
            request.Headers.Add("Content-Type", "application/json; odata=minimalmetadata");
            request.Content = content;
            return message;
        }

        internal HttpMessage CreateAddRequest(RequestContent content, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier201);
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs", false);
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            request.Headers.Add("Content-Type", "application/json; odata=minimalmetadata");
            request.Content = content;
            return message;
        }

        internal HttpMessage CreateGetJobsRequest(string filter, string select, string expand, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs", false);
            if (filter != null)
            {
                uri.AppendQuery("$filter", filter, true);
            }
            if (select != null)
            {
                uri.AppendQuery("$select", select, true);
            }
            if (expand != null)
            {
                uri.AppendQuery("$expand", expand, true);
            }
            if (maxResults != null)
            {
                uri.AppendQuery("maxresults", maxResults.Value, true);
            }
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        internal HttpMessage CreateGetFromJobSchedulesRequest(string jobScheduleId, string filter, string select, string expand, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobschedules/", false);
            uri.AppendPath(jobScheduleId, true);
            uri.AppendPath("/jobs", false);
            if (filter != null)
            {
                uri.AppendQuery("$filter", filter, true);
            }
            if (select != null)
            {
                uri.AppendQuery("$select", select, true);
            }
            if (expand != null)
            {
                uri.AppendQuery("$expand", expand, true);
            }
            if (maxResults != null)
            {
                uri.AppendQuery("maxresults", maxResults.Value, true);
            }
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        internal HttpMessage CreateGetPreparationAndReleaseTaskStatusesRequest(string jobId, string filter, string select, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs/", false);
            uri.AppendPath(jobId, true);
            uri.AppendPath("/jobpreparationandreleasetaskstatus", false);
            if (filter != null)
            {
                uri.AppendQuery("$filter", filter, true);
            }
            if (select != null)
            {
                uri.AppendQuery("$select", select, true);
            }
            if (maxResults != null)
            {
                uri.AppendQuery("maxresults", maxResults.Value, true);
            }
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        internal HttpMessage CreateGetTaskCountsRequest(string jobId, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendPath("/jobs/", false);
            uri.AppendPath(jobId, true);
            uri.AppendPath("/taskcounts", false);
            if (timeout != null)
            {
                uri.AppendQuery("timeout", timeout.Value, true);
            }
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        internal HttpMessage CreateGetJobsNextPageRequest(string nextLink, string filter, string select, string expand, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendRawNextLink(nextLink, false);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        internal HttpMessage CreateGetFromJobSchedulesNextPageRequest(string nextLink, string jobScheduleId, string filter, string select, string expand, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendRawNextLink(nextLink, false);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        internal HttpMessage CreateGetPreparationAndReleaseTaskStatusesNextPageRequest(string nextLink, string jobId, string filter, string select, int? maxResults, int? timeout, Guid? clientRequestId, bool? returnClientRequestId, DateTimeOffset? ocpDate, RequestContext context)
        {
            var message = _pipeline.CreateMessage(context, ResponseClassifier200);
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.AppendRaw(_batchUrl, false);
            uri.AppendRawNextLink(nextLink, false);
            request.Uri = uri;
            if (clientRequestId != null)
            {
                request.Headers.Add("client-request-id", clientRequestId.Value);
            }
            if (returnClientRequestId != null)
            {
                request.Headers.Add("return-client-request-id", returnClientRequestId.Value);
            }
            if (ocpDate != null)
            {
                request.Headers.Add("ocp-date", ocpDate.Value, "R");
            }
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        private static ResponseClassifier _responseClassifier200;
        private static ResponseClassifier ResponseClassifier200 => _responseClassifier200 ??= new StatusCodeClassifier(stackalloc ushort[] { 200 });
        private static ResponseClassifier _responseClassifier202;
        private static ResponseClassifier ResponseClassifier202 => _responseClassifier202 ??= new StatusCodeClassifier(stackalloc ushort[] { 202 });
        private static ResponseClassifier _responseClassifier201;
        private static ResponseClassifier ResponseClassifier201 => _responseClassifier201 ??= new StatusCodeClassifier(stackalloc ushort[] { 201 });
    }
}
